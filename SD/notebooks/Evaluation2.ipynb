{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a4d5bdc8-5cb3-4ee2-a133-ffff956f66da",
      "metadata": {
        "id": "a4d5bdc8-5cb3-4ee2-a133-ffff956f66da"
      },
      "source": [
        "# Getting Started with Permuted MNIST Environment\n",
        "\n",
        "## Objective\n",
        "\n",
        "The goal of this tutorial is to:\n",
        "- Learn how to use the PermutedMNIST environment for meta-learning experiments\n",
        "- Train and evaluate different agents on permuted MNIST tasks\n",
        "- Compare the performance of a **Random baseline** vs a **Linear classifier**\n",
        "- Achieve training and prediction in **less than 1 minute** per task\n",
        "\n",
        "## What is Permuted MNIST?\n",
        "\n",
        "Permuted MNIST is a meta-learning benchmark where:\n",
        "- Each task uses the same MNIST dataset\n",
        "- For each task, pixels are randomly permuted (shuffled) in a consistent way\n",
        "- Labels are also randomly permuted (e.g., all 3s might become 7s)\n",
        "- The agent must quickly adapt to each new permutation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8903fa6d-3fc9-4d4c-8520-cbb67975b272",
      "metadata": {
        "id": "8903fa6d-3fc9-4d4c-8520-cbb67975b272"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "MY4PsPTGm0wb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY4PsPTGm0wb",
        "outputId": "7b994b22-4cc8-4822-a50b-a3ff101082f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'permuted_mnist'...\n",
            "remote: Enumerating objects: 194, done.\u001b[K\n",
            "remote: Counting objects: 100% (194/194), done.\u001b[K\n",
            "remote: Compressing objects: 100% (134/134), done.\u001b[K\n",
            "remote: Total 194 (delta 77), reused 163 (delta 50), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (194/194), 12.62 MiB | 7.03 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ml-arena/permuted_mnist.git\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "YN2llLljnjq_",
      "metadata": {
        "id": "YN2llLljnjq_"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/permuted_mnist')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "p0edaud30tf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0edaud30tf",
        "outputId": "72e4e4b2-d6af-4304-fa53-5f1b17623b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Imports successful\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List\n",
        "\n",
        "# Import the environment and agents\n",
        "from permuted_mnist.env.permuted_mnist import PermutedMNISTEnv\n",
        "from permuted_mnist.agent.random.agent import Agent as RandomAgent\n",
        "from permuted_mnist.agent.linear.agent import Agent as LinearAgent\n",
        "from permuted_mnist.agent.torch_mlp.agent import Agent as TorchMLP\n",
        "\n",
        "print(\"✓ Imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vco5lhcip5b",
      "metadata": {
        "id": "vco5lhcip5b"
      },
      "source": [
        "## 2. Create the Environment\n",
        "\n",
        "Let's create an environment with 10 different permuted tasks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c7u41d9cvln",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7u41d9cvln",
        "outputId": "cf96cf23-2c40-42e4-9637-ce1a6a9b1951"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment created with 10 permuted tasks\n",
            "Training set size: 60000 samples\n",
            "Test set size: 10000 samples\n"
          ]
        }
      ],
      "source": [
        "# Create environment with 10 episodes (tasks)\n",
        "env = PermutedMNISTEnv(number_episodes=10)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "env.set_seed(42)\n",
        "\n",
        "print(f\"Environment created with {env.number_episodes} permuted tasks\")\n",
        "print(f\"Training set size: {env.train_size} samples\")\n",
        "print(f\"Test set size: {env.test_size} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33a72b29-bd35-4254-b0fe-774b73dc4609",
      "metadata": {
        "id": "33a72b29-bd35-4254-b0fe-774b73dc4609",
        "scrolled": true
      },
      "source": [
        "## 3. Understanding the Task Structure\n",
        "\n",
        "Let's examine what a single task looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "z74jqitydh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z74jqitydh",
        "outputId": "4e9353be-3d9a-44e3-961b-59464887f044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task structure:\n",
            "- X_train shape: (60000, 28, 28)\n",
            "- y_train shape: (60000, 1)\n",
            "- X_test shape: (10000, 28, 28)\n",
            "- y_test shape: (10000,)\n",
            "\n",
            "Label distribution in training set:\n",
            "  Label 0: 6131 samples\n",
            "  Label 1: 6742 samples\n",
            "  Label 2: 5421 samples\n",
            "  Label 3: 5851 samples\n",
            "  Label 4: 6265 samples\n",
            "  Label 5: 5958 samples\n",
            "  Label 6: 5949 samples\n",
            "  Label 7: 5842 samples\n",
            "  Label 8: 5923 samples\n",
            "  Label 9: 5918 samples\n"
          ]
        }
      ],
      "source": [
        "# Get the first task\n",
        "task = env.get_next_task()\n",
        "\n",
        "print(\"Task structure:\")\n",
        "print(f\"- X_train shape: {task['X_train'].shape}\")\n",
        "print(f\"- y_train shape: {task['y_train'].shape}\")\n",
        "print(f\"- X_test shape: {task['X_test'].shape}\")\n",
        "print(f\"- y_test shape: {task['y_test'].shape}\")\n",
        "print(f\"\\nLabel distribution in training set:\")\n",
        "unique, counts = np.unique(task['y_train'], return_counts=True)\n",
        "for label, count in zip(unique, counts):\n",
        "    print(f\"  Label {label}: {count} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ea4ardqha",
      "metadata": {
        "id": "5ea4ardqha"
      },
      "source": [
        "## 4. Visualize Permuted Images\n",
        "\n",
        "Let's see how the permutation affects the images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "joow2rsdve8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "joow2rsdve8",
        "outputId": "45731858-58c2-40ac-c39f-ec3d4bf8b2b9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: The images look scrambled due to pixel permutation!\n",
            "The labels are also permuted (not the original MNIST labels).\n"
          ]
        }
      ],
      "source": [
        "# Display some examples from the permuted task\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "fig.suptitle('Permuted MNIST Examples (Task 1)', fontsize=16)\n",
        "\n",
        "for i in range(10):\n",
        "    ax = axes[i // 5, i % 5]\n",
        "    ax.imshow(task['X_train'][i], cmap='gray')\n",
        "    ax.set_title(f'Label: {task[\"y_train\"][i][0]}')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Note: The images look scrambled due to pixel permutation!\")\n",
        "print(\"The labels are also permuted (not the original MNIST labels).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b6luam88gn",
      "metadata": {
        "id": "6b6luam88gn"
      },
      "source": [
        "## 5. Baseline: Random Agent\n",
        "\n",
        "First, let's establish a baseline with an agent that makes random predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "lucs492dcp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lucs492dcp",
        "outputId": "9180d342-3eae-402c-8ee4-b0a6eb8f0c1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Random Agent (Baseline)\n",
            "==================================================\n",
            "Task 1: Accuracy = 9.96%, Time = 0.0003s\n",
            "Task 2: Accuracy = 9.70%, Time = 0.0003s\n",
            "Task 3: Accuracy = 10.41%, Time = 0.0003s\n",
            "Task 4: Accuracy = 10.02%, Time = 0.0003s\n",
            "Task 5: Accuracy = 10.23%, Time = 0.0003s\n",
            "Task 6: Accuracy = 9.94%, Time = 0.0003s\n",
            "Task 7: Accuracy = 10.29%, Time = 0.0003s\n",
            "Task 8: Accuracy = 10.27%, Time = 0.0002s\n",
            "Task 9: Accuracy = 9.93%, Time = 0.0003s\n",
            "Task 10: Accuracy = 10.09%, Time = 0.0003s\n",
            "\n",
            "Random Agent Summary:\n",
            "  Mean accuracy: 10.08% ± 0.20%\n",
            "  Total time: 0.00s\n"
          ]
        }
      ],
      "source": [
        "# Reset environment for fresh start\n",
        "env.reset()\n",
        "env.set_seed(42)\n",
        "\n",
        "# Create random agent\n",
        "random_agent = RandomAgent(output_dim=10, seed=42)\n",
        "\n",
        "# Track performance\n",
        "random_accuracies = []\n",
        "random_times = []\n",
        "\n",
        "print(\"Evaluating Random Agent (Baseline)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Evaluate on all tasks\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # \"Train\" (random agent doesn't actually learn)\n",
        "    random_agent.train(task['X_train'], task['y_train'])\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = random_agent.predict(task['X_test'])\n",
        "\n",
        "    # Calculate time and accuracy\n",
        "    elapsed_time = time.time() - start_time\n",
        "    accuracy = env.evaluate(predictions, task['y_test'])\n",
        "\n",
        "    random_accuracies.append(accuracy)\n",
        "    random_times.append(elapsed_time)\n",
        "\n",
        "    print(f\"Task {task_num}: Accuracy = {accuracy:.2%}, Time = {elapsed_time:.4f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nRandom Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(random_accuracies):.2%} ± {np.std(random_accuracies):.2%}\")\n",
        "print(f\"  Total time: {np.sum(random_times):.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y1zoqupvez",
      "metadata": {
        "id": "y1zoqupvez"
      },
      "source": [
        "## 6. Linear Agent\n",
        "\n",
        "Now let's train a simple linear classifier that actually learns from the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lf16ji120qa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf16ji120qa",
        "outputId": "cb2a7d88-554b-49d4-d6e2-ce1cfa6cc9a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Linear Agent\n",
            "==================================================\n",
            "Task 1: Accuracy = 90.98%, Time = 2.81s\n",
            "Task 2: Accuracy = 90.84%, Time = 2.55s\n",
            "Task 3: Accuracy = 90.91%, Time = 2.54s\n",
            "Task 4: Accuracy = 90.81%, Time = 2.37s\n",
            "Task 5: Accuracy = 90.90%, Time = 2.72s\n",
            "Task 6: Accuracy = 90.90%, Time = 2.49s\n",
            "Task 7: Accuracy = 90.81%, Time = 2.41s\n",
            "Task 8: Accuracy = 90.76%, Time = 2.38s\n",
            "Task 9: Accuracy = 90.83%, Time = 2.76s\n",
            "Task 10: Accuracy = 90.85%, Time = 2.41s\n",
            "\n",
            "Linear Agent Summary:\n",
            "  Mean accuracy: 90.86% ± 0.06%\n",
            "  Total time: 25.44s\n"
          ]
        }
      ],
      "source": [
        "# Reset environment\n",
        "env.reset()\n",
        "env.set_seed(42)\n",
        "\n",
        "# Create linear agent\n",
        "linear_agent = LinearAgent(input_dim=784, output_dim=10, learning_rate=0.01)\n",
        "\n",
        "# Track performance\n",
        "linear_accuracies = []\n",
        "linear_times = []\n",
        "\n",
        "print(\"Evaluating Linear Agent\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Evaluate on all tasks\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    # Reset agent for new task\n",
        "    linear_agent.reset()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the linear model\n",
        "    linear_agent.train(task['X_train'], task['y_train'], epochs=5, batch_size=32)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = linear_agent.predict(task['X_test'])\n",
        "\n",
        "    # Calculate time and accuracy\n",
        "    elapsed_time = time.time() - start_time\n",
        "    accuracy = env.evaluate(predictions, task['y_test'])\n",
        "\n",
        "    linear_accuracies.append(accuracy)\n",
        "    linear_times.append(elapsed_time)\n",
        "\n",
        "    print(f\"Task {task_num}: Accuracy = {accuracy:.2%}, Time = {elapsed_time:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nLinear Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(linear_accuracies):.2%} ± {np.std(linear_accuracies):.2%}\")\n",
        "print(f\"  Total time: {np.sum(linear_times):.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uPUx972-JR9b",
      "metadata": {
        "id": "uPUx972-JR9b"
      },
      "source": [
        "Build a deep model capable of truly learning nonlinear structures.\n",
        "Compared with the previous linear classifier, this model:\n",
        "\n",
        "Contains at least one hidden layer (for example: 784 → 256 → 10);\n",
        "\n",
        "Uses activation functions (such as ReLU or Tanh) to introduce nonlinearity;\n",
        "\n",
        "Is retrained and evaluated on each permuted task independently.\n",
        "\n",
        "👇it takes too long to achive 98%. So i stopped it in the middle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AmYncvF4Rn2O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AmYncvF4Rn2O",
        "outputId": "f6130346-8690-4b39-8434-918511d9860f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating TorchMLP Agent\n",
            "==================================================\n",
            "epoch 0: 0.9623%\n",
            "epoch 1: 0.9715%\n",
            "epoch 2: 0.9735%\n",
            "epoch 3: 0.9763%\n",
            "epoch 4: 0.9776%\n",
            "epoch 5: 0.9781%\n",
            "epoch 6: 0.9812%\n",
            "epoch 7: 0.9777%\n",
            "epoch 8: 0.9816%\n",
            "epoch 9: 0.9832%\n",
            "Task 1: Accuracy = 98.02%, Time = 155.17s\n",
            "epoch 0: 0.9636%\n",
            "epoch 1: 0.9732%\n",
            "epoch 2: 0.9742%\n",
            "epoch 3: 0.9779%\n",
            "epoch 4: 0.9813%\n",
            "epoch 5: 0.9793%\n",
            "epoch 6: 0.9802%\n",
            "epoch 7: 0.9817%\n",
            "epoch 8: 0.9801%\n",
            "epoch 9: 0.9817%\n",
            "Task 2: Accuracy = 98.18%, Time = 150.47s\n",
            "epoch 0: 0.9665%\n",
            "epoch 1: 0.9719%\n",
            "epoch 2: 0.9732%\n",
            "epoch 3: 0.9742%\n",
            "epoch 4: 0.9782%\n",
            "epoch 5: 0.9769%\n",
            "epoch 6: 0.9764%\n",
            "epoch 7: 0.9810%\n",
            "epoch 8: 0.9779%\n",
            "epoch 9: 0.9811%\n",
            "Task 3: Accuracy = 98.05%, Time = 151.65s\n",
            "epoch 0: 0.9617%\n",
            "epoch 1: 0.9732%\n",
            "epoch 2: 0.9751%\n",
            "epoch 3: 0.9774%\n",
            "epoch 4: 0.9782%\n",
            "epoch 5: 0.9773%\n",
            "epoch 6: 0.9821%\n",
            "epoch 7: 0.9829%\n",
            "epoch 8: 0.9834%\n",
            "epoch 9: 0.9806%\n",
            "Task 4: Accuracy = 98.07%, Time = 147.96s\n",
            "epoch 0: 0.9576%\n",
            "epoch 1: 0.9756%\n",
            "epoch 2: 0.9762%\n",
            "epoch 3: 0.9770%\n",
            "epoch 4: 0.9807%\n",
            "epoch 5: 0.9776%\n",
            "epoch 6: 0.9808%\n",
            "epoch 7: 0.9821%\n",
            "epoch 8: 0.9795%\n",
            "epoch 9: 0.9826%\n",
            "Task 5: Accuracy = 98.19%, Time = 151.91s\n",
            "epoch 0: 0.9618%\n",
            "epoch 1: 0.9712%\n",
            "epoch 2: 0.9748%\n",
            "epoch 3: 0.9753%\n",
            "epoch 4: 0.9750%\n",
            "epoch 5: 0.9770%\n",
            "epoch 6: 0.9792%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3403513514.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Train the linear model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtorchmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/permuted_mnist/permuted_mnist/agent/torch_mlp/agent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/permuted_mnist/permuted_mnist/agent/torch_mlp/agent.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Reset environment\n",
        "env.reset()\n",
        "env.set_seed(42)\n",
        "\n",
        "# Create linear agent\n",
        "torchmlp = TorchMLP(output_dim=10,seed=42)\n",
        "\n",
        "# Track performance\n",
        "torchmlp_accuracies = []\n",
        "torchmlp_times = []\n",
        "\n",
        "print(\"Evaluating TorchMLP Agent\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Evaluate on all tasks\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    # Reset agent for new task\n",
        "    torchmlp.reset()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the linear model\n",
        "    torchmlp.train(task['X_train'], task['y_train'])\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = torchmlp.predict(task['X_test'])\n",
        "\n",
        "    # Calculate time and accuracy\n",
        "    elapsed_time = time.time() - start_time\n",
        "    accuracy = env.evaluate(predictions, task['y_test'])\n",
        "\n",
        "    torchmlp_accuracies.append(accuracy)\n",
        "    torchmlp_times.append(elapsed_time)\n",
        "\n",
        "    print(f\"Task {task_num}: Accuracy = {accuracy:.2%}, Time = {elapsed_time:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(torchmlp_accuracies):.2%} ± {np.std(torchmlp_accuracies):.2%}\")\n",
        "print(f\"  Total time: {np.sum(torchmlp_times):.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the above version took so long to generate, so it occured me to try early stopping."
      ],
      "metadata": {
        "id": "8imRO90yD7NW"
      },
      "id": "8imRO90yD7NW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lzWLcFX5YPTD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzWLcFX5YPTD",
        "outputId": "0c301820-3cea-43bb-f5a3-b7baabbeb58f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating TorchMLP Agent\n",
            "==================================================\n",
            "Task 1: Accuracy = 94.90%, Time = 5.73s\n",
            "Task 2: Accuracy = 94.73%, Time = 5.72s\n",
            "Task 3: Accuracy = 94.71%, Time = 5.94s\n",
            "Task 4: Accuracy = 94.86%, Time = 5.73s\n",
            "Task 5: Accuracy = 95.21%, Time = 5.80s\n",
            "Task 6: Accuracy = 95.36%, Time = 5.79s\n",
            "Task 7: Accuracy = 94.70%, Time = 5.83s\n",
            "Task 8: Accuracy = 94.80%, Time = 5.73s\n",
            "Task 9: Accuracy = 95.03%, Time = 5.72s\n",
            "Task 10: Accuracy = 94.72%, Time = 5.76s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 94.90% ± 0.22%\n",
            "  Total time: 57.74s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP (speed profile with early stopping) =====\n",
        "import time, math, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _flatten_np(X):\n",
        "    return X.reshape(X.shape[0], -1).astype(np.float32, copy=False)\n",
        "\n",
        "def _labels_1d(y):\n",
        "    y = np.asarray(y)\n",
        "    return y.reshape(-1).astype(np.int64, copy=False)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _scalar_norm_fit(X):\n",
        "    m = float(X.mean()); s = float(X.std())\n",
        "    if s < 1e-6: s = 1.0\n",
        "    return m, s\n",
        "\n",
        "class _SpeedMLP(nn.Module):\n",
        "    def __init__(self, in_dim=784, h1=512, h2=256, out_dim=10, p=0.10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h1), nn.ReLU(), nn.Dropout(p),\n",
        "            nn.Linear(h1, h2),     nn.ReLU(), nn.Dropout(p),\n",
        "            nn.Linear(h2, out_dim)\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    def __init__(self, output_dim=10, seed=42,\n",
        "                 time_budget=5.5, subsample_n=20000,\n",
        "                 val_ratio=0.1, batch_size=2048,\n",
        "                 lr=3e-3, weight_decay=1e-4,\n",
        "                 patience=2, min_delta=0.001, min_train_seconds=2.5):\n",
        "        self.output_dim = output_dim\n",
        "        self.seed = seed\n",
        "        self.time_budget = float(time_budget)\n",
        "        self.subsample_n = int(subsample_n)\n",
        "        self.val_ratio = float(val_ratio)\n",
        "        self.batch_size = int(batch_size)\n",
        "        self.lr = float(lr)\n",
        "        self.weight_decay = float(weight_decay)\n",
        "        self.patience = int(patience)\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.min_train_seconds = float(min_train_seconds)\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        self.model = None\n",
        "        self.mean_, self.std_ = 0.0, 1.0\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        np.random.seed(self.seed); torch.manual_seed(self.seed)\n",
        "        self.model = _SpeedMLP(out_dim=self.output_dim).to(self.device)\n",
        "        self.mean_, self.std_ = 0.0, 1.0\n",
        "\n",
        "    def _make_loader(self, X, y, bs, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=bs, shuffle=shuffle,\n",
        "                                           drop_last=False, num_workers=0, pin_memory=False)\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        t0 = time.time()\n",
        "        Xf = _flatten_np(X_train)\n",
        "        y = _labels_1d(y_train)\n",
        "        N = Xf.shape[0]\n",
        "        ss = min(self.subsample_n, N)\n",
        "        idx = np.random.default_rng(self.seed).choice(N, ss, replace=False)\n",
        "        Xs = torch.from_numpy(Xf[idx]).float().to(self.device)\n",
        "        ys = torch.from_numpy(y[idx]).to(self.device)\n",
        "\n",
        "        n_val = max(1000, int(len(Xs) * 0.1))\n",
        "        n_tr  = len(Xs) - n_val\n",
        "        X_tr, X_val = Xs[:n_tr], Xs[n_tr:]\n",
        "        y_tr, y_val = ys[:n_tr], ys[n_tr:]\n",
        "\n",
        "        self.mean_, self.std_ = _scalar_norm_fit(X_tr)\n",
        "        X_tr = (X_tr - self.mean_) / self.std_\n",
        "        X_val = (X_val - self.mean_) / self.std_\n",
        "\n",
        "        opt = optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "        loader = self._make_loader(X_tr, y_tr, self.batch_size, shuffle=True)\n",
        "\n",
        "        best_acc, no_improve = 0.0, 0\n",
        "        self.model.train()\n",
        "        while True:\n",
        "            for xb, yb in loader:\n",
        "                if time.time() - t0 > self.time_budget: break\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                loss = crit(self.model(xb), yb)\n",
        "                loss.backward(); opt.step()\n",
        "            if time.time() - t0 > self.time_budget: break\n",
        "\n",
        "            # 验证\n",
        "            with torch.no_grad():\n",
        "                self.model.eval()\n",
        "                bs_eval = 8192\n",
        "                correct, total = 0, 0\n",
        "                for i in range(0, X_val.shape[0], bs_eval):\n",
        "                    logits = self.model(X_val[i:i+bs_eval])\n",
        "                    pred = logits.argmax(dim=1)\n",
        "                    correct += (pred == y_val[i:i+bs_eval]).sum().item()\n",
        "                    total   += min(bs_eval, X_val.shape[0]-i)\n",
        "                val_acc = correct / max(1, total)\n",
        "                self.model.train()\n",
        "\n",
        "            if val_acc > best_acc + self.min_delta:\n",
        "                best_acc = val_acc\n",
        "                no_improve = 0\n",
        "            else:\n",
        "                no_improve += 1\n",
        "\n",
        "            if no_improve >= self.patience and (time.time()-t0) >= self.min_train_seconds:\n",
        "                break\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test):\n",
        "        X = torch.from_numpy(_flatten_np(X_test)).float().to(self.device)\n",
        "        X = (X - self.mean_) / self.std_\n",
        "        self.model.eval()\n",
        "        bs = 8192\n",
        "        out = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            out.append(torch.argmax(self.model(X[i:i+bs]), dim=1).cpu().numpy())\n",
        "        return np.concatenate(out, axis=0)\n",
        "# Reset environment\n",
        "env.reset()\n",
        "env.set_seed(42)\n",
        "\n",
        "# Create agent\n",
        "torchmlp = TorchMLP(output_dim=10, seed=42)\n",
        "\n",
        "# Track performance\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating TorchMLP Agent\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    torchmlp.reset()\n",
        "    start = time.time()\n",
        "    torchmlp.train(task['X_train'], task['y_train'])\n",
        "    preds = torchmlp.predict(task['X_test'])\n",
        "    elapsed = time.time() - start\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J9tMuyVPfi-6",
      "metadata": {
        "id": "J9tMuyVPfi-6"
      },
      "source": [
        "#TorchMLP with BN ( i tried some other staff as well but this one worked the best so i kept it )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QS_5kU4_Yeou",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS_5kU4_Yeou",
        "outputId": "49bfbf28-959c-4ecf-f11d-f5262608ea6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating TorchMLP Agent (BN, 2x100)\n",
            "==================================================\n",
            "Task 1: Accuracy = 97.26%, Time = 6.70s\n",
            "Task 2: Accuracy = 97.25%, Time = 6.09s\n",
            "Task 3: Accuracy = 97.25%, Time = 5.44s\n",
            "Task 4: Accuracy = 97.19%, Time = 5.62s\n",
            "Task 5: Accuracy = 96.93%, Time = 6.29s\n",
            "Task 6: Accuracy = 97.50%, Time = 5.83s\n",
            "Task 7: Accuracy = 97.10%, Time = 5.33s\n",
            "Task 8: Accuracy = 97.35%, Time = 5.84s\n",
            "Task 9: Accuracy = 97.31%, Time = 6.16s\n",
            "Task 10: Accuracy = 97.15%, Time = 5.68s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 97.23% ± 0.15%\n",
            "  Total time: 58.98s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP-BN (2x100, BN, ReLU; Adam lr=1e-3; bs=128; epochs=3; val=0.2) =====\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 限制CPU线程（贴合评测约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,28,28)/(N,784) np.uint8/float -> torch.float32 in [0,1]\"\"\"\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    # 如果数据是 0-255 则缩放到 0-1；否则保持原样（避免重复除法）\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,) or (N,1) -> (N,) int64\"\"\"\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"784 -> 100 -> 100 -> 10 with BatchNorm + ReLU\"\"\"\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h, out_dim),\n",
        "        )\n",
        "        # 轻量初始化\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 3:           # (N,28,28)\n",
        "            x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    Torch MLP with BatchNorm (参数按你的要求)：\n",
        "      - 两个隐藏层各 100 单元，ReLU + BatchNorm\n",
        "      - Adam(lr=1e-3)，batch_size=128，epochs=3\n",
        "      - 验证集比例 0.2（用于监控）\n",
        "      - 输入归一化到 [0,1]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dim: int = 10, seed: int = 42,\n",
        "                 epochs: int = 3, batch_size: int = 128,\n",
        "                 lr: float = 1e-3, val_ratio: float = 0.2, weight_decay: float = 0.0):\n",
        "        self.output_dim   = output_dim\n",
        "        self.epochs       = int(epochs)\n",
        "        self.batch_size   = int(batch_size)\n",
        "        self.lr           = float(lr)\n",
        "        self.val_ratio    = float(val_ratio)\n",
        "        self.weight_decay = float(weight_decay)\n",
        "        self.device       = torch.device(\"cpu\")\n",
        "\n",
        "        # 固定随机种子\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "\n",
        "        self.model = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_BN(in_dim=784, h=100, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X: torch.Tensor, y: torch.Tensor, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        # 数据 -> Tensor\n",
        "        X = _as_float_01(X_train)\n",
        "        y = _labels_1d(y_train)\n",
        "\n",
        "        # 划分验证集\n",
        "        n_total = X.shape[0]\n",
        "        n_val   = int(self.val_ratio * n_total)\n",
        "        idx     = np.arange(n_total)\n",
        "        np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]\n",
        "        tr_idx  = idx[n_val:]\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for ep in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "            # 可选：最后一轮打印一次验证准确率（不影响速度/结果）\n",
        "            # if ep == self.epochs - 1:\n",
        "            #     self.model.eval()\n",
        "            #     with torch.no_grad():\n",
        "            #         pred = self.model(X_val).argmax(dim=1)\n",
        "            #         acc = (pred == y_val).float().mean().item()\n",
        "            #     self.model.train()\n",
        "            #     print(f\"[val] epoch {ep}: acc={acc:.4f}\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        self.model.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "# ===== Evaluate TorchMLP-BN (fixed) =====\n",
        "import time, numpy as np\n",
        "\n",
        "env.reset()\n",
        "env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []   # ← 这里要两个空列表\n",
        "\n",
        "print(\"Evaluating TorchMLP Agent (BN, 2x100)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GsRXr2K_Qy7x",
      "metadata": {
        "id": "GsRXr2K_Qy7x"
      },
      "source": [
        "#TorchMLP　with BN and L２ normalization as FE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-HPVI_B8M9is",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HPVI_B8M9is",
        "outputId": "852199c9-0ad3-489b-c79c-871af9623b7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating TorchMLP Agent (BN, 2x100) with Per-Sample L2 Normalization\n",
            "==================================================\n",
            "Task 1: Accuracy = 97.41%, Time = 5.68s\n",
            "Task 2: Accuracy = 97.34%, Time = 5.50s\n",
            "Task 3: Accuracy = 97.35%, Time = 5.85s\n",
            "Task 4: Accuracy = 97.46%, Time = 7.38s\n",
            "Task 5: Accuracy = 97.15%, Time = 6.18s\n",
            "Task 6: Accuracy = 97.37%, Time = 5.32s\n",
            "Task 7: Accuracy = 97.15%, Time = 5.56s\n",
            "Task 8: Accuracy = 97.27%, Time = 6.30s\n",
            "Task 9: Accuracy = 97.33%, Time = 6.26s\n",
            "Task 10: Accuracy = 97.33%, Time = 5.45s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 97.32% ± 0.10%\n",
            "  Total time: 59.47s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP-BN (2x100, BN, ReLU; Adam lr=1e-3; bs=128; epochs=3; val=0.2) + FE: per-sample L2 normalization =====\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 限制CPU线程（贴合评测约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,28,28)/(N,784) np.uint8/float -> torch.float32 in [0,1]\"\"\"\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,) or (N,1) -> (N,) int64\"\"\"\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# ------------ FE: per-sample L2 normalization ------------\n",
        "def _l2_per_sample(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    对每一张图做 L2 归一化；返回与输入相同形状。\n",
        "    只做输入侧FE，其他训练/结构/超参一律不变。\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "    if x.dim() == 3:  # (N,28,28) -> (N,784)\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig_shape) == 3:\n",
        "        x = x.view(orig_shape)\n",
        "    return x\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"784 -> 100 -> 100 -> 10 with BatchNorm + ReLU\"\"\"\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h, out_dim),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 3:           # (N,28,28)\n",
        "            x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    Torch MLP with BatchNorm（参数完全不变） + FE：\n",
        "      * 每张图做 L2 归一化；\n",
        "      * 训练和预测都使用同样的 FE；\n",
        "      * 其它超参/结构/循环不变。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dim: int = 10, seed: int = 42,\n",
        "                 epochs: int = 3, batch_size: int = 128,\n",
        "                 lr: float = 1e-3, val_ratio: float = 0.2, weight_decay: float = 0.0):\n",
        "        self.output_dim   = output_dim\n",
        "        self.epochs       = int(epochs)\n",
        "        self.batch_size   = int(batch_size)\n",
        "        self.lr           = float(lr)\n",
        "        self.val_ratio    = float(val_ratio)\n",
        "        self.weight_decay = float(weight_decay)\n",
        "        self.device       = torch.device(\"cpu\")\n",
        "\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_BN(in_dim=784, h=100, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X: torch.Tensor, y: torch.Tensor, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    # === 训练/预测逻辑保持不变，仅在输入端加入 per-sample L2 FE ===\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        X = _as_float_01(X_train)\n",
        "        y = _labels_1d(y_train)\n",
        "\n",
        "        n_total = X.shape[0]\n",
        "        n_val   = int(self.val_ratio * n_total)\n",
        "        idx     = np.arange(n_total)\n",
        "        np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]\n",
        "        tr_idx  = idx[n_val:]\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # ---- FE: 每样本 L2 归一化（仅此改动）----\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for ep in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        # ---- FE: 同样做每样本 L2 归一化（仅此改动）----\n",
        "        X = _l2_per_sample(X)\n",
        "        self.model.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ===== Evaluate TorchMLP-BN (same params, per-sample L2 FE) =====\n",
        "import time, numpy as np\n",
        "\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating TorchMLP Agent (BN, 2x100) with Per-Sample L2 Normalization\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yDLV-riaWvW-",
      "metadata": {
        "id": "yDLV-riaWvW-"
      },
      "source": [
        "#TorchMLP BN ＋ L2 normalization + hidden(256,128) wide first and then narrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UDx_2M-6WaXz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDx_2M-6WaXz",
        "outputId": "3009f064-e299-462d-bdf2-ec10778d0e0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating TorchMLP Agent (BN, 256→128 pyramid) with Per-Sample L2 Normalization\n",
            "==================================================\n",
            "Task 1: Accuracy = 97.59%, Time = 8.12s\n",
            "Task 2: Accuracy = 97.38%, Time = 7.74s\n",
            "Task 3: Accuracy = 97.57%, Time = 7.81s\n",
            "Task 4: Accuracy = 97.58%, Time = 7.46s\n",
            "Task 5: Accuracy = 97.43%, Time = 7.90s\n",
            "Task 6: Accuracy = 97.66%, Time = 8.18s\n",
            "Task 7: Accuracy = 97.49%, Time = 8.57s\n",
            "Task 8: Accuracy = 97.59%, Time = 8.30s\n",
            "Task 9: Accuracy = 97.59%, Time = 11.83s\n",
            "Task 10: Accuracy = 97.69%, Time = 9.98s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 97.56% ± 0.09%\n",
            "  Total time: 85.89s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP-BN (256→128 pyramid, BN, ReLU; Adam lr=1e-3; bs=128; epochs=3; val=0.2) + FE: per-sample L2 normalization =====\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 限制CPU线程（贴合评测约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,28,28)/(N,784) np.uint8/float -> torch.float32 in [0,1]\"\"\"\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,) or (N,1) -> (N,) int64\"\"\"\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# ------------ FE: per-sample L2 normalization ------------\n",
        "def _l2_per_sample(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    对每一张图做 L2 归一化；返回与输入相同形状。\n",
        "    只做输入侧FE，其他训练/结构/超参一律不变。\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "    if x.dim() == 3:  # (N,28,28) -> (N,784)\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig_shape) == 3:\n",
        "        x = x.view(orig_shape)\n",
        "    return x\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"\n",
        "    方案B：金字塔宽度 784 -> 256 -> 128 -> 10，层内 BN + ReLU；\n",
        "    为了与外部调用保持完全兼容，保留参数签名(in_dim, h, out_dim)但忽略 h。\n",
        "    隐藏层用 bias=False（配合BN），输出层 bias=True；Xavier 初始化，bias=0。\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10):\n",
        "        super().__init__()\n",
        "        h1, h2 = 256, 128\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h1, bias=False),\n",
        "            nn.BatchNorm1d(h1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(h1, h2, bias=False),\n",
        "            nn.BatchNorm1d(h2),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(h2, out_dim, bias=True),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 3:           # (N,28,28)\n",
        "            x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    Torch MLP with BatchNorm（参数完全不变） + FE：\n",
        "      * 每张图做 L2 归一化；\n",
        "      * 训练和预测都使用同样的 FE；\n",
        "      * 其它超参/结构/循环不变。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dim: int = 10, seed: int = 42,\n",
        "                 epochs: int = 3, batch_size: int = 128,\n",
        "                 lr: float = 1e-3, val_ratio: float = 0.2, weight_decay: float = 0.0):\n",
        "        self.output_dim   = output_dim\n",
        "        self.epochs       = int(epochs)\n",
        "        self.batch_size   = int(batch_size)\n",
        "        self.lr           = float(lr)\n",
        "        self.val_ratio    = float(val_ratio)\n",
        "        self.weight_decay = float(weight_decay)\n",
        "        self.device       = torch.device(\"cpu\")\n",
        "\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # 保持外部调用格式不变：仍然传 h=100；内部模型忽略 h，固定为 256→128\n",
        "        self.model = _MLP_BN(in_dim=784, h=100, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X: torch.Tensor, y: torch.Tensor, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    # === 训练/预测逻辑保持不变，仅在输入端加入 per-sample L2 FE ===\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        X = _as_float_01(X_train)\n",
        "        y = _labels_1d(y_train)\n",
        "\n",
        "        n_total = X.shape[0]\n",
        "        n_val   = int(self.val_ratio * n_total)\n",
        "        idx     = np.arange(n_total)\n",
        "        np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]\n",
        "        tr_idx  = idx[n_val:]\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # ---- FE: 每样本 L2 归一化（仅此改动）----\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for ep in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        # ---- FE: 同样做每样本 L2 归一化（仅此改动）----\n",
        "        X = _l2_per_sample(X)\n",
        "        self.model.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ===== Evaluate TorchMLP-BN (same params, per-sample L2 FE) =====\n",
        "import time, numpy as np\n",
        "\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating TorchMLP Agent (BN, 256→128 pyramid) with Per-Sample L2 Normalization\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yXeK4nqWXc-Y",
      "metadata": {
        "id": "yXeK4nqWXc-Y"
      },
      "source": [
        "#TorchMLP BN + L2 normalization + first layer SiLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VnzpA4XWX4O4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnzpA4XWX4O4",
        "outputId": "8ef54fcc-b8f4-4597-ad1e-6b14b373fb2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating TorchMLP Agent (BN, 100→100; first SiLU then ReLU) with Per-Sample L2 Normalization\n",
            "==================================================\n",
            "Task 1: Accuracy = 97.42%, Time = 6.03s\n",
            "Task 2: Accuracy = 97.27%, Time = 5.29s\n",
            "Task 3: Accuracy = 97.25%, Time = 6.32s\n",
            "Task 4: Accuracy = 97.33%, Time = 7.10s\n",
            "Task 5: Accuracy = 97.24%, Time = 6.10s\n",
            "Task 6: Accuracy = 97.27%, Time = 5.52s\n",
            "Task 7: Accuracy = 97.19%, Time = 5.08s\n",
            "Task 8: Accuracy = 97.41%, Time = 5.71s\n",
            "Task 9: Accuracy = 97.34%, Time = 5.88s\n",
            "Task 10: Accuracy = 97.46%, Time = 5.11s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 97.32% ± 0.08%\n",
            "  Total time: 58.14s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP (首层 SiLU，其余 ReLU；Adam lr=1e-3; bs=128; epochs=3; val=0.2) + FE: per-sample L2 normalization =====\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 限制CPU线程（贴合评测约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,28,28)/(N,784) np.uint8/float -> torch.float32 in [0,1]\"\"\"\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,) or (N,1) -> (N,) int64\"\"\"\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# ------------ FE: per-sample L2 normalization ------------\n",
        "def _l2_per_sample(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    对每一张图做 L2 归一化；返回与输入相同形状。\n",
        "    只做输入侧FE，其他训练/结构/超参一律不变。\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "    if x.dim() == 3:  # (N,28,28) -> (N,784)\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig_shape) == 3:\n",
        "        x = x.view(orig_shape)\n",
        "    return x\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"\n",
        "    方案3：结构仍为 784 -> 100 -> 100 -> 10，层内 BN；\n",
        "          首层激活使用 SiLU，第二层保持 ReLU。\n",
        "    隐藏层 bias=False（配合BN），输出层 bias=True；Xavier 初始化 + bias=0。\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h, bias=False),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.SiLU(inplace=True),          # ← 首层激活换为 SiLU\n",
        "\n",
        "            nn.Linear(h, h, bias=False),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),          # ← 第二层保持 ReLU\n",
        "\n",
        "            nn.Linear(h, out_dim, bias=True),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 3:           # (N,28,28)\n",
        "            x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    Torch MLP with BatchNorm（参数完全不变） + FE：\n",
        "      * 每张图做 L2 归一化；\n",
        "      * 训练和预测都使用同样的 FE；\n",
        "      * 其它超参/结构/循环不变。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dim: int = 10, seed: int = 42,\n",
        "                 epochs: int = 3, batch_size: int = 128,\n",
        "                 lr: float = 1e-3, val_ratio: float = 0.2, weight_decay: float = 0.0):\n",
        "        self.output_dim   = output_dim\n",
        "        self.epochs       = int(epochs)\n",
        "        self.batch_size   = int(batch_size)\n",
        "        self.lr           = float(lr)\n",
        "        self.val_ratio    = float(val_ratio)\n",
        "        self.weight_decay = float(weight_decay)\n",
        "        self.device       = torch.device(\"cpu\")\n",
        "\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # 仍然以 h=100 调用，结构内部已按方案3定义\n",
        "        self.model = _MLP_BN(in_dim=784, h=100, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X: torch.Tensor, y: torch.Tensor, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    # === 训练/预测逻辑保持不变，仅在输入端加入 per-sample L2 FE ===\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        X = _as_float_01(X_train)\n",
        "        y = _labels_1d(y_train)\n",
        "\n",
        "        n_total = X.shape[0]\n",
        "        n_val   = int(self.val_ratio * n_total)\n",
        "        idx     = np.arange(n_total)\n",
        "        np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]\n",
        "        tr_idx  = idx[n_val:]\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # ---- FE: 每样本 L2 归一化（仅此改动）----\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for ep in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        # ---- FE: 同样做每样本 L2 归一化（仅此改动）----\n",
        "        X = _l2_per_sample(X)\n",
        "        self.model.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ===== Evaluate TorchMLP (SiLU first layer, ReLU second) with per-sample L2 FE =====\n",
        "import time, numpy as np\n",
        "\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating TorchMLP Agent (BN, 100→100; first SiLU then ReLU) with Per-Sample L2 Normalization\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TorchMLP-Deep (128-128-128; BN+ReLU; Adam lr=1e-3; bs=128; epochs=3; val=0.2 FE: per-sample L2 normalization"
      ],
      "metadata": {
        "id": "8qcMxEp7Fbte"
      },
      "id": "8qcMxEp7Fbte"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lE5ioYC4Ynm2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE5ioYC4Ynm2",
        "outputId": "709253be-241c-4565-b4e9-b0e87d2dbc2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating TorchMLP Agent (Deep 128×3) with Per-Sample L2 Normalization\n",
            "==================================================\n",
            "Task 1: Accuracy = 97.62%, Time = 8.10s\n",
            "Task 2: Accuracy = 97.50%, Time = 6.56s\n",
            "Task 3: Accuracy = 97.34%, Time = 7.27s\n",
            "Task 4: Accuracy = 97.33%, Time = 7.02s\n",
            "Task 5: Accuracy = 97.33%, Time = 8.13s\n",
            "Task 6: Accuracy = 97.29%, Time = 7.94s\n",
            "Task 7: Accuracy = 97.37%, Time = 7.09s\n",
            "Task 8: Accuracy = 97.73%, Time = 6.62s\n",
            "Task 9: Accuracy = 97.41%, Time = 8.41s\n",
            "Task 10: Accuracy = 97.48%, Time = 6.60s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 97.44% ± 0.14%\n",
            "  Total time: 73.74s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP-Deep (128-128-128; BN+ReLU; Adam lr=1e-3; bs=128; epochs=3; val=0.2) + FE: per-sample L2 normalization =====\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 限制CPU线程（贴合评测约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,28,28)/(N,784) np.uint8/float -> torch.float32 in [0,1]\"\"\"\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,) or (N,1) -> (N,) int64\"\"\"\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# ------------ FE: per-sample L2 normalization ------------\n",
        "def _l2_per_sample(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    对每一张图做 L2 归一化；返回与输入相同形状。\n",
        "    只做输入侧FE，其他训练/结构/超参一律不变。\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "    if x.dim() == 3:  # (N,28,28) -> (N,784)\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig_shape) == 3:\n",
        "        x = x.view(orig_shape)\n",
        "    return x\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"\n",
        "    方案4：瘦而稍深 —— 784 -> 128 -> 128 -> 128 -> 10\n",
        "    三个隐藏层均为 BN + ReLU；隐藏层 bias=False（配合BN），输出层 bias=True。\n",
        "    保持与外部相同签名(in_dim, h, out_dim)，但内部固定为 128-128-128。\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10):\n",
        "        super().__init__()\n",
        "        d = 128\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, d, bias=False), nn.BatchNorm1d(d), nn.ReLU(inplace=True),\n",
        "            nn.Linear(d, d, bias=False),      nn.BatchNorm1d(d), nn.ReLU(inplace=True),\n",
        "            nn.Linear(d, d, bias=False),      nn.BatchNorm1d(d), nn.ReLU(inplace=True),\n",
        "            nn.Linear(d, out_dim, bias=True),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 3:           # (N,28,28)\n",
        "            x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    Torch MLP with BatchNorm（参数完全不变） + FE：\n",
        "      * 每张图做 L2 归一化；\n",
        "      * 训练和预测都使用同样的 FE；\n",
        "      * 其它超参/结构/循环不变。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dim: int = 10, seed: int = 42,\n",
        "                 epochs: int = 3, batch_size: int = 128,\n",
        "                 lr: float = 1e-3, val_ratio: float = 0.2, weight_decay: float = 0.0):\n",
        "        self.output_dim   = output_dim\n",
        "        self.epochs       = int(epochs)\n",
        "        self.batch_size   = int(batch_size)\n",
        "        self.lr           = float(lr)\n",
        "        self.val_ratio    = float(val_ratio)\n",
        "        self.weight_decay = float(weight_decay)\n",
        "        self.device       = torch.device(\"cpu\")\n",
        "\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # 保持外部调用格式不变：仍然传 h=100；内部固定为 128-128-128\n",
        "        self.model = _MLP_BN(in_dim=784, h=100, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X: torch.Tensor, y: torch.Tensor, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    # === 训练/预测逻辑保持不变，仅在输入端加入 per-sample L2 FE ===\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        X = _as_float_01(X_train)\n",
        "        y = _labels_1d(y_train)\n",
        "\n",
        "        n_total = X.shape[0]\n",
        "        n_val   = int(self.val_ratio * n_total)\n",
        "        idx     = np.arange(n_total)\n",
        "        np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]\n",
        "        tr_idx  = idx[n_val:]\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # ---- FE: 每样本 L2 归一化（仅此改动）----\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for ep in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        # ---- FE: 同样做每样本 L2 归一化（仅此改动）----\n",
        "        X = _l2_per_sample(X)\n",
        "        self.model.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ===== Evaluate TorchMLP-Deep (same params, per-sample L2 FE) =====\n",
        "import time, numpy as np\n",
        "\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating TorchMLP Agent (Deep 128×3) with Per-Sample L2 Normalization\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TorchMLP-BottleneckRes (784→256 → [Bottleneck 256→64→256] → 128 → 10; Adam lr=1e-3; bs=128; epochs=3; val=0.2) + FE: per-sample L2 normalization"
      ],
      "metadata": {
        "id": "IzVK9ETQFirg"
      },
      "id": "IzVK9ETQFirg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8DcdbR7pY_Rw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DcdbR7pY_Rw",
        "outputId": "3f48ffbf-fe8d-47cf-a14d-b0d4ed2c8a7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating TorchMLP Agent (Bottleneck Residual 256→64→256, then 128) with Per-Sample L2 Normalization\n",
            "==================================================\n",
            "Task 1: Accuracy = 97.63%, Time = 10.09s\n",
            "Task 2: Accuracy = 97.77%, Time = 11.39s\n",
            "Task 3: Accuracy = 97.85%, Time = 11.02s\n",
            "Task 4: Accuracy = 97.77%, Time = 13.79s\n",
            "Task 5: Accuracy = 97.57%, Time = 10.88s\n",
            "Task 6: Accuracy = 97.83%, Time = 10.73s\n",
            "Task 7: Accuracy = 97.80%, Time = 11.75s\n",
            "Task 8: Accuracy = 97.61%, Time = 11.13s\n",
            "Task 9: Accuracy = 97.75%, Time = 11.13s\n",
            "Task 10: Accuracy = 97.73%, Time = 10.94s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 97.73% ± 0.09%\n",
            "  Total time: 112.85s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 限制CPU线程（贴合评测约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,28,28)/(N,784) np.uint8/float -> torch.float32 in [0,1]\"\"\"\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,) or (N,1) -> (N,) int64\"\"\"\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# ------------ FE: per-sample L2 normalization ------------\n",
        "def _l2_per_sample(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    对每一张图做 L2 归一化；返回与输入相同形状。\n",
        "    只做输入侧FE，其他训练/结构/超参一律不变。\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "    if x.dim() == 3:  # (N,28,28) -> (N,784)\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig_shape) == 3:\n",
        "        x = x.view(orig_shape)\n",
        "    return x\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# ===== 方案5：瓶颈残差（更省算力的残差块）=====\n",
        "class _BottleneckBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    预激活瓶颈残差：\n",
        "      BN → SiLU → Linear(d→b, bias=False)\n",
        "      BN → SiLU → Linear(b→d, bias=False)\n",
        "    其中 b << d（本实现 b=64, d=256），计算量更低，3个epoch内更易稳定提升。\n",
        "    \"\"\"\n",
        "    def __init__(self, d: int, b: int):\n",
        "        super().__init__()\n",
        "        self.bn1  = nn.BatchNorm1d(d)\n",
        "        self.act1 = nn.SiLU(inplace=True)\n",
        "        self.fc1  = nn.Linear(d, b, bias=False)\n",
        "\n",
        "        self.bn2  = nn.BatchNorm1d(b)\n",
        "        self.act2 = nn.SiLU(inplace=True)\n",
        "        self.fc2  = nn.Linear(b, d, bias=False)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        h = self.fc1(self.act1(self.bn1(x)))\n",
        "        h = self.fc2(self.act2(self.bn2(h)))\n",
        "        return x + h\n",
        "\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"\n",
        "    为与外部调用保持兼容，保留签名(in_dim, h, out_dim)但内部结构固定：\n",
        "      784 → 256 → [Bottle(256→64→256) × 1] → 128 → 10\n",
        "    隐藏层 bias=False（配合BN），输出层 bias=True；Xavier 初始化 + bias=0。\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10, d1=256, bottleneck=64, d2=128):\n",
        "        super().__init__()\n",
        "        self.inp = nn.Linear(in_dim, d1, bias=False)\n",
        "        self.in_bn = nn.BatchNorm1d(d1)\n",
        "        self.in_act = nn.SiLU(inplace=True)\n",
        "\n",
        "        self.block = _BottleneckBlock(d=d1, b=bottleneck)\n",
        "\n",
        "        self.mid = nn.Sequential(\n",
        "            nn.BatchNorm1d(d1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(d1, d2, bias=False),\n",
        "            nn.BatchNorm1d(d2),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.head = nn.Linear(d2, out_dim, bias=True)\n",
        "\n",
        "        # 初始化\n",
        "        nn.init.xavier_uniform_(self.inp.weight)\n",
        "        nn.init.xavier_uniform_(self.head.weight)\n",
        "        if self.head.bias is not None:\n",
        "            nn.init.zeros_(self.head.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 3:           # (N,28,28)\n",
        "            x = x.view(x.size(0), -1)\n",
        "        h = self.in_act(self.in_bn(self.inp(x)))\n",
        "        h = self.block(h)\n",
        "        h = self.mid(h)\n",
        "        return self.head(h)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    Torch MLP with BatchNorm（参数完全不变） + FE：\n",
        "      * 每张图做 L2 归一化；\n",
        "      * 训练和预测都使用同样的 FE；\n",
        "      * 其它超参/结构/循环不变。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dim: int = 10, seed: int = 42,\n",
        "                 epochs: int = 3, batch_size: int = 128,\n",
        "                 lr: float = 1e-3, val_ratio: float = 0.2, weight_decay: float = 0.0):\n",
        "        self.output_dim   = output_dim\n",
        "        self.epochs       = int(epochs)\n",
        "        self.batch_size   = int(batch_size)\n",
        "        self.lr           = float(lr)\n",
        "        self.val_ratio    = float(val_ratio)\n",
        "        self.weight_decay = float(weight_decay)\n",
        "        self.device       = torch.device(\"cpu\")\n",
        "\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # 仍然以 h=100 调用；内部固定为瓶颈残差结构\n",
        "        self.model = _MLP_BN(in_dim=784, h=100, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X: torch.Tensor, y: torch.Tensor, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    # === 训练/预测逻辑保持不变，仅在输入端加入 per-sample L2 FE ===\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        X = _as_float_01(X_train)\n",
        "        y = _labels_1d(y_train)\n",
        "\n",
        "        n_total = X.shape[0]\n",
        "        n_val   = int(self.val_ratio * n_total)\n",
        "        idx     = np.arange(n_total)\n",
        "        np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]\n",
        "        tr_idx  = idx[n_val:]\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # ---- FE: 每样本 L2 归一化（仅此改动）----\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for ep in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        # ---- FE: 同样做每样本 L2 归一化（仅此改动）----\n",
        "        X = _l2_per_sample(X)\n",
        "        self.model.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ===== Evaluate TorchMLP-BottleneckRes (same params, per-sample L2 FE) =====\n",
        "import time, numpy as np\n",
        "\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating TorchMLP Agent (Bottleneck Residual 256→64→256, then 128) with Per-Sample L2 Normalization\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ===== TorchMLP-DualPath (256→128 + 64；Concat→10; Adam lr=1e-3; bs=128; epochs=3; val=0.2)\n",
        "# + FE: per-sample L2 normalization ====="
      ],
      "metadata": {
        "id": "PLMfXi5rFrmL"
      },
      "id": "PLMfXi5rFrmL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ooBbOXzwZPJ1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooBbOXzwZPJ1",
        "outputId": "25c33ea2-35fa-40e9-bd38-5b7993c54113"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating TorchMLP Agent (DualPath: main 256→128 + skip 64, concat) with Per-Sample L2 Normalization\n",
            "==================================================\n",
            "Task 1: Accuracy = 97.56%, Time = 9.96s\n",
            "Task 2: Accuracy = 97.94%, Time = 9.63s\n",
            "Task 3: Accuracy = 97.68%, Time = 9.55s\n",
            "Task 4: Accuracy = 97.79%, Time = 9.88s\n",
            "Task 5: Accuracy = 97.79%, Time = 9.63s\n",
            "Task 6: Accuracy = 97.85%, Time = 9.87s\n",
            "Task 7: Accuracy = 97.55%, Time = 9.95s\n",
            "Task 8: Accuracy = 97.75%, Time = 10.88s\n",
            "Task 9: Accuracy = 97.63%, Time = 9.83s\n",
            "Task 10: Accuracy = 97.53%, Time = 10.21s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 97.71% ± 0.13%\n",
            "  Total time: 99.39s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 限制CPU线程（贴合评测约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,28,28)/(N,784) np.uint8/float -> torch.float32 in [0,1]\"\"\"\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,) or (N,1) -> (N,) int64\"\"\"\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# ------------ FE: per-sample L2 normalization ------------\n",
        "def _l2_per_sample(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    对每一张图做 L2 归一化；返回与输入相同形状。\n",
        "    只做输入侧FE，其他训练/结构/超参一律不变。\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "    if x.dim() == 3:  # (N,28,28) -> (N,784)\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig_shape) == 3:\n",
        "        x = x.view(orig_shape)\n",
        "    return x\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"\n",
        "    方案6：并联“双路径”：一个主干非线性通道 + 一个浅层线性快路，最后拼接后分类。\n",
        "      - 主干：784 → 256 → 128  （BN+ReLU）\n",
        "      - 快路：784 → 64         （BN+ReLU）   —— 相当于给模型一条“近端”路径捕获线性/弱非线性\n",
        "      - 融合：Concat(128, 64)=192 → 10\n",
        "    隐藏层 bias=False（配合BN），输出层 bias=True；Xavier 初始化 + bias=0。\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10, d1=256, d2=128, d_skip=64):\n",
        "        super().__init__()\n",
        "        # 主干路径\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(in_dim, d1, bias=False), nn.BatchNorm1d(d1), nn.ReLU(inplace=True),\n",
        "            nn.Linear(d1, d2,   bias=False),   nn.BatchNorm1d(d2), nn.ReLU(inplace=True),\n",
        "        )\n",
        "        # 线性快路（浅路径）\n",
        "        self.skip = nn.Sequential(\n",
        "            nn.Linear(in_dim, d_skip, bias=False), nn.BatchNorm1d(d_skip), nn.ReLU(inplace=True),\n",
        "        )\n",
        "        # 融合后的分类头\n",
        "        self.head = nn.Linear(d2 + d_skip, out_dim, bias=True)\n",
        "\n",
        "        # 初始化\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 3:           # (N,28,28)\n",
        "            x = x.view(x.size(0), -1)\n",
        "        h_main = self.main(x)\n",
        "        h_skip = self.skip(x)\n",
        "        h = torch.cat([h_main, h_skip], dim=1)\n",
        "        return self.head(h)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    Torch MLP with BatchNorm（参数完全不变） + FE：\n",
        "      * 每张图做 L2 归一化；\n",
        "      * 训练和预测都使用同样的 FE；\n",
        "      * 其它超参/结构/循环不变。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dim: int = 10, seed: int = 42,\n",
        "                 epochs: int = 3, batch_size: int = 128,\n",
        "                 lr: float = 1e-3, val_ratio: float = 0.2, weight_decay: float = 0.0):\n",
        "        self.output_dim   = output_dim\n",
        "        self.epochs       = int(epochs)\n",
        "        self.batch_size   = int(batch_size)\n",
        "        self.lr           = float(lr)\n",
        "        self.val_ratio    = float(val_ratio)\n",
        "        self.weight_decay = float(weight_decay)\n",
        "        self.device       = torch.device(\"cpu\")\n",
        "\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # 保持外部调用签名一致（h=100）；内部采用双路径结构\n",
        "        self.model = _MLP_BN(in_dim=784, h=100, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X: torch.Tensor, y: torch.Tensor, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    # === 训练/预测逻辑保持不变，仅在输入端加入 per-sample L2 FE ===\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        X = _as_float_01(X_train)\n",
        "        y = _labels_1d(y_train)\n",
        "\n",
        "        n_total = X.shape[0]\n",
        "        n_val   = int(self.val_ratio * n_total)\n",
        "        idx     = np.arange(n_total)\n",
        "        np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]\n",
        "        tr_idx  = idx[n_val:]\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # ---- FE: 每样本 L2 归一化（仅此改动）----\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for ep in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        # ---- FE: 同样做每样本 L2 归一化（仅此改动）----\n",
        "        X = _l2_per_sample(X)\n",
        "        self.model.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ===== Evaluate TorchMLP-DualPath (same params, per-sample L2 FE) =====\n",
        "import time, numpy as np\n",
        "\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating TorchMLP Agent (DualPath: main 256→128 + skip 64, concat) with Per-Sample L2 Normalization\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mj_8RktPat_n",
      "metadata": {
        "id": "mj_8RktPat_n"
      },
      "source": [
        "#　TorchMLM＋BN＋L２normalization＋optimisor（tried and find out ｒｍｓｐｒｏｐ+warmcosine＞ａｄａｍ＞ｓｇｄ）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gR8FOeg5flLD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gR8FOeg5flLD",
        "outputId": "78925070-7d04-4d47-e5ca-0f96234b95f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating TorchMLP (RMSprop + Cosine, 256→128) with Per-Sample L2 Normalization\n",
            "==================================================\n",
            "Task 1: Accuracy = 98.05%, Time = 7.75s\n",
            "Task 2: Accuracy = 98.11%, Time = 8.03s\n",
            "Task 3: Accuracy = 97.94%, Time = 8.53s\n",
            "Task 4: Accuracy = 98.03%, Time = 7.86s\n",
            "Task 5: Accuracy = 98.15%, Time = 8.10s\n",
            "Task 6: Accuracy = 98.08%, Time = 7.85s\n",
            "Task 7: Accuracy = 98.05%, Time = 7.86s\n",
            "Task 8: Accuracy = 97.92%, Time = 7.63s\n",
            "Task 9: Accuracy = 98.06%, Time = 7.35s\n",
            "Task 10: Accuracy = 98.09%, Time = 6.86s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 98.05% ± 0.07%\n",
            "  Total time: 77.83s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP-RMSprop + Cosine (与两优化器脚本中 RMSprop 分支等价) =====\n",
        "# 结构/超参/FE 均与原脚本一致：lr=1e-3, bs=128, epochs=3, val=0.2, per-sample L2\n",
        "import time, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# 限制CPU线程（贴合评测约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np):\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0: x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np):\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# ------------ FE: per-sample L2 normalization ------------\n",
        "def _l2_per_sample(x, eps=1e-6):\n",
        "    orig = x.shape\n",
        "    if x.dim()==3: x = x.view(x.size(0), -1)  # (N,28,28)->(N,784)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig)==3: x = x.view(orig)\n",
        "    return x\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# === 模型：金字塔 256→128（BN+ReLU），与对比脚本相同 ===\n",
        "class _MLP_BN(nn.Module):\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10):\n",
        "        super().__init__()\n",
        "        h1, h2 = 256, 128\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h1, bias=False), nn.BatchNorm1d(h1), nn.ReLU(inplace=True),\n",
        "            nn.Linear(h1, h2, bias=False),     nn.BatchNorm1d(h2), nn.ReLU(inplace=True),\n",
        "            nn.Linear(h2, out_dim, bias=True),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim()==3: x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    固定 RMSprop（alpha=0.99, momentum=0.0, centered=False）+ 余弦退火（与对比脚本一致）\n",
        "    其它保持：epochs=3, bs=128, lr=1e-3, val_ratio=0.2, FE=per-sample L2\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim=10, seed=42, epochs=3, batch_size=128, lr=1e-3,\n",
        "                 val_ratio=0.2, weight_decay=0.0, use_cosine=True):\n",
        "        self.output_dim=output_dim; self.epochs=epochs; self.batch_size=batch_size\n",
        "        self.lr=lr; self.val_ratio=val_ratio; self.weight_decay=weight_decay\n",
        "        self.use_cosine=use_cosine\n",
        "        self.device=torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model=None; self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_BN(in_dim=784, h=100, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X, y, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X = _as_float_01(X_train); y = _labels_1d(y_train)\n",
        "        n_total = X.shape[0]; n_val = int(self.val_ratio * n_total)\n",
        "        idx = np.arange(n_total); np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]; tr_idx = idx[n_val:]\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # FE：每样本 L2（训练/验证一致）\n",
        "        X_tr = _l2_per_sample(X_tr); X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "\n",
        "        # === RMSprop 与对比脚本一致 ===\n",
        "        opt = optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99,\n",
        "                            momentum=0.0, centered=False, weight_decay=self.weight_decay)\n",
        "        sch = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=self.epochs, eta_min=0.0) if self.use_cosine else None\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for _ in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            if sch is not None: sch.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test):\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        X = _l2_per_sample(X)\n",
        "        self.model.eval()\n",
        "        bs=4096; outs=[]\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ===== Evaluate: 单独 RMSprop（与对比脚本 RMSprop 分支完全一致）=====\n",
        "import time, numpy as np\n",
        "\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2,\n",
        "                 use_cosine=True)  # 与两优化器脚本里的 RMSprop=True 对齐\n",
        "\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating TorchMLP (RMSprop + Cosine, 256→128) with Per-Sample L2 Normalization\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None: break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VshOu8SynmDS",
      "metadata": {
        "id": "VshOu8SynmDS"
      },
      "source": [
        "#　TorchMLM＋L２normalization＋rmsprop+BN+different epoch（epoch5，7，10，batchsize 100 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cZae4IQ1nhg_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cZae4IQ1nhg_",
        "outputId": "0d7e2646-54fd-426b-9f85-d10a3c3f9292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Grid Search (RMSprop + FE + Dynamic INT8 inference) ===\n",
            "\n",
            "---- Config: epochs=5, hidden=(256,128), batch_size=128 ----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-941692077.py:109: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  self.model_int8 = tq.quantize_dynamic(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task 1: acc=98.20%, time=15.02s\n",
            "Task 2: acc=98.24%, time=13.96s\n",
            "Task 3: acc=98.19%, time=13.90s\n",
            "Task 4: acc=98.20%, time=13.91s\n",
            "Task 5: acc=98.23%, time=13.66s\n",
            "Task 6: acc=98.18%, time=13.55s\n",
            "Task 7: acc=98.31%, time=13.80s\n",
            "Task 8: acc=98.27%, time=13.82s\n",
            "Task 9: acc=98.13%, time=13.54s\n",
            "Task 10: acc=98.27%, time=13.59s\n",
            "** Summary -> Mean acc: 98.22% | Total time: 138.76s\n",
            "\n",
            "---- Config: epochs=5, hidden=(384,192), batch_size=128 ----\n",
            "Task 1: acc=98.54%, time=17.65s\n",
            "Task 2: acc=98.36%, time=18.68s\n",
            "Task 3: acc=98.27%, time=18.17s\n",
            "Task 4: acc=98.29%, time=17.69s\n",
            "Task 5: acc=98.35%, time=17.94s\n",
            "Task 6: acc=98.20%, time=18.60s\n",
            "Task 7: acc=98.24%, time=17.98s\n",
            "Task 8: acc=98.35%, time=17.73s\n",
            "Task 9: acc=98.24%, time=18.28s\n",
            "Task 10: acc=98.25%, time=18.50s\n",
            "** Summary -> Mean acc: 98.31% | Total time: 181.22s\n",
            "\n",
            "---- Config: epochs=7, hidden=(256,128), batch_size=128 ----\n",
            "Task 1: acc=98.32%, time=18.34s\n",
            "Task 2: acc=98.27%, time=18.32s\n",
            "Task 3: acc=98.09%, time=19.01s\n",
            "Task 4: acc=98.15%, time=19.22s\n",
            "Task 5: acc=98.27%, time=18.91s\n",
            "Task 6: acc=98.10%, time=18.54s\n",
            "Task 7: acc=98.19%, time=18.69s\n",
            "Task 8: acc=98.23%, time=19.39s\n",
            "Task 9: acc=98.28%, time=19.13s\n",
            "Task 10: acc=98.13%, time=18.35s\n",
            "** Summary -> Mean acc: 98.20% | Total time: 187.90s\n",
            "\n",
            "---- Config: epochs=7, hidden=(384,192), batch_size=128 ----\n",
            "Task 1: acc=98.47%, time=25.04s\n",
            "Task 2: acc=98.38%, time=29.47s\n",
            "Task 3: acc=98.37%, time=25.36s\n",
            "Task 4: acc=98.42%, time=25.28s\n",
            "Task 5: acc=98.50%, time=25.17s\n",
            "Task 6: acc=98.19%, time=24.75s\n",
            "Task 7: acc=98.39%, time=25.12s\n",
            "Task 8: acc=98.32%, time=25.01s\n",
            "Task 9: acc=98.29%, time=25.13s\n",
            "Task 10: acc=98.20%, time=25.47s\n",
            "** Summary -> Mean acc: 98.35% | Total time: 255.82s\n",
            "\n",
            "---- Config: epochs=10, hidden=(256,128), batch_size=128 ----\n",
            "Task 1: acc=98.47%, time=26.65s\n",
            "Task 2: acc=98.30%, time=27.59s\n",
            "Task 3: acc=98.28%, time=26.42s\n",
            "Task 4: acc=98.27%, time=26.44s\n",
            "Task 5: acc=98.32%, time=26.38s\n",
            "Task 6: acc=98.27%, time=27.07s\n",
            "Task 7: acc=98.33%, time=28.11s\n",
            "Task 8: acc=98.36%, time=26.64s\n",
            "Task 9: acc=98.40%, time=26.94s\n",
            "Task 10: acc=98.24%, time=26.48s\n",
            "** Summary -> Mean acc: 98.32% | Total time: 268.72s\n",
            "\n",
            "---- Config: epochs=10, hidden=(384,192), batch_size=128 ----\n",
            "Task 1: acc=98.54%, time=35.68s\n",
            "Task 2: acc=98.40%, time=35.93s\n",
            "Task 3: acc=98.39%, time=35.65s\n",
            "Task 4: acc=98.49%, time=35.68s\n",
            "Task 5: acc=98.40%, time=35.05s\n",
            "Task 6: acc=98.38%, time=34.98s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-941692077.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mtid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/permuted_mnist/permuted_mnist/env/permuted_mnist.py\u001b[0m in \u001b[0;36mget_next_task\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Apply pixel permutation and task-specific noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mtrain_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_permute_pixels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mtest_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_permute_pixels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/permuted_mnist/permuted_mnist/env/permuted_mnist.py\u001b[0m in \u001b[0;36m_permute_pixels\u001b[0;34m(self, images, task_id)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# Add per-pixel Gaussian noise (std=0.015)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_rng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.015\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermuted_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mpermuted_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermuted_images\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP-RMSprop(+Cosine) + Dynamic INT8（仅推理） + 6组网格评测 =====\n",
        "# 保持：FE=per-sample L2、RMSprop(alpha=0.99,momentum=0,centered=False)、Cosine LR、CE 损失、CPU 推理\n",
        "import time, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# 评测约束：限制CPU线程\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---------------- I/O 与 FE ----------------\n",
        "def _as_float_01(x_np):\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0: x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np):\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "def _l2_per_sample(x, eps=1e-6):\n",
        "    orig = x.shape\n",
        "    if x.dim()==3: x = x.view(x.size(0), -1)  # (N,28,28)->(N,784)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig)==3: x = x.view(orig)\n",
        "    return x\n",
        "\n",
        "# ---------------- 模型（隐藏层可参数化） ----------------\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"\n",
        "    结构：784 -> h1 -> h2 -> 10，每层 BN+ReLU（输出层无 BN/激活）\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=784, h1=256, h2=128, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h1, bias=False), nn.BatchNorm1d(h1), nn.ReLU(inplace=True),\n",
        "            nn.Linear(h1, h2, bias=False),     nn.BatchNorm1d(h2), nn.ReLU(inplace=True),\n",
        "            nn.Linear(h2, out_dim, bias=True),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim()==3: x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "# ---------------- Agent（新增 dynamic INT8 压缩器） ----------------\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    固定：RMSprop(+Cosine)、CE、FE=L2、CPU\n",
        "    仅新增：compress_dynamic_int8() —— 推理前对 Linear 做 dynamic quant(INT8)\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim=10, seed=42, epochs=3, batch_size=128, lr=1e-3,\n",
        "                 val_ratio=0.2, weight_decay=0.0, use_cosine=True, h1=256, h2=128):\n",
        "        self.output_dim=output_dim; self.epochs=epochs; self.batch_size=batch_size\n",
        "        self.lr=lr; self.val_ratio=val_ratio; self.weight_decay=weight_decay\n",
        "        self.use_cosine=use_cosine; self.h1=h1; self.h2=h2\n",
        "        self.device=torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model=None; self.model_int8=None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_BN(in_dim=784, h1=self.h1, h2=self.h2, out_dim=self.output_dim).to(self.device)\n",
        "        self.model_int8 = None\n",
        "\n",
        "    def _make_loader(self, X, y, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X = _as_float_01(X_train); y = _labels_1d(y_train)\n",
        "        n_total = X.shape[0]; n_val = int(self.val_ratio * n_total)\n",
        "        idx = np.arange(n_total); np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]; tr_idx = idx[n_val:]\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # FE\n",
        "        X_tr = _l2_per_sample(X_tr); X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "\n",
        "        opt = optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99,\n",
        "                            momentum=0.0, centered=False, weight_decay=self.weight_decay)\n",
        "        sch = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=self.epochs, eta_min=0.0) if self.use_cosine else None\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for _ in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            if sch is not None: sch.step()\n",
        "\n",
        "    # ---- 动态 INT8 压缩（只量化 Linear，推理专用）----\n",
        "    def compress_dynamic_int8(self):\n",
        "        import torch.quantization as tq  # 与经典用法一致\n",
        "        mdl = self.model.to(\"cpu\").eval()\n",
        "        self.model_int8 = tq.quantize_dynamic(\n",
        "            mdl, {nn.Linear}, dtype=torch.qint8, inplace=False\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test):\n",
        "        X = _as_float_01(X_test)          # CPU\n",
        "        X = _l2_per_sample(X)\n",
        "        mdl = self.model_int8 if self.model_int8 is not None else self.model\n",
        "        mdl.eval()\n",
        "        bs=4096; outs=[]\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = mdl(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ---------------- 6 组网格评测：epochs×hidden ----------------\n",
        "from permuted_mnist.env.permuted_mnist import PermutedMNISTEnv\n",
        "\n",
        "# 若已有 env，可沿用其 episode 数；否则默认 10\n",
        "try:\n",
        "    EPISODES = env.number_episodes\n",
        "except Exception:\n",
        "    EPISODES = 10\n",
        "\n",
        "search_epochs = [5, 7, 10]\n",
        "hidden_cfgs   = [(256,128), (384,192)]   # 两套隐藏层：常规模型 vs 更大容量\n",
        "\n",
        "all_results = []\n",
        "print(\"\\n=== Grid Search (RMSprop + FE + Dynamic INT8 inference) ===\")\n",
        "\n",
        "for ep in search_epochs:\n",
        "    for (h1, h2) in hidden_cfgs:\n",
        "        # 为可比性，重建环境并固定随机种子\n",
        "        e = PermutedMNISTEnv(number_episodes=EPISODES)\n",
        "        e.set_seed(42)\n",
        "\n",
        "        agent = TorchMLP(output_dim=10, seed=42,\n",
        "                         epochs=ep, batch_size=128, lr=1e-3, val_ratio=0.2,\n",
        "                         use_cosine=True, h1=h1, h2=h2)\n",
        "\n",
        "        accs, times = [], []\n",
        "        print(f\"\\n---- Config: epochs={ep}, hidden=({h1},{h2}), batch_size=128 ----\")\n",
        "        tid = 1\n",
        "        while True:\n",
        "            task = e.get_next_task()\n",
        "            if task is None: break\n",
        "            agent.reset()\n",
        "\n",
        "            t0 = time.time()\n",
        "            agent.train(task['X_train'], task['y_train'])\n",
        "\n",
        "            # 仅在推理前做动态 INT8 压缩；其他完全不变\n",
        "            agent.compress_dynamic_int8()\n",
        "\n",
        "            preds   = agent.predict(task['X_test'])\n",
        "            elapsed = time.time() - t0\n",
        "\n",
        "            acc = e.evaluate(preds, task['y_test'])\n",
        "            accs.append(acc); times.append(elapsed)\n",
        "            print(f\"Task {tid}: acc={acc:.2%}, time={elapsed:.2f}s\")\n",
        "            tid += 1\n",
        "\n",
        "        mean_acc = float(np.mean(accs))\n",
        "        total_t  = float(np.sum(times))\n",
        "        all_results.append((ep, h1, h2, mean_acc, total_t))\n",
        "        print(f\"** Summary -> Mean acc: {mean_acc:.2%} | Total time: {total_t:.2f}s\")\n",
        "\n",
        "# 终汇总\n",
        "print(\"\\n===== Overall Summary (Dynamic INT8 inference) =====\")\n",
        "print(\"epochs |  h1  |  h2  | mean_acc | total_time(s)\")\n",
        "for ep, h1, h2, ma, tt in all_results:\n",
        "    print(f\"{ep:>6} | {h1:>4} | {h2:>4} | {ma:>8.2%} | {tt:>12.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sAWODlPtqmEy",
      "metadata": {
        "id": "sAWODlPtqmEy"
      },
      "source": [
        "### By now i alreay found a stable combinatio: TorchMLP + epoch=3+ batchsize+128+ FE L2 normalization + BN,,, so i turned to try one of the advanced way for optimisation: compressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CsYqMZuNwLWo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsYqMZuNwLWo",
        "outputId": "60766cb5-ee32-47e2-91ed-832c3042833a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compression study under fixed training budget (RMSprop + FE):\n",
            "Variants: Baseline (FP32) | Dynamic-Quant (INT8 Linear) | Prune(40%)+Dynamic-Quant\n",
            "=====================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-32467005.py:108: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  qmodel = torch.quantization.quantize_dynamic(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task 1: FP32 acc=98.18%, time=7.64s  |  INT8 acc=98.18%, time=0.43s  |  Prune40%+INT8 acc=98.12%, time=0.20s\n",
            "Task 2: FP32 acc=98.12%, time=7.37s  |  INT8 acc=98.12%, time=0.12s  |  Prune40%+INT8 acc=97.94%, time=0.11s\n",
            "Task 3: FP32 acc=97.95%, time=7.37s  |  INT8 acc=97.91%, time=0.12s  |  Prune40%+INT8 acc=97.87%, time=0.12s\n",
            "Task 4: FP32 acc=97.97%, time=7.54s  |  INT8 acc=97.99%, time=0.10s  |  Prune40%+INT8 acc=97.79%, time=0.10s\n",
            "Task 5: FP32 acc=97.96%, time=7.65s  |  INT8 acc=97.95%, time=0.12s  |  Prune40%+INT8 acc=97.89%, time=0.12s\n",
            "Task 6: FP32 acc=98.14%, time=7.64s  |  INT8 acc=98.12%, time=0.12s  |  Prune40%+INT8 acc=97.93%, time=0.12s\n",
            "Task 7: FP32 acc=98.06%, time=7.14s  |  INT8 acc=98.04%, time=0.17s  |  Prune40%+INT8 acc=98.05%, time=0.17s\n",
            "Task 8: FP32 acc=98.08%, time=6.66s  |  INT8 acc=98.10%, time=0.12s  |  Prune40%+INT8 acc=97.87%, time=0.13s\n",
            "Task 9: FP32 acc=98.10%, time=7.16s  |  INT8 acc=98.07%, time=0.13s  |  Prune40%+INT8 acc=97.73%, time=0.12s\n",
            "Task 10: FP32 acc=97.92%, time=7.37s  |  INT8 acc=97.94%, time=0.12s  |  Prune40%+INT8 acc=97.70%, time=0.10s\n",
            "\n",
            "Summary (mean ± std):\n",
            "  Baseline     : acc=98.05% ± 0.09% | time(total)=73.55s\n",
            "  Dynamic INT8 : acc=98.04% ± 0.09% | time(total)=1.55s\n",
            "  Prune40%+INT8: acc=97.89% ± 0.12% | time(total)=1.30s\n"
          ]
        }
      ],
      "source": [
        "# ===== Compression under fixed budget: Dynamic Quantization & Global Pruning =====\n",
        "# 固定：epochs=3, bs=128, lr=1e-3, val=0.2, FE=L2 per-sample, Optimizer=RMSprop\n",
        "import time, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "# ---- 贴合评测约束：限制CPU线程 ----\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---------------- 工具/FE ----------------\n",
        "def _as_float_01(x_np):\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0: x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np):\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "def _l2_per_sample(x, eps=1e-6):\n",
        "    orig = x.shape\n",
        "    if x.dim()==3: x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig)==3: x = x.view(orig)\n",
        "    return x\n",
        "\n",
        "# --------------- 模型（与你当前基线一致：784→256→128→10, BN+ReLU） ---------------\n",
        "class _MLP_BN(nn.Module):\n",
        "    def __init__(self, in_dim=784, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 256, bias=False), nn.BatchNorm1d(256), nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 128, bias=False),    nn.BatchNorm1d(128), nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, out_dim, bias=True),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "    def forward(self, x):\n",
        "        if x.dim()==3: x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    def __init__(self, output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2, weight_decay=0.0, use_cosine=True):\n",
        "        self.output_dim=output_dim; self.epochs=epochs; self.batch_size=batch_size\n",
        "        self.lr=lr; self.val_ratio=val_ratio; self.weight_decay=weight_decay; self.use_cosine=use_cosine\n",
        "        self.device=torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model=None; self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_BN(in_dim=784, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X, y, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size, shuffle=shuffle,\n",
        "                                           drop_last=False, num_workers=0, pin_memory=False)\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X = _as_float_01(X_train); y = _labels_1d(y_train)\n",
        "        n_total = X.shape[0]; n_val = int(self.val_ratio*n_total)\n",
        "        idx = np.arange(n_total); np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]; tr_idx = idx[n_val:]\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        X_tr = _l2_per_sample(X_tr); X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99,\n",
        "                            momentum=0.0, centered=False, weight_decay=self.weight_decay)\n",
        "        sch = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=self.epochs, eta_min=0.0) if self.use_cosine else None\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for _ in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            if sch is not None: sch.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test, model=None, bs=4096):\n",
        "        if model is None: model = self.model\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        X = _l2_per_sample(X)\n",
        "        model.eval()\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, 0)\n",
        "\n",
        "# --------------- 压缩函数 ---------------\n",
        "def dynamic_quantize(model: nn.Module) -> nn.Module:\n",
        "    \"\"\"\n",
        "    仅对 Linear 层做 INT8 动态量化；BN/激活保持 FP32。\n",
        "    \"\"\"\n",
        "    qmodel = torch.quantization.quantize_dynamic(\n",
        "        model, {nn.Linear}, dtype=torch.qint8, inplace=False\n",
        "    )\n",
        "    return qmodel\n",
        "\n",
        "def global_prune_and_make_permanent(model: nn.Module, amount: float = 0.4) -> nn.Module:\n",
        "    \"\"\"\n",
        "    对所有 Linear.weight 做全局无结构剪枝，按幅值排名置零，随后移除reparam使其永久化。\n",
        "    amount=0.4 表示总体 40% 权重为零（经验上对该任务较稳）。\n",
        "    \"\"\"\n",
        "    params_to_prune = []\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            params_to_prune.append((m, 'weight'))\n",
        "\n",
        "    # 全局剪枝（按绝对值）\n",
        "    prune.global_unstructured(params_to_prune, pruning_method=prune.L1Unstructured, amount=amount)\n",
        "\n",
        "    # 移除 reparam（把掩码乘积并入 weight.data）\n",
        "    for (m, _) in params_to_prune:\n",
        "        prune.remove(m, 'weight')\n",
        "    return model\n",
        "\n",
        "# --------------- 评测：Baseline vs Quant vs Prune+Quant ---------------\n",
        "import time\n",
        "\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "acc_base, acc_quant, acc_pruneq = [], [], []\n",
        "time_base, time_quant, time_pruneq = [], [], []\n",
        "\n",
        "print(\"Compression study under fixed training budget (RMSprop + FE):\")\n",
        "print(\"Variants: Baseline (FP32) | Dynamic-Quant (INT8 Linear) | Prune(40%)+Dynamic-Quant\")\n",
        "print(\"=\"*85)\n",
        "\n",
        "task_id = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None: break\n",
        "\n",
        "    # 训练（固定预算）\n",
        "    agent = TorchMLP(output_dim=10, seed=42, epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    # ---- Baseline 推理 ----\n",
        "    preds_base = agent.predict(task['X_test'])\n",
        "    t_base = time.time() - t0\n",
        "    a_base = env.evaluate(preds_base, task['y_test'])\n",
        "\n",
        "    # ---- 动态量化（不改变训练；仅推理形态）----\n",
        "    qmodel = dynamic_quantize(agent.model)\n",
        "    t1 = time.time()\n",
        "    preds_q = agent.predict(task['X_test'], model=qmodel)\n",
        "    t_quant = time.time() - t1\n",
        "    a_quant = env.evaluate(preds_q, task['y_test'])\n",
        "\n",
        "    # ---- 剪枝 + 动态量化（剪枝后不做额外训练，以满足“预算不变”）----\n",
        "    pruned = global_prune_and_make_permanent(_MLP_BN(), amount=0.4)\n",
        "    pruned.load_state_dict(agent.model.state_dict(), strict=True)  # 从训练权重复制\n",
        "    pruned = global_prune_and_make_permanent(pruned, amount=0.4)  # 再执行一次确保持久剪枝\n",
        "    pqmodel = dynamic_quantize(pruned)\n",
        "\n",
        "    t2 = time.time()\n",
        "    preds_pq = agent.predict(task['X_test'], model=pqmodel)\n",
        "    t_pruneq = time.time() - t2\n",
        "    a_pruneq = env.evaluate(preds_pq, task['y_test'])\n",
        "\n",
        "    acc_base.append(a_base); acc_quant.append(a_quant); acc_pruneq.append(a_pruneq)\n",
        "    time_base.append(t_base); time_quant.append(t_quant); time_pruneq.append(t_pruneq)\n",
        "\n",
        "    print(f\"Task {task_id}: \"\n",
        "          f\"FP32 acc={a_base:.2%}, time={t_base:.2f}s  |  \"\n",
        "          f\"INT8 acc={a_quant:.2%}, time={t_quant:.2f}s  |  \"\n",
        "          f\"Prune40%+INT8 acc={a_pruneq:.2%}, time={t_pruneq:.2f}s\")\n",
        "    task_id += 1\n",
        "\n",
        "print(\"\\nSummary (mean ± std):\")\n",
        "print(f\"  Baseline     : acc={np.mean(acc_base):.2%} ± {np.std(acc_base):.2%} | time(total)={np.sum(time_base):.2f}s\")\n",
        "print(f\"  Dynamic INT8 : acc={np.mean(acc_quant):.2%} ± {np.std(acc_quant):.2%} | time(total)={np.sum(time_quant):.2f}s\")\n",
        "print(f\"  Prune40%+INT8: acc={np.mean(acc_pruneq):.2%} ± {np.std(acc_pruneq):.2%} | time(total)={np.sum(time_pruneq):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3uvq-VYenXGh",
      "metadata": {
        "id": "3uvq-VYenXGh"
      },
      "source": [
        "Model structure improvement change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f-nIXNHDpoVw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-nIXNHDpoVw",
        "outputId": "e326a9be-e0d7-4cac-f754-ef98b8732db9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating TorchMLP (RMSprop + FE + SiLU + Bottleneck + WeightNorm + LS + WarmupCosine)\n",
            "======================================================================\n",
            "Task 1: acc=98.17%, time=10.08s\n",
            "Task 2: acc=98.18%, time=10.52s\n",
            "Task 3: acc=98.13%, time=9.48s\n",
            "Task 4: acc=98.05%, time=9.43s\n",
            "Task 5: acc=98.22%, time=9.68s\n",
            "Task 6: acc=98.31%, time=10.29s\n",
            "Task 7: acc=98.13%, time=10.12s\n",
            "Task 8: acc=98.15%, time=10.29s\n",
            "Task 9: acc=98.28%, time=15.23s\n",
            "Task 10: acc=98.19%, time=9.54s\n",
            "\n",
            "Summary:\n",
            "  Mean accuracy: 98.18% ± 0.07%\n",
            "  Total time: 104.67s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP (RMSprop + FE) with SiLU-first + Bottleneck-ResMLP + WeightNorm head + LabelSmoothing + WarmupCosine =====\n",
        "import time, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# CPU 线程限制\n",
        "try: torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception: pass\n",
        "\n",
        "def _as_float_01(x_np):\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0: x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np):\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# ------------ FE: per-sample L2 normalization ------------\n",
        "def _l2_per_sample(x, eps=1e-6):\n",
        "    orig = x.shape\n",
        "    if x.dim()==3: x = x.view(x.size(0), -1)  # (N,28,28)->(N,784)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig)==3: x = x.view(orig)\n",
        "    return x\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# ---- Label Smoothing CE（稳定短训）----\n",
        "class SmoothCE(nn.Module):\n",
        "    def __init__(self, eps=0.05, num_classes=10):\n",
        "        super().__init__(); self.eps=eps; self.C=num_classes\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, logits, target):\n",
        "        logp = self.logsoftmax(logits)\n",
        "        with torch.no_grad():\n",
        "            t = torch.zeros_like(logp).scatter_(1, target.view(-1,1), 1.0)\n",
        "            t = t*(1-self.eps) + self.eps/self.C\n",
        "        return (-t*logp).sum(dim=1).mean()\n",
        "\n",
        "# ---- 1-epoch warmup + cosine（总 epoch 不变）----\n",
        "class WarmupCosine:\n",
        "    def __init__(self, optimizer, total_epochs, warmup_epochs=1):\n",
        "        self.opt=optimizer; self.total=total_epochs; self.warm=warmup_epochs\n",
        "        self.base=[g['lr'] for g in optimizer.param_groups]; self.ep=0\n",
        "    def step(self):\n",
        "        self.ep += 1\n",
        "        for i,g in enumerate(self.opt.param_groups):\n",
        "            base=self.base[i]\n",
        "            if self.ep<=self.warm:\n",
        "                lr = base*self.ep/max(1,self.warm)\n",
        "            else:\n",
        "                prog=(self.ep-self.warm)/max(1,(self.total-self.warm))\n",
        "                lr = 0.5*base*(1+np.cos(np.pi*prog))\n",
        "            g['lr']=lr\n",
        "\n",
        "# ---- 轻量残差瓶颈（pre-act，256→64→256）----\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, d=256, b=64):\n",
        "        super().__init__()\n",
        "        self.bn1=nn.BatchNorm1d(d); self.act1=nn.SiLU(inplace=True)\n",
        "        self.fc1=nn.Linear(d,b, bias=False)\n",
        "        self.bn2=nn.BatchNorm1d(b); self.act2=nn.SiLU(inplace=True)\n",
        "        self.fc2=nn.Linear(b,d, bias=False)\n",
        "        nn.init.kaiming_uniform_(self.fc1.weight, a=0, nonlinearity='relu')\n",
        "        nn.init.kaiming_uniform_(self.fc2.weight, a=0, nonlinearity='relu')\n",
        "    def forward(self, x):\n",
        "        h=self.fc1(self.act1(self.bn1(x)))\n",
        "        h=self.fc2(self.act2(self.bn2(h)))\n",
        "        return x+h\n",
        "\n",
        "# ---- 模型：784→256→(bottleneck)→128→10；首层 SiLU；分类头 WeightNorm ----\n",
        "class _MLP_ResBN(nn.Module):\n",
        "    def __init__(self, in_dim=784, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, 256, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(256); self.act1 = nn.SiLU(inplace=True)\n",
        "\n",
        "        self.block = Bottleneck(d=256, b=64)  # 仅 1 个，计算开销很小\n",
        "\n",
        "        self.fc2 = nn.Linear(256, 128, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(128); self.act2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        head = nn.Linear(128, out_dim, bias=True)\n",
        "        self.head = nn.utils.weight_norm(head)   # 稳定最后分类层\n",
        "\n",
        "        # 初始化（ReLU/SiLU 更适合 Kaiming）\n",
        "        for m in [self.fc1, self.fc2, head]:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, a=0, nonlinearity='relu')\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim()==3: x=x.view(x.size(0), -1)\n",
        "        x = self.act1(self.bn1(self.fc1(x)))\n",
        "        x = self.block(x)\n",
        "        x = self.act2(self.bn2(self.fc2(x)))\n",
        "        return self.head(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    不改基础预算：epochs=3, batch=128, lr=1e-3, RMSprop, FE=L2 per-sample\n",
        "    仅加：SiLU首层 + 1个瓶颈残差 + WeightNorm头 + LabelSmoothing + WarmupCosine\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim=10, seed=42, epochs=3, batch_size=128, lr=1e-3,\n",
        "                 val_ratio=0.2, weight_decay=0.0):\n",
        "        self.output_dim=output_dim; self.epochs=epochs; self.batch_size=batch_size\n",
        "        self.lr=lr; self.val_ratio=val_ratio; self.weight_decay=weight_decay\n",
        "        self.device=torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model=None; self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_ResBN(in_dim=784, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X, y, shuffle=True):\n",
        "        ds=torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X=_as_float_01(X_train); y=_labels_1d(y_train)\n",
        "        n_total=X.shape[0]; n_val=int(self.val_ratio*n_total)\n",
        "        idx=np.arange(n_total); np.random.shuffle(idx)\n",
        "        val_idx=idx[:n_val]; tr_idx=idx[n_val:]\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # FE：每样本 L2\n",
        "        X_tr=_l2_per_sample(X_tr); X_val=_l2_per_sample(X_val)\n",
        "\n",
        "        loader=self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "\n",
        "        opt=optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99,\n",
        "                          momentum=0.0, centered=False, weight_decay=self.weight_decay)\n",
        "        sch=WarmupCosine(opt, total_epochs=self.epochs, warmup_epochs=1)\n",
        "        crit=SmoothCE(0.05, self.output_dim)\n",
        "\n",
        "        self.model.train()\n",
        "        for _ in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits=self.model(xb)\n",
        "                loss=crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            sch.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test):\n",
        "        X=_as_float_01(X_test).to(self.device)\n",
        "        X=_l2_per_sample(X)\n",
        "        self.model.eval()\n",
        "        bs=4096; outs=[]\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits=self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ===== Evaluate =====\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "print(\"Evaluating TorchMLP (RMSprop + FE + SiLU + Bottleneck + WeightNorm + LS + WarmupCosine)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "task_num=1\n",
        "while True:\n",
        "    task=env.get_next_task()\n",
        "    if task is None: break\n",
        "    agent.reset()\n",
        "    t0=time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds=agent.predict(task['X_test'])\n",
        "    elapsed=time.time()-t0\n",
        "    acc=env.evaluate(preds, task['y_test'])\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: acc={acc:.2%}, time={elapsed:.2f}s\")\n",
        "    task_num+=1\n",
        "\n",
        "print(\"\\nSummary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nStk5bOleFE8",
      "metadata": {
        "id": "nStk5bOleFE8"
      },
      "source": [
        "# ===== Model structure improvement:TorchMLP (RMSprop + FE) with SiLU-first + Bottleneck-ResMLP + WeightNorm head\n",
        "#       + LabelSmoothing + WarmupCosine + Dynamic INT8（仅推理） + 网格搜索 =====\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9yjNbcxxeJET",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yjNbcxxeJET",
        "outputId": "60f8ea0c-07d9-4856-b510-446eb2b200fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Grid Search with Dynamic INT8 (only inference) ===\n",
            "\n",
            "---- Config: epochs=5, batch_size=100 ----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "/tmp/ipython-input-2664704861.py:195: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  self.model_int8 = tq.quantize_dynamic(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task 1: acc=98.56%, time=21.31s\n",
            "Task 2: acc=98.56%, time=20.59s\n",
            "Task 3: acc=98.56%, time=20.75s\n",
            "Task 4: acc=98.42%, time=20.53s\n",
            "Task 5: acc=98.28%, time=21.12s\n",
            "Task 6: acc=98.32%, time=20.81s\n",
            "Task 7: acc=98.42%, time=20.61s\n",
            "Task 8: acc=98.39%, time=19.71s\n",
            "Task 9: acc=98.40%, time=19.49s\n",
            "Task 10: acc=98.52%, time=19.71s\n",
            "** Summary (epochs=5, batch=100) -> Mean acc: 98.44% | Total time: 204.62s\n",
            "\n",
            "---- Config: epochs=5, batch_size=128 ----\n",
            "Task 1: acc=98.56%, time=18.38s\n",
            "Task 2: acc=98.43%, time=21.80s\n",
            "Task 3: acc=98.39%, time=19.53s\n",
            "Task 4: acc=98.46%, time=18.72s\n",
            "Task 5: acc=98.40%, time=18.18s\n",
            "Task 6: acc=98.49%, time=18.03s\n",
            "Task 7: acc=98.43%, time=18.76s\n",
            "Task 8: acc=98.47%, time=18.62s\n",
            "Task 9: acc=98.45%, time=17.97s\n",
            "Task 10: acc=98.52%, time=18.25s\n",
            "** Summary (epochs=5, batch=128) -> Mean acc: 98.46% | Total time: 188.24s\n",
            "\n",
            "---- Config: epochs=7, batch_size=100 ----\n",
            "Task 1: acc=98.50%, time=28.63s\n",
            "Task 2: acc=98.44%, time=27.89s\n",
            "Task 3: acc=98.54%, time=28.80s\n",
            "Task 4: acc=98.39%, time=28.29s\n",
            "Task 5: acc=98.54%, time=29.58s\n",
            "Task 6: acc=98.49%, time=28.00s\n",
            "Task 7: acc=98.48%, time=28.28s\n",
            "Task 8: acc=98.49%, time=28.14s\n",
            "Task 9: acc=98.40%, time=28.14s\n",
            "Task 10: acc=98.53%, time=28.40s\n",
            "** Summary (epochs=7, batch=100) -> Mean acc: 98.48% | Total time: 284.16s\n",
            "\n",
            "---- Config: epochs=7, batch_size=128 ----\n",
            "Task 1: acc=98.52%, time=25.30s\n",
            "Task 2: acc=98.46%, time=25.21s\n",
            "Task 3: acc=98.40%, time=25.20s\n",
            "Task 4: acc=98.54%, time=25.19s\n",
            "Task 5: acc=98.50%, time=24.92s\n",
            "Task 6: acc=98.42%, time=24.96s\n",
            "Task 7: acc=98.43%, time=25.73s\n",
            "Task 8: acc=98.52%, time=25.55s\n",
            "Task 9: acc=98.48%, time=25.46s\n",
            "Task 10: acc=98.39%, time=25.51s\n",
            "** Summary (epochs=7, batch=128) -> Mean acc: 98.47% | Total time: 253.04s\n",
            "\n",
            "---- Config: epochs=10, batch_size=100 ----\n",
            "Task 1: acc=98.54%, time=39.60s\n",
            "Task 2: acc=98.50%, time=39.89s\n",
            "Task 3: acc=98.40%, time=39.66s\n",
            "Task 4: acc=98.66%, time=39.92s\n",
            "Task 5: acc=98.44%, time=39.67s\n",
            "Task 6: acc=98.47%, time=39.63s\n",
            "Task 7: acc=98.50%, time=40.31s\n",
            "Task 8: acc=98.61%, time=39.22s\n",
            "Task 9: acc=98.52%, time=44.68s\n",
            "Task 10: acc=98.51%, time=58.75s\n",
            "** Summary (epochs=10, batch=100) -> Mean acc: 98.52% | Total time: 421.34s\n",
            "\n",
            "---- Config: epochs=10, batch_size=128 ----\n",
            "Task 1: acc=98.55%, time=35.99s\n",
            "Task 2: acc=98.57%, time=36.20s\n",
            "Task 3: acc=98.51%, time=35.53s\n",
            "Task 4: acc=98.51%, time=37.01s\n",
            "Task 5: acc=98.50%, time=36.43s\n",
            "Task 6: acc=98.57%, time=34.93s\n",
            "Task 7: acc=98.39%, time=35.06s\n",
            "Task 8: acc=98.56%, time=35.62s\n",
            "Task 9: acc=98.51%, time=36.55s\n",
            "Task 10: acc=98.55%, time=36.94s\n",
            "** Summary (epochs=10, batch=128) -> Mean acc: 98.52% | Total time: 360.26s\n",
            "\n",
            "===== Overall Summary (Dynamic INT8) =====\n",
            "epochs | batch | mean_acc | total_time(s)\n",
            "     5 |   100 |   98.44% |       204.62\n",
            "     5 |   128 |   98.46% |       188.24\n",
            "     7 |   100 |   98.48% |       284.16\n",
            "     7 |   128 |   98.47% |       253.04\n",
            "    10 |   100 |   98.52% |       421.34\n",
            "    10 |   128 |   98.52% |       360.26\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP (RMSprop + FE) with SiLU-first + Bottleneck-ResMLP + WeightNorm head\n",
        "#       + LabelSmoothing + WarmupCosine + Dynamic INT8（仅推理） + 网格搜索 =====\n",
        "import time, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# CPU 线程限制\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ------------------------- I/O 与预处理 -------------------------\n",
        "def _as_float_01(x_np):\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np):\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# FE：每样本 L2 归一化\n",
        "def _l2_per_sample(x, eps=1e-6):\n",
        "    orig = x.shape\n",
        "    if x.dim() == 3:\n",
        "        x = x.view(x.size(0), -1)  # (N,28,28)->(N,784)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig) == 3:\n",
        "        x = x.view(orig)\n",
        "    return x\n",
        "\n",
        "# ------------------------- 损失与调度 -------------------------\n",
        "class SmoothCE(nn.Module):\n",
        "    def __init__(self, eps=0.05, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.C = num_classes\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        logp = self.logsoftmax(logits)\n",
        "        with torch.no_grad():\n",
        "            t = torch.zeros_like(logp).scatter_(1, target.view(-1, 1), 1.0)\n",
        "            t = t * (1 - self.eps) + self.eps / self.C\n",
        "        return (-t * logp).sum(dim=1).mean()\n",
        "\n",
        "class WarmupCosine:\n",
        "    def __init__(self, optimizer, total_epochs, warmup_epochs=1):\n",
        "        self.opt = optimizer\n",
        "        self.total = total_epochs\n",
        "        self.warm = warmup_epochs\n",
        "        self.base = [g['lr'] for g in optimizer.param_groups]\n",
        "        self.ep = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.ep += 1\n",
        "        for i, g in enumerate(self.opt.param_groups):\n",
        "            base = self.base[i]\n",
        "            if self.ep <= self.warm:\n",
        "                lr = base * self.ep / max(1, self.warm)\n",
        "            else:\n",
        "                prog = (self.ep - self.warm) / max(1, (self.total - self.warm))\n",
        "                lr = 0.5 * base * (1 + np.cos(np.pi * prog))\n",
        "            g['lr'] = lr\n",
        "\n",
        "# ------------------------- 模型 -------------------------\n",
        "class Bottleneck(nn.Module):\n",
        "    # 轻量残差瓶颈（pre-act，256→64→256）\n",
        "    def __init__(self, d=256, b=64):\n",
        "        super().__init__()\n",
        "        self.bn1 = nn.BatchNorm1d(d)\n",
        "        self.act1 = nn.SiLU(inplace=True)\n",
        "        self.fc1 = nn.Linear(d, b, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(b)\n",
        "        self.act2 = nn.SiLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(b, d, bias=False)\n",
        "        nn.init.kaiming_uniform_(self.fc1.weight, a=0, nonlinearity='relu')\n",
        "        nn.init.kaiming_uniform_(self.fc2.weight, a=0, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.fc1(self.act1(self.bn1(x)))\n",
        "        h = self.fc2(self.act2(self.bn2(h)))\n",
        "        return x + h\n",
        "\n",
        "class _MLP_ResBN(nn.Module):\n",
        "    # 784→256→(bottleneck)→128→10；首层 SiLU；分类头 WeightNorm\n",
        "    def __init__(self, in_dim=784, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, 256, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.act1 = nn.SiLU(inplace=True)\n",
        "\n",
        "        self.block = Bottleneck(d=256, b=64)\n",
        "\n",
        "        self.fc2 = nn.Linear(256, 128, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.act2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        head = nn.Linear(128, out_dim, bias=True)\n",
        "        self.head = nn.utils.weight_norm(head)  # 最后一层加 WeightNorm\n",
        "\n",
        "        for m in [self.fc1, self.fc2, head]:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, a=0, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 3:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        x = self.act1(self.bn1(self.fc1(x)))\n",
        "        x = self.block(x)\n",
        "        x = self.act2(self.bn2(self.fc2(x)))\n",
        "        return self.head(x)\n",
        "\n",
        "# ------------------------- Agent（仅新增 dynamic INT8 压缩器） -------------------------\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    预算与流程不变：\n",
        "      RMSprop、FE(L2)、SiLU首层、瓶颈、WeightNorm、LabelSmoothing、WarmupCosine\n",
        "    仅新增：\n",
        "      - compress_dynamic_int8(): 推理前做 dynamic quant（INT8, Linear）\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim=10, seed=42, epochs=3, batch_size=128, lr=1e-3,\n",
        "                 val_ratio=0.2, weight_decay=0.0):\n",
        "        self.output_dim = output_dim\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.val_ratio = val_ratio\n",
        "        self.weight_decay = weight_decay\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.model_int8 = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_ResBN(in_dim=784, out_dim=self.output_dim).to(self.device)\n",
        "        self.model_int8 = None  # 每个任务重置\n",
        "\n",
        "    def _make_loader(self, X, y, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(\n",
        "            ds, batch_size=self.batch_size, shuffle=shuffle,\n",
        "            drop_last=False, num_workers=0, pin_memory=False\n",
        "        )\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X = _as_float_01(X_train); y = _labels_1d(y_train)\n",
        "        n_total = X.shape[0]; n_val = int(self.val_ratio * n_total)\n",
        "        idx = np.arange(n_total); np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]; tr_idx = idx[n_val:]\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # FE：每样本 L2\n",
        "        X_tr = _l2_per_sample(X_tr); X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "\n",
        "        opt = optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99,\n",
        "                            momentum=0.0, centered=False, weight_decay=self.weight_decay)\n",
        "        sch = WarmupCosine(opt, total_epochs=self.epochs, warmup_epochs=1)\n",
        "        crit = SmoothCE(0.05, self.output_dim)\n",
        "\n",
        "        self.model.train()\n",
        "        for _ in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            sch.step()\n",
        "\n",
        "    # ---- 动态 INT8（与截图同款：torch.quantization.quantize_dynamic） ----\n",
        "    def compress_dynamic_int8(self):\n",
        "        \"\"\"\n",
        "        - 只量化 nn.Linear 到 INT8\n",
        "        - 推理专用，训练完全不改\n",
        "        - 量化前移除 head 的 weight_norm，避免 deepcopy 报错\n",
        "        \"\"\"\n",
        "        import torch.quantization as tq\n",
        "        mdl = self.model.to(\"cpu\").eval()\n",
        "\n",
        "        # 移除 WeightNorm（否则 quantize_dynamic 内部 deepcopy 可能报错）\n",
        "        try:\n",
        "            nn.utils.remove_weight_norm(mdl.head)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        self.model_int8 = tq.quantize_dynamic(\n",
        "            mdl, {nn.Linear}, dtype=torch.qint8, inplace=False\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test):\n",
        "        X = _as_float_01(X_test)  # 保持 CPU\n",
        "        X = _l2_per_sample(X)\n",
        "        mdl = self.model_int8 if self.model_int8 is not None else self.model\n",
        "        mdl.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = mdl(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ------------------------- 网格搜索（6 组） -------------------------\n",
        "from permuted_mnist.env.permuted_mnist import PermutedMNISTEnv\n",
        "\n",
        "# 若你 notebook 里已有 env，则沿用它的 episode 数；否则默认 10\n",
        "try:\n",
        "    EPISODES = env.number_episodes\n",
        "except Exception:\n",
        "    EPISODES = 10\n",
        "\n",
        "search_epochs = [5, 7, 10]\n",
        "search_batch  = [100, 128]\n",
        "\n",
        "all_results = []\n",
        "print(\"\\n=== Grid Search with Dynamic INT8 (only inference) ===\")\n",
        "\n",
        "for ep in search_epochs:\n",
        "    for bs in search_batch:\n",
        "        env_gs = PermutedMNISTEnv(number_episodes=EPISODES)\n",
        "        env_gs.set_seed(42)\n",
        "\n",
        "        agent = TorchMLP(output_dim=10, seed=42,\n",
        "                         epochs=ep, batch_size=bs, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "        accs, times = [], []\n",
        "        print(f\"\\n---- Config: epochs={ep}, batch_size={bs} ----\")\n",
        "        tid = 1\n",
        "        while True:\n",
        "            task = env_gs.get_next_task()\n",
        "            if task is None:\n",
        "                break\n",
        "            agent.reset()\n",
        "\n",
        "            t0 = time.time()\n",
        "            agent.train(task['X_train'], task['y_train'])\n",
        "\n",
        "            # 只在推理前做动态 INT8；其他保持不变\n",
        "            agent.compress_dynamic_int8()\n",
        "\n",
        "            preds   = agent.predict(task['X_test'])\n",
        "            elapsed = time.time() - t0\n",
        "\n",
        "            acc = env_gs.evaluate(preds, task['y_test'])\n",
        "            accs.append(acc); times.append(elapsed)\n",
        "            print(f\"Task {tid}: acc={acc:.2%}, time={elapsed:.2f}s\")\n",
        "            tid += 1\n",
        "\n",
        "        mean_acc = float(np.mean(accs))\n",
        "        total_t  = float(np.sum(times))\n",
        "        all_results.append((ep, bs, mean_acc, total_t))\n",
        "        print(f\"** Summary (epochs={ep}, batch={bs}) -> \"\n",
        "              f\"Mean acc: {mean_acc:.2%} | Total time: {total_t:.2f}s\")\n",
        "\n",
        "print(\"\\n===== Overall Summary (Dynamic INT8) =====\")\n",
        "print(\"epochs | batch | mean_acc | total_time(s)\")\n",
        "for ep, bs, ma, tt in all_results:\n",
        "    print(f\"{ep:>6} | {bs:>5} | {ma:>8.2%} | {tt:>12.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rAxaZMIMyixB",
      "metadata": {
        "id": "rAxaZMIMyixB"
      },
      "source": [
        "# BEST ONE 👇\n",
        "##Model structure improvement+that good combination + compressor + good paramater epoch=10 batch 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KAPNW_ZQtn7Z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAPNW_ZQtn7Z",
        "outputId": "c6332ccc-b731-4a81-c7e7-4f304332a3b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== TorchMLP (Dynamic INT8, epochs=10, batch=128) ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-880761408.py:169: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  self.model_int8 = tq.quantize_dynamic(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task 1: acc=98.55%, time=45.08s\n",
            "Task 2: acc=98.57%, time=36.06s\n",
            "Task 3: acc=98.51%, time=37.34s\n",
            "Task 4: acc=98.51%, time=37.17s\n",
            "Task 5: acc=98.50%, time=35.49s\n",
            "Task 6: acc=98.57%, time=36.63s\n",
            "Task 7: acc=98.39%, time=36.53s\n",
            "Task 8: acc=98.56%, time=36.52s\n",
            "Task 9: acc=98.51%, time=36.53s\n",
            "Task 10: acc=98.55%, time=36.45s\n",
            "\n",
            "** Summary (epochs=10, batch=128) -> Mean acc: 98.52% | Total time: 373.79s **\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP (RMSprop + FE + SiLU + Bottleneck + WeightNorm + LabelSmoothing + WarmupCosine + Dynamic INT8) =====\n",
        "import time, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# 限制 CPU 线程（2 核约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ------------------------- I/O 与预处理 -------------------------\n",
        "def _as_float_01(x_np):\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np):\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "def _l2_per_sample(x, eps=1e-6):\n",
        "    orig = x.shape\n",
        "    if x.dim() == 3:\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig) == 3:\n",
        "        x = x.view(orig)\n",
        "    return x\n",
        "\n",
        "# ------------------------- 损失与调度 -------------------------\n",
        "class SmoothCE(nn.Module):\n",
        "    def __init__(self, eps=0.05, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.C = num_classes\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        logp = self.logsoftmax(logits)\n",
        "        with torch.no_grad():\n",
        "            t = torch.zeros_like(logp).scatter_(1, target.view(-1, 1), 1.0)\n",
        "            t = t * (1 - self.eps) + self.eps / self.C\n",
        "        return (-t * logp).sum(dim=1).mean()\n",
        "\n",
        "class WarmupCosine:\n",
        "    def __init__(self, optimizer, total_epochs, warmup_epochs=1):\n",
        "        self.opt = optimizer\n",
        "        self.total = total_epochs\n",
        "        self.warm = warmup_epochs\n",
        "        self.base = [g['lr'] for g in optimizer.param_groups]\n",
        "        self.ep = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.ep += 1\n",
        "        for i, g in enumerate(self.opt.param_groups):\n",
        "            base = self.base[i]\n",
        "            if self.ep <= self.warm:\n",
        "                lr = base * self.ep / max(1, self.warm)\n",
        "            else:\n",
        "                prog = (self.ep - self.warm) / max(1, (self.total - self.warm))\n",
        "                lr = 0.5 * base * (1 + np.cos(np.pi * prog))\n",
        "            g['lr'] = lr\n",
        "\n",
        "# ------------------------- 模型 -------------------------\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, d=256, b=64):\n",
        "        super().__init__()\n",
        "        self.bn1 = nn.BatchNorm1d(d)\n",
        "        self.act1 = nn.SiLU(inplace=True)\n",
        "        self.fc1 = nn.Linear(d, b, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(b)\n",
        "        self.act2 = nn.SiLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(b, d, bias=False)\n",
        "        nn.init.kaiming_uniform_(self.fc1.weight, a=0, nonlinearity='relu')\n",
        "        nn.init.kaiming_uniform_(self.fc2.weight, a=0, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.fc1(self.act1(self.bn1(x)))\n",
        "        h = self.fc2(self.act2(self.bn2(h)))\n",
        "        return x + h\n",
        "\n",
        "class _MLP_ResBN(nn.Module):\n",
        "    def __init__(self, in_dim=784, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, 256, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.act1 = nn.SiLU(inplace=True)\n",
        "        self.block = Bottleneck(d=256, b=64)\n",
        "        self.fc2 = nn.Linear(256, 128, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.act2 = nn.ReLU(inplace=True)\n",
        "        head = nn.Linear(128, out_dim, bias=True)\n",
        "        self.head = nn.utils.weight_norm(head)\n",
        "        for m in [self.fc1, self.fc2, head]:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, a=0, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 3:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        x = self.act1(self.bn1(self.fc1(x)))\n",
        "        x = self.block(x)\n",
        "        x = self.act2(self.bn2(self.fc2(x)))\n",
        "        return self.head(x)\n",
        "\n",
        "# ------------------------- Agent（含 dynamic INT8 压缩器） -------------------------\n",
        "class TorchMLP:\n",
        "    def __init__(self, output_dim=10, seed=42, epochs=10, batch_size=128, lr=1e-3,\n",
        "                 val_ratio=0.2, weight_decay=0.0):\n",
        "        self.output_dim = output_dim\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.val_ratio = val_ratio\n",
        "        self.weight_decay = weight_decay\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.model_int8 = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_ResBN(in_dim=784, out_dim=self.output_dim).to(self.device)\n",
        "        self.model_int8 = None\n",
        "\n",
        "    def _make_loader(self, X, y, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(\n",
        "            ds, batch_size=self.batch_size, shuffle=shuffle,\n",
        "            drop_last=False, num_workers=0, pin_memory=False\n",
        "        )\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X = _as_float_01(X_train); y = _labels_1d(y_train)\n",
        "        n_total = X.shape[0]; n_val = int(self.val_ratio * n_total)\n",
        "        idx = np.arange(n_total); np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]; tr_idx = idx[n_val:]\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "\n",
        "        opt = optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99,\n",
        "                            momentum=0.0, centered=False, weight_decay=self.weight_decay)\n",
        "        sch = WarmupCosine(opt, total_epochs=self.epochs, warmup_epochs=1)\n",
        "        crit = SmoothCE(0.05, self.output_dim)\n",
        "\n",
        "        self.model.train()\n",
        "        for _ in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            sch.step()\n",
        "\n",
        "    def compress_dynamic_int8(self):\n",
        "        import torch.quantization as tq\n",
        "        mdl = self.model.to(\"cpu\").eval()\n",
        "        try:\n",
        "            nn.utils.remove_weight_norm(mdl.head)\n",
        "        except Exception:\n",
        "            pass\n",
        "        self.model_int8 = tq.quantize_dynamic(\n",
        "            mdl, {nn.Linear}, dtype=torch.qint8, inplace=False\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test):\n",
        "        X = _as_float_01(X_test)\n",
        "        X = _l2_per_sample(X)\n",
        "        mdl = self.model_int8 if self.model_int8 is not None else self.model\n",
        "        mdl.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = mdl(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ------------------------- 单组评测：epoch=10, batch=128 -------------------------\n",
        "from permuted_mnist.env.permuted_mnist import PermutedMNISTEnv\n",
        "env_gs = PermutedMNISTEnv(number_episodes=10)\n",
        "env_gs.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42, epochs=10, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "print(\"\\n=== TorchMLP (Dynamic INT8, epochs=10, batch=128) ===\")\n",
        "tid = 1\n",
        "while True:\n",
        "    task = env_gs.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "    agent.reset()\n",
        "\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    agent.compress_dynamic_int8()\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "\n",
        "    acc = env_gs.evaluate(preds, task['y_test'])\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {tid}: acc={acc:.2%}, time={elapsed:.2f}s\")\n",
        "    tid += 1\n",
        "\n",
        "mean_acc = float(np.mean(accs))\n",
        "total_t = float(np.sum(times))\n",
        "print(f\"\\n** Summary (epochs=10, batch=128) -> Mean acc: {mean_acc:.2%} | Total time: {total_t:.2f}s **\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model structure improvement+that good combination + compressor + good paramater epoch=15 batch 128 （ no need)"
      ],
      "metadata": {
        "id": "ThScRTgAZTP1"
      },
      "id": "ThScRTgAZTP1"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== TorchMLP (RMSprop + FE + SiLU + Bottleneck + WeightNorm + LabelSmoothing + WarmupCosine + Dynamic INT8) =====\n",
        "import time, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# 限制 CPU 线程（2 核约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ------------------------- I/O 与预处理 -------------------------\n",
        "def _as_float_01(x_np):\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np):\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "def _l2_per_sample(x, eps=1e-6):\n",
        "    orig = x.shape\n",
        "    if x.dim() == 3:\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig) == 3:\n",
        "        x = x.view(orig)\n",
        "    return x\n",
        "\n",
        "# ------------------------- 损失与调度 -------------------------\n",
        "class SmoothCE(nn.Module):\n",
        "    def __init__(self, eps=0.05, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.C = num_classes\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        logp = self.logsoftmax(logits)\n",
        "        with torch.no_grad():\n",
        "            t = torch.zeros_like(logp).scatter_(1, target.view(-1, 1), 1.0)\n",
        "            t = t * (1 - self.eps) + self.eps / self.C\n",
        "        return (-t * logp).sum(dim=1).mean()\n",
        "\n",
        "class WarmupCosine:\n",
        "    def __init__(self, optimizer, total_epochs, warmup_epochs=1):\n",
        "        self.opt = optimizer\n",
        "        self.total = total_epochs\n",
        "        self.warm = warmup_epochs\n",
        "        self.base = [g['lr'] for g in optimizer.param_groups]\n",
        "        self.ep = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.ep += 1\n",
        "        for i, g in enumerate(self.opt.param_groups):\n",
        "            base = self.base[i]\n",
        "            if self.ep <= self.warm:\n",
        "                lr = base * self.ep / max(1, self.warm)\n",
        "            else:\n",
        "                prog = (self.ep - self.warm) / max(1, (self.total - self.warm))\n",
        "                lr = 0.5 * base * (1 + np.cos(np.pi * prog))\n",
        "            g['lr'] = lr\n",
        "\n",
        "# ------------------------- 模型 -------------------------\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, d=256, b=64):\n",
        "        super().__init__()\n",
        "        self.bn1 = nn.BatchNorm1d(d)\n",
        "        self.act1 = nn.SiLU(inplace=True)\n",
        "        self.fc1 = nn.Linear(d, b, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(b)\n",
        "        self.act2 = nn.SiLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(b, d, bias=False)\n",
        "        nn.init.kaiming_uniform_(self.fc1.weight, a=0, nonlinearity='relu')\n",
        "        nn.init.kaiming_uniform_(self.fc2.weight, a=0, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.fc1(self.act1(self.bn1(x)))\n",
        "        h = self.fc2(self.act2(self.bn2(h)))\n",
        "        return x + h\n",
        "\n",
        "class _MLP_ResBN(nn.Module):\n",
        "    def __init__(self, in_dim=784, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, 256, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.act1 = nn.SiLU(inplace=True)\n",
        "        self.block = Bottleneck(d=256, b=64)\n",
        "        self.fc2 = nn.Linear(256, 128, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.act2 = nn.ReLU(inplace=True)\n",
        "        head = nn.Linear(128, out_dim, bias=True)\n",
        "        self.head = nn.utils.weight_norm(head)\n",
        "        for m in [self.fc1, self.fc2, head]:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, a=0, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 3:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        x = self.act1(self.bn1(self.fc1(x)))\n",
        "        x = self.block(x)\n",
        "        x = self.act2(self.bn2(self.fc2(x)))\n",
        "        return self.head(x)\n",
        "\n",
        "# ------------------------- Agent（含 dynamic INT8 压缩器） -------------------------\n",
        "class TorchMLP:\n",
        "    def __init__(self, output_dim=10, seed=42, epochs=15, batch_size=128, lr=1e-3,\n",
        "                 val_ratio=0.2, weight_decay=0.0):\n",
        "        self.output_dim = output_dim\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.val_ratio = val_ratio\n",
        "        self.weight_decay = weight_decay\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.model_int8 = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_ResBN(in_dim=784, out_dim=self.output_dim).to(self.device)\n",
        "        self.model_int8 = None\n",
        "\n",
        "    def _make_loader(self, X, y, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(\n",
        "            ds, batch_size=self.batch_size, shuffle=shuffle,\n",
        "            drop_last=False, num_workers=0, pin_memory=False\n",
        "        )\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X = _as_float_01(X_train); y = _labels_1d(y_train)\n",
        "        n_total = X.shape[0]; n_val = int(self.val_ratio * n_total)\n",
        "        idx = np.arange(n_total); np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]; tr_idx = idx[n_val:]\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "\n",
        "        opt = optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99,\n",
        "                            momentum=0.0, centered=False, weight_decay=self.weight_decay)\n",
        "        sch = WarmupCosine(opt, total_epochs=self.epochs, warmup_epochs=1)\n",
        "        crit = SmoothCE(0.05, self.output_dim)\n",
        "\n",
        "        self.model.train()\n",
        "        for _ in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            sch.step()\n",
        "\n",
        "    def compress_dynamic_int8(self):\n",
        "        import torch.quantization as tq\n",
        "        mdl = self.model.to(\"cpu\").eval()\n",
        "        try:\n",
        "            nn.utils.remove_weight_norm(mdl.head)\n",
        "        except Exception:\n",
        "            pass\n",
        "        self.model_int8 = tq.quantize_dynamic(\n",
        "            mdl, {nn.Linear}, dtype=torch.qint8, inplace=False\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test):\n",
        "        X = _as_float_01(X_test)\n",
        "        X = _l2_per_sample(X)\n",
        "        mdl = self.model_int8 if self.model_int8 is not None else self.model\n",
        "        mdl.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = mdl(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ------------------------- 单组评测：epoch=15, batch=128 -------------------------\n",
        "from permuted_mnist.env.permuted_mnist import PermutedMNISTEnv\n",
        "env_gs = PermutedMNISTEnv(number_episodes=10)\n",
        "env_gs.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42, epochs=15, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "print(\"\\n=== TorchMLP (Dynamic INT8, epochs=15, batch=128) ===\")\n",
        "tid = 1\n",
        "while True:\n",
        "    task = env_gs.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "    agent.reset()\n",
        "\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    agent.compress_dynamic_int8()\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "\n",
        "    acc = env_gs.evaluate(preds, task['y_test'])\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {tid}: acc={acc:.2%}, time={elapsed:.2f}s\")\n",
        "    tid += 1\n",
        "\n",
        "mean_acc = float(np.mean(accs))\n",
        "total_t = float(np.sum(times))\n",
        "print(f\"\\n** Summary (epochs=15, batch=128) -> Mean acc: {mean_acc:.2%} | Total time: {total_t:.2f}s **\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LwHf1QlchlA",
        "outputId": "fc814dee-0547-4e50-dcab-f0ba42fb93d3"
      },
      "id": "0LwHf1QlchlA",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TorchMLP (Dynamic INT8, epochs=15, batch=128) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1042237381.py:169: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  self.model_int8 = tq.quantize_dynamic(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1: acc=98.64%, time=53.58s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 2: acc=98.48%, time=49.74s\n",
            "Task 3: acc=98.63%, time=51.36s\n",
            "Task 4: acc=98.52%, time=50.37s\n",
            "Task 5: acc=98.61%, time=49.20s\n",
            "Task 6: acc=98.59%, time=48.75s\n",
            "Task 7: acc=98.45%, time=50.95s\n",
            "Task 8: acc=98.61%, time=49.44s\n",
            "Task 9: acc=98.52%, time=49.52s\n",
            "Task 10: acc=98.61%, time=49.18s\n",
            "\n",
            "** Summary (epochs=15, batch=128) -> Mean acc: 98.57% | Total time: 502.09s **\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model structure improvement+that good combination + compressor + good paramater epoch=20 batch 128 (no need)"
      ],
      "metadata": {
        "id": "TfTK8EcPcWDT"
      },
      "id": "TfTK8EcPcWDT"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== TorchMLP (RMSprop + FE + SiLU + Bottleneck + WeightNorm + LabelSmoothing + WarmupCosine + Dynamic INT8) =====\n",
        "import time, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# 限制 CPU 线程（2 核约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ------------------------- I/O 与预处理 -------------------------\n",
        "def _as_float_01(x_np):\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np):\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "def _l2_per_sample(x, eps=1e-6):\n",
        "    orig = x.shape\n",
        "    if x.dim() == 3:\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig) == 3:\n",
        "        x = x.view(orig)\n",
        "    return x\n",
        "\n",
        "# ------------------------- 损失与调度 -------------------------\n",
        "class SmoothCE(nn.Module):\n",
        "    def __init__(self, eps=0.05, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.C = num_classes\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        logp = self.logsoftmax(logits)\n",
        "        with torch.no_grad():\n",
        "            t = torch.zeros_like(logp).scatter_(1, target.view(-1, 1), 1.0)\n",
        "            t = t * (1 - self.eps) + self.eps / self.C\n",
        "        return (-t * logp).sum(dim=1).mean()\n",
        "\n",
        "class WarmupCosine:\n",
        "    def __init__(self, optimizer, total_epochs, warmup_epochs=1):\n",
        "        self.opt = optimizer\n",
        "        self.total = total_epochs\n",
        "        self.warm = warmup_epochs\n",
        "        self.base = [g['lr'] for g in optimizer.param_groups]\n",
        "        self.ep = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.ep += 1\n",
        "        for i, g in enumerate(self.opt.param_groups):\n",
        "            base = self.base[i]\n",
        "            if self.ep <= self.warm:\n",
        "                lr = base * self.ep / max(1, self.warm)\n",
        "            else:\n",
        "                prog = (self.ep - self.warm) / max(1, (self.total - self.warm))\n",
        "                lr = 0.5 * base * (1 + np.cos(np.pi * prog))\n",
        "            g['lr'] = lr\n",
        "\n",
        "# ------------------------- 模型 -------------------------\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, d=256, b=64):\n",
        "        super().__init__()\n",
        "        self.bn1 = nn.BatchNorm1d(d)\n",
        "        self.act1 = nn.SiLU(inplace=True)\n",
        "        self.fc1 = nn.Linear(d, b, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(b)\n",
        "        self.act2 = nn.SiLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(b, d, bias=False)\n",
        "        nn.init.kaiming_uniform_(self.fc1.weight, a=0, nonlinearity='relu')\n",
        "        nn.init.kaiming_uniform_(self.fc2.weight, a=0, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.fc1(self.act1(self.bn1(x)))\n",
        "        h = self.fc2(self.act2(self.bn2(h)))\n",
        "        return x + h\n",
        "\n",
        "class _MLP_ResBN(nn.Module):\n",
        "    def __init__(self, in_dim=784, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, 256, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.act1 = nn.SiLU(inplace=True)\n",
        "        self.block = Bottleneck(d=256, b=64)\n",
        "        self.fc2 = nn.Linear(256, 128, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.act2 = nn.ReLU(inplace=True)\n",
        "        head = nn.Linear(128, out_dim, bias=True)\n",
        "        self.head = nn.utils.weight_norm(head)\n",
        "        for m in [self.fc1, self.fc2, head]:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, a=0, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 3:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        x = self.act1(self.bn1(self.fc1(x)))\n",
        "        x = self.block(x)\n",
        "        x = self.act2(self.bn2(self.fc2(x)))\n",
        "        return self.head(x)\n",
        "\n",
        "# ------------------------- Agent（含 dynamic INT8 压缩器） -------------------------\n",
        "class TorchMLP:\n",
        "    def __init__(self, output_dim=10, seed=42, epochs=20, batch_size=128, lr=1e-3,\n",
        "                 val_ratio=0.2, weight_decay=0.0):\n",
        "        self.output_dim = output_dim\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.val_ratio = val_ratio\n",
        "        self.weight_decay = weight_decay\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.model_int8 = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_ResBN(in_dim=784, out_dim=self.output_dim).to(self.device)\n",
        "        self.model_int8 = None\n",
        "\n",
        "    def _make_loader(self, X, y, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(\n",
        "            ds, batch_size=self.batch_size, shuffle=shuffle,\n",
        "            drop_last=False, num_workers=0, pin_memory=False\n",
        "        )\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X = _as_float_01(X_train); y = _labels_1d(y_train)\n",
        "        n_total = X.shape[0]; n_val = int(self.val_ratio * n_total)\n",
        "        idx = np.arange(n_total); np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]; tr_idx = idx[n_val:]\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "\n",
        "        opt = optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99,\n",
        "                            momentum=0.0, centered=False, weight_decay=self.weight_decay)\n",
        "        sch = WarmupCosine(opt, total_epochs=self.epochs, warmup_epochs=1)\n",
        "        crit = SmoothCE(0.05, self.output_dim)\n",
        "\n",
        "        self.model.train()\n",
        "        for _ in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            sch.step()\n",
        "\n",
        "    def compress_dynamic_int8(self):\n",
        "        import torch.quantization as tq\n",
        "        mdl = self.model.to(\"cpu\").eval()\n",
        "        try:\n",
        "            nn.utils.remove_weight_norm(mdl.head)\n",
        "        except Exception:\n",
        "            pass\n",
        "        self.model_int8 = tq.quantize_dynamic(\n",
        "            mdl, {nn.Linear}, dtype=torch.qint8, inplace=False\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test):\n",
        "        X = _as_float_01(X_test)\n",
        "        X = _l2_per_sample(X)\n",
        "        mdl = self.model_int8 if self.model_int8 is not None else self.model\n",
        "        mdl.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = mdl(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ------------------------- 单组评测：epoch=15, batch=128 -------------------------\n",
        "from permuted_mnist.env.permuted_mnist import PermutedMNISTEnv\n",
        "env_gs = PermutedMNISTEnv(number_episodes=10)\n",
        "env_gs.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42, epochs=15, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "print(\"\\n=== TorchMLP (Dynamic INT8, epochs=20, batch=128) ===\")\n",
        "tid = 1\n",
        "while True:\n",
        "    task = env_gs.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "    agent.reset()\n",
        "\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    agent.compress_dynamic_int8()\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "\n",
        "    acc = env_gs.evaluate(preds, task['y_test'])\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {tid}: acc={acc:.2%}, time={elapsed:.2f}s\")\n",
        "    tid += 1\n",
        "\n",
        "mean_acc = float(np.mean(accs))\n",
        "total_t = float(np.sum(times))\n",
        "print(f\"\\n** Summary (epochs=20, batch=128) -> Mean acc: {mean_acc:.2%} | Total time: {total_t:.2f}s **\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YPskzUbZMMs",
        "outputId": "f319022d-e5b4-49ad-8a0e-3507f8920292"
      },
      "id": "1YPskzUbZMMs",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TorchMLP (Dynamic INT8, epochs=20, batch=128) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3407324467.py:169: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  self.model_int8 = tq.quantize_dynamic(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1: acc=98.64%, time=49.20s\n",
            "Task 2: acc=98.48%, time=49.55s\n",
            "Task 3: acc=98.63%, time=49.30s\n",
            "Task 4: acc=98.52%, time=51.28s\n",
            "Task 5: acc=98.61%, time=49.85s\n",
            "Task 6: acc=98.59%, time=49.47s\n",
            "Task 7: acc=98.45%, time=50.42s\n",
            "Task 8: acc=98.61%, time=50.18s\n",
            "Task 9: acc=98.52%, time=49.36s\n",
            "Task 10: acc=98.61%, time=50.09s\n",
            "\n",
            "** Summary (epochs=20, batch=128) -> Mean acc: 98.57% | Total time: 498.70s **\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TorchMLP (RMSprop + FE + SiLU + Bottleneck + WeightNorm + LabelSmoothing + WarmupCosine + EarlyStopping + Dynamic INT8)"
      ],
      "metadata": {
        "id": "-ERCMEfbIUBO"
      },
      "id": "-ERCMEfbIUBO"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# 限制 CPU 线程（2 核约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ------------------------- I/O 与预处理 -------------------------\n",
        "def _as_float_01(x_np):\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np):\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "def _l2_per_sample(x, eps=1e-6):\n",
        "    orig = x.shape\n",
        "    if x.dim() == 3:\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig) == 3:\n",
        "        x = x.view(orig)\n",
        "    return x\n",
        "\n",
        "# ------------------------- 损失与调度 -------------------------\n",
        "class SmoothCE(nn.Module):\n",
        "    def __init__(self, eps=0.05, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.C = num_classes\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        logp = self.logsoftmax(logits)\n",
        "        with torch.no_grad():\n",
        "            t = torch.zeros_like(logp).scatter_(1, target.view(-1, 1), 1.0)\n",
        "            t = t * (1 - self.eps) + self.eps / self.C\n",
        "        return (-t * logp).sum(dim=1).mean()\n",
        "\n",
        "class WarmupCosine:\n",
        "    def __init__(self, optimizer, total_epochs, warmup_epochs=1):\n",
        "        self.opt = optimizer\n",
        "        self.total = total_epochs\n",
        "        self.warm = warmup_epochs\n",
        "        self.base = [g['lr'] for g in optimizer.param_groups]\n",
        "        self.ep = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.ep += 1\n",
        "        for i, g in enumerate(self.opt.param_groups):\n",
        "            base = self.base[i]\n",
        "            if self.ep <= self.warm:\n",
        "                lr = base * self.ep / max(1, self.warm)\n",
        "            else:\n",
        "                prog = (self.ep - self.warm) / max(1, (self.total - self.warm))\n",
        "                lr = 0.5 * base * (1 + np.cos(np.pi * prog))\n",
        "            g['lr'] = lr\n",
        "\n",
        "# ------------------------- 模型 -------------------------\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, d=256, b=64):\n",
        "        super().__init__()\n",
        "        self.bn1 = nn.BatchNorm1d(d)\n",
        "        self.act1 = nn.SiLU(inplace=True)\n",
        "        self.fc1 = nn.Linear(d, b, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(b)\n",
        "        self.act2 = nn.SiLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(b, d, bias=False)\n",
        "        nn.init.kaiming_uniform_(self.fc1.weight, a=0, nonlinearity='relu')\n",
        "        nn.init.kaiming_uniform_(self.fc2.weight, a=0, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.fc1(self.act1(self.bn1(x)))\n",
        "        h = self.fc2(self.act2(self.bn2(h)))\n",
        "        return x + h\n",
        "\n",
        "class _MLP_ResBN(nn.Module):\n",
        "    def __init__(self, in_dim=784, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, 256, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.act1 = nn.SiLU(inplace=True)\n",
        "        self.block = Bottleneck(d=256, b=64)\n",
        "        self.fc2 = nn.Linear(256, 128, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.act2 = nn.ReLU(inplace=True)\n",
        "        head = nn.Linear(128, out_dim, bias=True)\n",
        "        self.head = nn.utils.weight_norm(head)\n",
        "        for m in [self.fc1, self.fc2, head]:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, a=0, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 3:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        x = self.act1(self.bn1(self.fc1(x)))\n",
        "        x = self.block(x)\n",
        "        x = self.act2(self.bn2(self.fc2(x)))\n",
        "        return self.head(x)\n",
        "\n",
        "# ------------------------- Agent（含 dynamic INT8 压缩器 + EarlyStopping） -------------------------\n",
        "class TorchMLP:\n",
        "    def __init__(self,\n",
        "                 output_dim=10,\n",
        "                 seed=42,\n",
        "                 epochs=20,\n",
        "                 batch_size=128,\n",
        "                 lr=1e-3,\n",
        "                 val_ratio=0.2,\n",
        "                 weight_decay=0.0,\n",
        "                 early_stop: bool = True,\n",
        "                 patience: int = 3,\n",
        "                 min_delta: float = 0.0,\n",
        "                 monitor: str = 'val_acc'\n",
        "                 ):\n",
        "        self.output_dim = output_dim\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.val_ratio = val_ratio\n",
        "        self.weight_decay = weight_decay\n",
        "        self.early_stop = early_stop\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.monitor = monitor\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.model_int8 = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_ResBN(in_dim=784, out_dim=self.output_dim).to(self.device)\n",
        "        self.model_int8 = None\n",
        "\n",
        "    def _make_loader(self, X, y, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(\n",
        "            ds, batch_size=self.batch_size, shuffle=shuffle,\n",
        "            drop_last=False, num_workers=0, pin_memory=False\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _evaluate(self, X_val, y_val):\n",
        "        if X_val is None or y_val is None or X_val.shape[0] == 0:\n",
        "            return None, None\n",
        "        self.model.eval()\n",
        "        crit = SmoothCE(0.05, self.output_dim)\n",
        "        bs = 4096\n",
        "        n = X_val.shape[0]\n",
        "        total_loss, correct = 0.0, 0\n",
        "        for i in range(0, n, bs):\n",
        "            xb = X_val[i:i+bs]\n",
        "            logits = self.model(xb)\n",
        "            loss = crit(logits, y_val[i:i+bs])\n",
        "            total_loss += float(loss.item()) * xb.shape[0]\n",
        "            pred = torch.argmax(logits, dim=1)\n",
        "            correct += int((pred == y_val[i:i+bs]).sum().item())\n",
        "        return total_loss / n, correct / n\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X = _as_float_01(X_train); y = _labels_1d(y_train)\n",
        "        if self.val_ratio > 0.0:\n",
        "            n_total = X.shape[0]; n_val = int(self.val_ratio * n_total)\n",
        "            idx = np.arange(n_total); np.random.shuffle(idx)\n",
        "            val_idx = idx[:n_val]; tr_idx = idx[n_val:]\n",
        "            X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "            X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "        else:\n",
        "            X_tr, y_tr = X.to(self.device), y.to(self.device)\n",
        "            X_val, y_val = None, None\n",
        "\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        if X_val is not None:\n",
        "            X_val = _l2_per_sample(X_val)\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "\n",
        "        opt = optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99,\n",
        "                            momentum=0.0, centered=False, weight_decay=self.weight_decay)\n",
        "        sch = WarmupCosine(opt, total_epochs=self.epochs, warmup_epochs=1)\n",
        "        crit = SmoothCE(0.05, self.output_dim)\n",
        "\n",
        "        use_early_stop = self.early_stop and (X_val is not None) and (y_val is not None) and (X_val.shape[0] > 0)\n",
        "        best_metric, best_state, wait = None, None, 0\n",
        "\n",
        "        self.model.train()\n",
        "        for ep in range(1, self.epochs + 1):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            sch.step()\n",
        "\n",
        "            if use_early_stop:\n",
        "                val_loss, val_acc = self._evaluate(X_val, y_val)\n",
        "                metric = val_acc if self.monitor == 'val_acc' else -val_loss\n",
        "                if (best_metric is None) or (metric > best_metric + self.min_delta):\n",
        "                    best_metric = metric\n",
        "                    best_state = {k: v.detach().cpu().clone() for k, v in self.model.state_dict().items()}\n",
        "                    wait = 0\n",
        "                else:\n",
        "                    wait += 1\n",
        "                    if wait >= self.patience:\n",
        "                        if best_state is not None:\n",
        "                            self.model.load_state_dict(best_state)\n",
        "                        break\n",
        "\n",
        "        if use_early_stop and (best_state is not None):\n",
        "            self.model.load_state_dict(best_state)\n",
        "\n",
        "    def compress_dynamic_int8(self):\n",
        "        import torch.quantization as tq\n",
        "        mdl = self.model.to(\"cpu\").eval()\n",
        "        for m in mdl.modules():\n",
        "            try:\n",
        "                nn.utils.remove_weight_norm(m, name='weight')\n",
        "            except Exception:\n",
        "                pass\n",
        "        self.model_int8 = tq.quantize_dynamic(\n",
        "            mdl, {nn.Linear}, dtype=torch.qint8, inplace=False\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test):\n",
        "        X = _as_float_01(X_test)\n",
        "        X = _l2_per_sample(X)\n",
        "        mdl = self.model_int8 if self.model_int8 is not None else self.model\n",
        "        mdl.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = mdl(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ------------------------- 评测：逐任务打印 + 汇总 -------------------------\n",
        "from permuted_mnist.env.permuted_mnist import PermutedMNISTEnv\n",
        "env = PermutedMNISTEnv(number_episodes=10)\n",
        "env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(\n",
        "    output_dim=10,\n",
        "    seed=42,\n",
        "    epochs=20,             # ✅ 全部改为 20\n",
        "    batch_size=128,\n",
        "    lr=1e-3,\n",
        "    val_ratio=0.2,\n",
        "    weight_decay=0.0,\n",
        "    early_stop=True,\n",
        "    patience=3,\n",
        "    min_delta=0.0,\n",
        "    monitor='val_acc'\n",
        ")\n",
        "\n",
        "accs, times = [], []\n",
        "print(f\"\\n=== TorchMLP (RMSprop + FE + SiLU + Bottleneck + WN + LabelSmoothing + WarmupCosine + EarlyStop + Dynamic INT8)\"\n",
        "      f\"  (epochs={agent.epochs}, batch={agent.batch_size}) ===\")\n",
        "tid = 1\n",
        "\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    agent.compress_dynamic_int8()\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {tid}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    tid += 1\n",
        "\n",
        "mean_acc = float(np.mean(accs))\n",
        "std_acc  = float(np.std(accs))\n",
        "total_t  = float(np.sum(times))\n",
        "\n",
        "print(f\"\\n** Summary (epochs={agent.epochs}, batch={agent.batch_size}) -> \"\n",
        "      f\"Mean acc: {mean_acc:.2%} ± {std_acc:.2%} | Total time: {total_t:.2f}s **\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "KvI-w98d6RYi",
        "outputId": "f9626293-773c-4689-b2c1-9659cc18b67a"
      },
      "id": "KvI-w98d6RYi",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TorchMLP (RMSprop + FE + SiLU + Bottleneck + WN + LabelSmoothing + WarmupCosine + EarlyStop + Dynamic INT8)  (epochs=20, batch=128) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3839390312.py:230: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  self.model_int8 = tq.quantize_dynamic(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1: Accuracy = 98.47%, Time = 61.84s\n",
            "Task 2: Accuracy = 98.49%, Time = 56.85s\n",
            "Task 3: Accuracy = 98.08%, Time = 30.30s\n",
            "Task 4: Accuracy = 98.51%, Time = 65.79s\n",
            "Task 5: Accuracy = 98.46%, Time = 65.78s\n",
            "Task 6: Accuracy = 98.41%, Time = 59.77s\n",
            "Task 7: Accuracy = 98.44%, Time = 51.10s\n",
            "Task 8: Accuracy = 98.42%, Time = 61.54s\n",
            "Task 9: Accuracy = 98.35%, Time = 57.65s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3839390312.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/permuted_mnist/permuted_mnist/env/permuted_mnist.py\u001b[0m in \u001b[0;36mget_next_task\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Apply pixel permutation and task-specific noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mtrain_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_permute_pixels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mtest_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_permute_pixels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_episode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/permuted_mnist/permuted_mnist/env/permuted_mnist.py\u001b[0m in \u001b[0;36m_permute_pixels\u001b[0;34m(self, images, task_id)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# Add per-pixel Gaussian noise (std=0.015)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_rng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.015\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermuted_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mpermuted_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermuted_images\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TorchMLP (ENSEMBLE: Multi-Head + EMA + TempScaling)RMSprop + FE + SiLU + Bottleneck + WeightNorm + WarmupCosine + Dynamic INT8"
      ],
      "metadata": {
        "id": "Wqd-f2NJIcc_"
      },
      "id": "Wqd-f2NJIcc_"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Lightweight ensemble under 1-minute/task CPU budget.\n",
        "\n",
        "import time, math, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# ---- CPU threads (2-core constraint) ----\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ------------------------- I/O & Preprocessing -------------------------\n",
        "def _as_float_01(x_np):\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np):\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "def _l2_per_sample(x, eps=1e-6):\n",
        "    orig = x.shape\n",
        "    if x.dim() == 3:\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig) == 3:\n",
        "        x = x.view(orig)\n",
        "    return x\n",
        "\n",
        "# ------------------------- Loss & Schedulers -------------------------\n",
        "class SmoothCE(nn.Module):\n",
        "    def __init__(self, eps=0.05, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.C = num_classes\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, logits, target):\n",
        "        logp = self.logsoftmax(logits)\n",
        "        with torch.no_grad():\n",
        "            t = torch.zeros_like(logp).scatter_(1, target.view(-1, 1), 1.0)\n",
        "            t = t * (1 - self.eps) + self.eps / self.C\n",
        "        return (-t * logp).sum(dim=1).mean()\n",
        "\n",
        "class WarmupCosine:\n",
        "    def __init__(self, optimizer, total_epochs, warmup_epochs=1):\n",
        "        self.opt = optimizer\n",
        "        self.total = total_epochs\n",
        "        self.warm = warmup_epochs\n",
        "        self.base = [g['lr'] for g in optimizer.param_groups]\n",
        "        self.ep = 0\n",
        "    def step(self):\n",
        "        self.ep += 1\n",
        "        for i, g in enumerate(self.opt.param_groups):\n",
        "            base = self.base[i]\n",
        "            if self.ep <= self.warm:\n",
        "                lr = base * self.ep / max(1, self.warm)\n",
        "            else:\n",
        "                prog = (self.ep - self.warm) / max(1, (self.total - self.warm))\n",
        "                lr = 0.5 * base * (1 + np.cos(np.pi * prog))\n",
        "            g['lr'] = lr\n",
        "\n",
        "# ------------------------- Model (Trunk + Multi-Head) -------------------------\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, d=256, b=64, p_drop=0.0):\n",
        "        super().__init__()\n",
        "        self.bn1 = nn.BatchNorm1d(d)\n",
        "        self.act1 = nn.SiLU(inplace=True)\n",
        "        self.fc1 = nn.Linear(d, b, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(b)\n",
        "        self.act2 = nn.SiLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(b, d, bias=False)\n",
        "        self.drop = nn.Dropout(p_drop)\n",
        "        nn.init.kaiming_uniform_(self.fc1.weight, a=0, nonlinearity='relu')\n",
        "        nn.init.kaiming_uniform_(self.fc2.weight, a=0, nonlinearity='relu')\n",
        "    def forward(self, x):\n",
        "        h = self.fc1(self.act1(self.bn1(x)))\n",
        "        h = self.fc2(self.act2(self.bn2(h)))\n",
        "        h = self.drop(h)\n",
        "        return x + h\n",
        "\n",
        "class Trunk(nn.Module):\n",
        "    def __init__(self, in_dim=784, feat_dim=128, p_drop=0.10):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, 256, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.act1 = nn.SiLU(inplace=True)\n",
        "        self.block = Bottleneck(d=256, b=64, p_drop=p_drop)  # light stochasticity\n",
        "        self.fc2 = nn.Linear(256, feat_dim, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(feat_dim)\n",
        "        self.act2 = nn.ReLU(inplace=True)\n",
        "        for m in [self.fc1, self.fc2]:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, a=0, nonlinearity='relu')\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 3:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        x = self.act1(self.bn1(self.fc1(x)))\n",
        "        x = self.block(x)\n",
        "        x = self.act2(self.bn2(self.fc2(x)))\n",
        "        return x  # [N, feat_dim]\n",
        "\n",
        "class MLP_MultiHead(nn.Module):\n",
        "    def __init__(self, in_dim=784, out_dim=10, feat_dim=128, n_heads=3):\n",
        "        super().__init__()\n",
        "        self.trunk = Trunk(in_dim=in_dim, feat_dim=feat_dim, p_drop=0.10)\n",
        "        heads = []\n",
        "        for _ in range(n_heads):\n",
        "            head = nn.Linear(feat_dim, out_dim, bias=True)\n",
        "            nn.init.kaiming_uniform_(head.weight, a=0, nonlinearity='relu')\n",
        "            nn.init.zeros_(head.bias)\n",
        "            head = nn.utils.weight_norm(head)\n",
        "            heads.append(head)\n",
        "        self.heads = nn.ModuleList(heads)\n",
        "        self.n_heads = n_heads\n",
        "    def forward(self, x):\n",
        "        feat = self.trunk(x)\n",
        "        logits_list = [h(feat) for h in self.heads]\n",
        "        return logits_list  # list of [N, C]\n",
        "    def remove_weight_norm_all_heads(self):\n",
        "        for h in self.heads:\n",
        "            try:\n",
        "                nn.utils.remove_weight_norm(h, name='weight')\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "# ------------------------- EMA (Exponential Moving Average) -------------------------\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.995):\n",
        "        self.decay = decay\n",
        "        self.shadow = {}\n",
        "        for n, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.shadow[n] = p.detach().clone()\n",
        "    @torch.no_grad()\n",
        "    def update(self, model):\n",
        "        for n, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.shadow[n].mul_(self.decay).add_(p.detach(), alpha=1.0 - self.decay)\n",
        "    @torch.no_grad()\n",
        "    def copy_to(self, model):\n",
        "        for n, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                p.copy_(self.shadow[n])\n",
        "\n",
        "# ------------------------- Temperature Scaling (1D optimization) -------------------------\n",
        "@torch.no_grad()\n",
        "def _fit_temperature(logits, targets, max_iter=50):\n",
        "    # logits: [N, C] (float32), targets: [N] (int64)\n",
        "    # optimize scalar T > 0 minimizing NLL on validation\n",
        "    T = torch.tensor(1.0, dtype=logits.dtype, requires_grad=True)\n",
        "    opt = torch.optim.LBFGS([T], lr=0.1, max_iter=max_iter, line_search_fn='strong_wolfe')\n",
        "    nll = nn.CrossEntropyLoss()\n",
        "    def closure():\n",
        "        opt.zero_grad()\n",
        "        loss = nll(logits / T.clamp_min(1e-4), targets)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "    try:\n",
        "        opt.step(closure)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return float(T.detach().clamp_min(1e-3).cpu().item())\n",
        "\n",
        "# ------------------------- Agent with Ensemble -------------------------\n",
        "class TorchMLPEnsemble:\n",
        "    def __init__(self, output_dim=10, seed=42, epochs=12, batch_size=128, lr=1e-3,\n",
        "                 val_ratio=0.2, weight_decay=0.0, n_heads=3, ema_decay=0.995):\n",
        "        self.output_dim = output_dim\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.val_ratio = val_ratio\n",
        "        self.weight_decay = weight_decay\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.n_heads = n_heads\n",
        "        self.ema_decay = ema_decay\n",
        "        self.model = None\n",
        "        self.ema = None\n",
        "        self.model_int8 = None\n",
        "        self.temp = 1.0  # temperature for calibrated logits\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = MLP_MultiHead(in_dim=784, out_dim=self.output_dim, feat_dim=128, n_heads=self.n_heads).to(self.device)\n",
        "        self.ema = EMA(self.model, decay=self.ema_decay)\n",
        "        self.model_int8 = None\n",
        "        self.temp = 1.0\n",
        "\n",
        "    def _make_loader(self, X, y, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(\n",
        "            ds, batch_size=self.batch_size, shuffle=shuffle,\n",
        "            drop_last=False, num_workers=0, pin_memory=False\n",
        "        )\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X = _as_float_01(X_train); y = _labels_1d(y_train)\n",
        "        n_total = X.shape[0]; n_val = int(self.val_ratio * n_total)\n",
        "        idx = np.arange(n_total); np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]; tr_idx = idx[n_val:]\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # FE\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "\n",
        "        opt = optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99,\n",
        "                            momentum=0.0, centered=False, weight_decay=self.weight_decay)\n",
        "        sch = WarmupCosine(opt, total_epochs=self.epochs, warmup_epochs=1)\n",
        "        crit = SmoothCE(0.05, self.output_dim)\n",
        "\n",
        "        self.model.train()\n",
        "        for _ in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits_list = self.model(xb)\n",
        "                # Average loss across heads (diversity from independent heads)\n",
        "                loss = 0.0\n",
        "                for lg in logits_list:\n",
        "                    loss = loss + crit(lg, yb)\n",
        "                loss = loss / self.n_heads\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=3.0)\n",
        "                opt.step()\n",
        "                # EMA update\n",
        "                self.ema.update(self.model)\n",
        "            sch.step()\n",
        "\n",
        "        # Swap to EMA weights for evaluation\n",
        "        self.ema.copy_to(self.model)\n",
        "\n",
        "        # ---- Lightweight temperature scaling on validation (calibration) ----\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            # ensemble logits = average over heads\n",
        "            logits_heads = []\n",
        "            bs = 4096\n",
        "            for i in range(0, X_val.shape[0], bs):\n",
        "                lg_list = self.model(X_val[i:i+bs])  # list of [B, C]\n",
        "                lg_stack = torch.stack(lg_list, dim=0).float()        # [H, B, C]\n",
        "                lg_mean = lg_stack.mean(dim=0)                        # [B, C]\n",
        "                logits_heads.append(lg_mean)\n",
        "            val_logits = torch.cat(logits_heads, dim=0)               # [N, C]\n",
        "        self.temp = _fit_temperature(val_logits, y_val)\n",
        "\n",
        "    def compress_dynamic_int8(self):\n",
        "        # remove weight norms on heads (reparam safety) -> quantize\n",
        "        mdl = self.model.to(\"cpu\").eval()\n",
        "        mdl.remove_weight_norm_all_heads()\n",
        "        try:\n",
        "            import torch.quantization as tq\n",
        "        except Exception:\n",
        "            tq = torch.ao.quantization  # fallback\n",
        "        self.model_int8 = tq.quantize_dynamic(mdl, {nn.Linear}, dtype=torch.qint8, inplace=False)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test):\n",
        "        X = _as_float_01(X_test)\n",
        "        X = _l2_per_sample(X)\n",
        "        mdl = self.model_int8 if self.model_int8 is not None else self.model\n",
        "        mdl.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            lg_list = mdl(X[i:i+bs])\n",
        "            lg_mean = torch.stack(lg_list, dim=0).float().mean(dim=0)  # [B, C]\n",
        "            lg_mean = lg_mean / max(1e-3, self.temp)                   # calibrated\n",
        "            outs.append(torch.argmax(lg_mean, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ------------------------- Example runner (same API as before) -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    from permuted_mnist.env.permuted_mnist import PermutedMNISTEnv\n",
        "    env = PermutedMNISTEnv(number_episodes=10)\n",
        "    env.set_seed(42)\n",
        "\n",
        "    agent = TorchMLPEnsemble(\n",
        "        output_dim=10, seed=42,\n",
        "        epochs=12, batch_size=128, lr=1e-3, val_ratio=0.2,\n",
        "        n_heads=3, ema_decay=0.995\n",
        "    )\n",
        "\n",
        "    accs, times = [], []\n",
        "    print(\"\\n=== TorchMLP Ensemble (3-head + EMA + TempScaling + Dynamic INT8) ===\")\n",
        "    tid = 1\n",
        "    while True:\n",
        "        task = env.get_next_task()\n",
        "        if task is None:\n",
        "            break\n",
        "        agent.reset()\n",
        "\n",
        "        t0 = time.time()\n",
        "        agent.train(task['X_train'], task['y_train'])\n",
        "        agent.compress_dynamic_int8()\n",
        "        preds = agent.predict(task['X_test'])\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        acc = env.evaluate(preds, task['y_test'])\n",
        "        accs.append(acc); times.append(elapsed)\n",
        "        print(f\"Task {tid}: acc={acc:.2%}, time={elapsed:.2f}s\")\n",
        "        tid += 1\n",
        "\n",
        "    mean_acc = float(np.mean(accs))\n",
        "    total_t = float(np.sum(times))\n",
        "    print(f\"\\n** Summary -> Mean acc: {mean_acc:.2%} | Total time: {total_t:.2f}s **\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZKJ-PoU-Opl",
        "outputId": "7728e68b-7e1a-4996-f66d-6af1d0a54e1e"
      },
      "id": "TZKJ-PoU-Opl",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TorchMLP Ensemble (3-head + EMA + TempScaling + Dynamic INT8) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3513515495.py:263: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  self.model_int8 = tq.quantize_dynamic(mdl, {nn.Linear}, dtype=torch.qint8, inplace=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1: acc=98.67%, time=57.91s\n",
            "Task 2: acc=98.48%, time=54.77s\n",
            "Task 3: acc=98.42%, time=53.33s\n",
            "Task 4: acc=98.45%, time=53.59s\n",
            "Task 5: acc=98.50%, time=54.49s\n",
            "Task 6: acc=98.39%, time=56.84s\n",
            "Task 7: acc=98.52%, time=54.39s\n",
            "Task 8: acc=98.49%, time=53.95s\n",
            "Task 9: acc=98.55%, time=54.13s\n",
            "Task 10: acc=98.49%, time=53.07s\n",
            "\n",
            "** Summary -> Mean acc: 98.50% | Total time: 546.46s **\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lsBydcXUy5Wr",
      "metadata": {
        "id": "lsBydcXUy5Wr"
      },
      "source": [
        "FE+rmsprop+good parameter(version teammate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "airOPEyszC9z",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "airOPEyszC9z",
        "outputId": "37944240-583d-43ab-f4f5-dcf2906af00d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating DualPathMLP Agent (RMSprop + Cosine, FE=L2)\n",
            "============================================================\n",
            "Task 1: acc=98.59%, time=45.88s\n",
            "Task 2: acc=98.57%, time=41.80s\n",
            "Task 3: acc=98.48%, time=41.20s\n",
            "Task 4: acc=98.52%, time=43.42s\n",
            "Task 5: acc=98.46%, time=41.66s\n",
            "Task 6: acc=98.47%, time=43.02s\n",
            "Task 7: acc=98.46%, time=42.04s\n",
            "Task 8: acc=98.44%, time=41.58s\n",
            "Task 9: acc=98.46%, time=43.03s\n",
            "Task 10: acc=98.52%, time=41.90s\n",
            "\n",
            "Summary:\n",
            "  Mean accuracy: 98.50% ± 0.05%\n",
            "  Total time: 425.53s\n"
          ]
        }
      ],
      "source": [
        "# ======================== Agent definition (UNCHANGED) ========================\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# -------------------- utils --------------------\n",
        "def _as_float_01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "def _normalize_per_sample(X: torch.Tensor) -> torch.Tensor:\n",
        "    if X.dim() == 3:\n",
        "        X = X.view(X.size(0), -1)\n",
        "    n = X.norm(dim=1, keepdim=True).clamp_min(1e-6)\n",
        "    X = X / n\n",
        "    return X.view(-1, 28, 28)\n",
        "\n",
        "# -------------------- Dual Path MLP --------------------\n",
        "class DualPathMLP(nn.Module):\n",
        "    def __init__(self, in_dim=784, h=128, out_dim=10, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # path1\n",
        "        self.path1 = nn.Sequential(\n",
        "            nn.Linear(in_dim, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(h, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        # path2\n",
        "        self.path2 = nn.Sequential(\n",
        "            nn.Linear(in_dim, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(h, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        # output\n",
        "        self.out = nn.Linear(h, out_dim)\n",
        "\n",
        "        # 初始化\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 3:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        out1 = self.path1(x)\n",
        "        out2 = self.path2(x)\n",
        "        fused = out1 + out2  # 加法融合\n",
        "        return self.out(fused)\n",
        "\n",
        "# -------------------- Agent --------------------\n",
        "class Agent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        output_dim: int = 10,\n",
        "        seed: int | None = 42,\n",
        "        *,\n",
        "        epochs: int = 10,\n",
        "        batch_size: int = 128,\n",
        "        lr: float = 1e-3,\n",
        "        val_ratio: float = 0.2,\n",
        "        weight_decay: float = 1e-4,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        self.output_dim = output_dim\n",
        "        self.epochs = int(epochs)\n",
        "        self.batch_size = int(batch_size)\n",
        "        self.lr = float(lr)\n",
        "        self.val_ratio = float(val_ratio)\n",
        "        self.weight_decay = float(weight_decay)\n",
        "        self.dropout = dropout\n",
        "\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "            torch.manual_seed(seed)\n",
        "\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        self.model: DualPathMLP | None = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = DualPathMLP(out_dim=self.output_dim, dropout=self.dropout).to(self.device)\n",
        "\n",
        "    # -------- train & predict --------\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        assert self.model is not None\n",
        "\n",
        "        # to tensor & normalize\n",
        "        X = _as_float_01(X_train)\n",
        "        X = _normalize_per_sample(X)\n",
        "        y = _labels_1d(y_train)\n",
        "\n",
        "        # split train/val\n",
        "        n_total = X.shape[0]\n",
        "        n_val = int(self.val_ratio * n_total)\n",
        "        idx = np.arange(n_total)\n",
        "        np.random.shuffle(idx)\n",
        "        val_idx, tr_idx = idx[:n_val], idx[n_val:]\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # dataloader\n",
        "        ds = torch.utils.data.TensorDataset(X_tr, y_tr)\n",
        "        loader = torch.utils.data.DataLoader(\n",
        "            ds,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            drop_last=False,\n",
        "            num_workers=0,\n",
        "            pin_memory=False,\n",
        "        )\n",
        "\n",
        "        # optimizer / loss\n",
        "        opt = torch.optim.RMSprop(\n",
        "            self.model.parameters(),\n",
        "            lr=self.lr,\n",
        "            alpha=0.99,\n",
        "            momentum=0.9,\n",
        "            weight_decay=self.weight_decay,\n",
        "        )\n",
        "        crit = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "        # CosineAnnealingLR\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=self.epochs)\n",
        "\n",
        "        self.model.train()\n",
        "        for _ in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            scheduler.step()  # 每个 epoch 调整一次学习率\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        assert self.model is not None\n",
        "        X = _as_float_01(X_test)\n",
        "        X = _normalize_per_sample(X)\n",
        "        X = X.to(self.device)\n",
        "\n",
        "        self.model.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i : i + bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ======================== Evaluation (minimal) ========================\n",
        "import time\n",
        "from permuted_mnist.env.permuted_mnist import PermutedMNISTEnv\n",
        "\n",
        "# 环境与代理\n",
        "env = PermutedMNISTEnv(number_episodes=10)\n",
        "env.set_seed(42)\n",
        "\n",
        "agent = Agent(\n",
        "    output_dim=10,\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    lr=1e-3,\n",
        "    val_ratio=0.2,\n",
        "    weight_decay=1e-4,\n",
        "    dropout=0.1,\n",
        ")\n",
        "\n",
        "# 评测循环\n",
        "accs, times = [], []\n",
        "print(\"Evaluating DualPathMLP Agent (RMSprop + Cosine, FE=L2)\")\n",
        "print(\"=\" * 60)\n",
        "task_id = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task[\"X_train\"], task[\"y_train\"])\n",
        "    preds = agent.predict(task[\"X_test\"])\n",
        "    elapsed = time.time() - t0\n",
        "\n",
        "    acc = env.evaluate(preds, task[\"y_test\"])\n",
        "    accs.append(acc)\n",
        "    times.append(elapsed)\n",
        "    print(f\"Task {task_id}: acc={acc:.2%}, time={elapsed:.2f}s\")\n",
        "    task_id += 1\n",
        "\n",
        "print(\"\\nSummary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zw1wIGDKlTVh",
      "metadata": {
        "id": "Zw1wIGDKlTVh"
      },
      "source": [
        "#　TorchMLP＋FE　＋ｒｍｓｐｒｏｐ＋ parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6KfjXyceihqp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KfjXyceihqp",
        "outputId": "36fdf8d8-b296-4a35-cfd2-9b991a65626c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating RMSprop + Cosine | epochs=3, batch_size=128\n",
            "============================================================\n",
            "Task 1: acc=98.05%, time=8.77s\n",
            "Task 2: acc=98.11%, time=7.48s\n",
            "Task 3: acc=97.94%, time=7.66s\n",
            "Task 4: acc=98.03%, time=7.66s\n",
            "Task 5: acc=98.15%, time=7.39s\n",
            "Task 6: acc=98.08%, time=6.93s\n",
            "Task 7: acc=98.05%, time=7.00s\n",
            "Task 8: acc=97.92%, time=7.17s\n",
            "Task 9: acc=98.06%, time=7.11s\n",
            "Task 10: acc=98.09%, time=8.90s\n",
            "→ Summary: mean=98.05% ± 0.07% | total=76.08s\n",
            "\n",
            "Evaluating RMSprop + Cosine | epochs=3, batch_size=64\n",
            "============================================================\n",
            "Task 1: acc=97.96%, time=9.69s\n",
            "Task 2: acc=98.28%, time=10.18s\n",
            "Task 3: acc=98.21%, time=9.72s\n",
            "Task 4: acc=98.04%, time=9.19s\n",
            "Task 5: acc=98.25%, time=8.79s\n",
            "Task 6: acc=98.03%, time=9.58s\n",
            "Task 7: acc=98.19%, time=9.61s\n",
            "Task 8: acc=98.12%, time=10.02s\n",
            "Task 9: acc=98.02%, time=9.93s\n",
            "Task 10: acc=98.05%, time=10.07s\n",
            "→ Summary: mean=98.11% ± 0.11% | total=96.78s\n",
            "\n",
            "Evaluating RMSprop + Cosine | epochs=4, batch_size=128\n",
            "============================================================\n",
            "Task 1: acc=98.15%, time=10.31s\n",
            "Task 2: acc=98.19%, time=9.72s\n",
            "Task 3: acc=98.13%, time=9.87s\n",
            "Task 4: acc=98.19%, time=10.46s\n",
            "Task 5: acc=98.22%, time=10.05s\n",
            "Task 6: acc=98.29%, time=9.43s\n",
            "Task 7: acc=98.13%, time=8.73s\n",
            "Task 8: acc=98.22%, time=9.07s\n",
            "Task 9: acc=98.09%, time=8.69s\n",
            "Task 10: acc=98.24%, time=9.37s\n",
            "→ Summary: mean=98.19% ± 0.06% | total=95.69s\n",
            "\n",
            "Evaluating RMSprop + Cosine | epochs=4, batch_size=64\n",
            "============================================================\n",
            "Task 1: acc=98.25%, time=13.00s\n",
            "Task 2: acc=98.40%, time=12.93s\n",
            "Task 3: acc=98.26%, time=14.40s\n",
            "Task 4: acc=98.33%, time=13.77s\n",
            "Task 5: acc=98.25%, time=12.71s\n",
            "Task 6: acc=98.19%, time=12.64s\n",
            "Task 7: acc=98.26%, time=12.82s\n",
            "Task 8: acc=98.27%, time=12.53s\n",
            "Task 9: acc=98.17%, time=12.91s\n",
            "Task 10: acc=98.34%, time=13.02s\n",
            "→ Summary: mean=98.27% ± 0.07% | total=130.72s\n",
            "\n",
            "=== Grid Search Summary (RMSprop + FE + Cosine) ===\n",
            "epochs= 3, batch=128 | acc=98.05% ± 0.07% | time=76.08s\n",
            "epochs= 3, batch= 64 | acc=98.11% ± 0.11% | time=96.78s\n",
            "epochs= 4, batch=128 | acc=98.19% ± 0.06% | time=95.69s\n",
            "epochs= 4, batch= 64 | acc=98.27% ± 0.07% | time=130.72s\n"
          ]
        }
      ],
      "source": [
        "# ===== Grid Search over (epochs, batch_size) for RMSprop + FE =====\n",
        "import time, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np):\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0: x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np):\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "def _l2_per_sample(x, eps=1e-6):\n",
        "    orig = x.shape\n",
        "    if x.dim()==3: x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig)==3: x = x.view(orig)\n",
        "    return x\n",
        "\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"784 -> 256 -> 128 -> 10, BatchNorm + ReLU\"\"\"\n",
        "    def __init__(self, in_dim=784, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 256, bias=False), nn.BatchNorm1d(256), nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 128, bias=False),    nn.BatchNorm1d(128), nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, out_dim, bias=True)\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "    def forward(self, x):\n",
        "        if x.dim()==3: x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "class TorchMLP_RMSprop:\n",
        "    def __init__(self, output_dim=10, seed=42, epochs=3, batch_size=128,\n",
        "                 lr=1e-3, val_ratio=0.2, weight_decay=0.0, use_cosine=True):\n",
        "        self.output_dim=output_dim; self.epochs=epochs; self.batch_size=batch_size\n",
        "        self.lr=lr; self.val_ratio=val_ratio; self.weight_decay=weight_decay\n",
        "        self.use_cosine=use_cosine\n",
        "        self.device=torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model=None; self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model=_MLP_BN(784, self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X, y, shuffle=True):\n",
        "        ds=torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X=_as_float_01(X_train); y=_labels_1d(y_train)\n",
        "        n_total=X.shape[0]; n_val=int(self.val_ratio*n_total)\n",
        "        idx=np.arange(n_total); np.random.shuffle(idx)\n",
        "        val_idx=idx[:n_val]; tr_idx=idx[n_val:]\n",
        "        X_tr, y_tr=X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val=X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "        X_tr=_l2_per_sample(X_tr); X_val=_l2_per_sample(X_val)\n",
        "\n",
        "        loader=self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt=optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99,\n",
        "                          momentum=0.0, centered=False, weight_decay=self.weight_decay)\n",
        "        sch=optim.lr_scheduler.CosineAnnealingLR(opt, T_max=self.epochs, eta_min=0.0) if self.use_cosine else None\n",
        "        crit=nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for _ in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits=self.model(xb)\n",
        "                loss=crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            if sch is not None: sch.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test):\n",
        "        X=_as_float_01(X_test).to(self.device)\n",
        "        X=_l2_per_sample(X)\n",
        "        self.model.eval()\n",
        "        bs=4096; outs=[]\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits=self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ====== 组合测试 ======\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "param_grid = [\n",
        "    (3, 128),\n",
        "    (3, 64),\n",
        "    (4, 128),\n",
        "    (4, 64),\n",
        "]\n",
        "\n",
        "results = []\n",
        "for ep, bs in param_grid:\n",
        "    agent = TorchMLP_RMSprop(output_dim=10, seed=42,\n",
        "                             epochs=ep, batch_size=bs, lr=1e-3,\n",
        "                             val_ratio=0.2, weight_decay=0.0,\n",
        "                             use_cosine=True)\n",
        "    accs, times = [], []\n",
        "    print(f\"\\nEvaluating RMSprop + Cosine | epochs={ep}, batch_size={bs}\")\n",
        "    print(\"=\"*60)\n",
        "    task_num = 1\n",
        "    env.reset()\n",
        "    while True:\n",
        "        task = env.get_next_task()\n",
        "        if task is None: break\n",
        "        agent.reset()\n",
        "        t0 = time.time()\n",
        "        agent.train(task['X_train'], task['y_train'])\n",
        "        preds = agent.predict(task['X_test'])\n",
        "        elapsed = time.time() - t0\n",
        "        acc = env.evaluate(preds, task['y_test'])\n",
        "        accs.append(acc); times.append(elapsed)\n",
        "        print(f\"Task {task_num}: acc={acc:.2%}, time={elapsed:.2f}s\")\n",
        "        task_num += 1\n",
        "    mean_acc, std_acc = np.mean(accs), np.std(accs)\n",
        "    total_time = np.sum(times)\n",
        "    print(f\"→ Summary: mean={mean_acc:.2%} ± {std_acc:.2%} | total={total_time:.2f}s\")\n",
        "    results.append((ep, bs, mean_acc, std_acc, total_time))\n",
        "\n",
        "print(\"\\n=== Grid Search Summary (RMSprop + FE + Cosine) ===\")\n",
        "for ep, bs, m, s, t in results:\n",
        "    print(f\"epochs={ep:>2}, batch={bs:>3} | acc={m:.2%} ± {s:.2%} | time={t:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SzuIh8CmQ-gD",
      "metadata": {
        "id": "SzuIh8CmQ-gD"
      },
      "source": [
        "#　×TorchMLP　par＋L２nor＋early stopping conservative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NQkqtbgFQ7ho",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQkqtbgFQ7ho",
        "outputId": "facc3269-517b-447e-9804-d0a8a9a1f8df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating TorchMLP Agent (BN, 2x100) + L2 FE + Conservative Early Stop (small-val)\n",
            "==================================================\n",
            "Task 1: Accuracy = 96.95%, Time = 5.53s\n",
            "Task 2: Accuracy = 96.83%, Time = 5.79s\n",
            "Task 3: Accuracy = 96.89%, Time = 6.27s\n",
            "Task 4: Accuracy = 96.85%, Time = 6.42s\n",
            "Task 5: Accuracy = 97.20%, Time = 5.40s\n",
            "Task 6: Accuracy = 97.15%, Time = 5.39s\n",
            "Task 7: Accuracy = 96.87%, Time = 6.21s\n",
            "Task 8: Accuracy = 97.04%, Time = 6.22s\n",
            "Task 9: Accuracy = 97.11%, Time = 5.39s\n",
            "Task 10: Accuracy = 96.94%, Time = 5.44s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 96.98% ± 0.13%\n",
            "  Total time: 58.06s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP-BN (2x100, BN, ReLU; Adam lr=1e-3; bs=128; epochs=3; val=0.2)\n",
        "#      + FE: per-sample L2 normalization\n",
        "#      + Conservative Early Stop (small-val check, best-weight restore) =====\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 限制CPU线程（贴合评测约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,28,28)/(N,784) np.uint8/float -> torch.float32 in [0,1]\"\"\"\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,) or (N,1) -> (N,) int64\"\"\"\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# ------------ FE: per-sample L2 normalization ------------\n",
        "def _l2_per_sample(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"对每一张图做 L2 归一化；返回与输入相同形状\"\"\"\n",
        "    orig_shape = x.shape\n",
        "    if x.dim() == 3:  # (N,28,28) -> (N,784)\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig_shape) == 3:\n",
        "        x = x.view(orig_shape)\n",
        "    return x\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"784 -> 100 -> 100 -> 10 with BatchNorm + ReLU\"\"\"\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h, out_dim),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 3:           # (N,28,28)\n",
        "            x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    Torch MLP with BatchNorm（参数保持不变） + 方案C FE + 保守早停：\n",
        "      - 结构/超参不变：2×100+BN, Adam(1e-3), bs=128, epochs=3, val=0.2\n",
        "      - FE：每样本 L2 归一化（train/test 一致）\n",
        "      - EarlyStopping：仅在 epoch1/epoch2 用小子集验证；若提升 < min_delta 则不跑 epoch3\n",
        "        并恢复 best 权重；其余流程不变。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dim: int = 10, seed: int = 42,\n",
        "                 epochs: int = 3, batch_size: int = 128,\n",
        "                 lr: float = 1e-3, val_ratio: float = 0.2, weight_decay: float = 0.0,\n",
        "                 es_min_delta: float = 0.0005, es_max_eval: int = 4000):\n",
        "        self.output_dim   = output_dim\n",
        "        self.epochs       = int(epochs)          # 3（不变）\n",
        "        self.batch_size   = int(batch_size)      # 128（不变）\n",
        "        self.lr           = float(lr)            # 1e-3（不变）\n",
        "        self.val_ratio    = float(val_ratio)     # 0.2（不变）\n",
        "        self.weight_decay = float(weight_decay)  # 0.0（不变）\n",
        "\n",
        "        # 早停相关（仅控制是否进入第3轮；不改变上限）\n",
        "        self.es_min_delta = float(es_min_delta)  # 0.05% 提升阈值\n",
        "        self.es_max_eval  = int(es_max_eval)     # 验证子集上限（降低验证时间）\n",
        "\n",
        "        self.device       = torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_BN(in_dim=784, h=100, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X: torch.Tensor, y: torch.Tensor, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _val_acc_small(self, X_val: torch.Tensor, y_val: torch.Tensor) -> float:\n",
        "        \"\"\"仅用小子集快速验证，用于决定是否进入 epoch3\"\"\"\n",
        "        n = X_val.shape[0]\n",
        "        m = min(self.es_max_eval, n)\n",
        "        Xs = X_val[:m]; ys = y_val[:m]\n",
        "        self.model.eval()\n",
        "        bs_eval = 16384\n",
        "        correct = 0\n",
        "        for i in range(0, m, bs_eval):\n",
        "            logits = self.model(Xs[i:i+bs_eval])\n",
        "            pred = logits.argmax(1)\n",
        "            correct += (pred == ys[i:i+bs_eval]).sum().item()\n",
        "        self.model.train()\n",
        "        return correct / float(m)\n",
        "\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        # === 数据准备（不变） ===\n",
        "        X = _as_float_01(X_train)\n",
        "        y = _labels_1d(y_train)\n",
        "\n",
        "        n_total = X.shape[0]\n",
        "        n_val   = int(self.val_ratio * n_total)\n",
        "        idx     = np.random.permutation(n_total)\n",
        "        val_idx = idx[:n_val]; tr_idx = idx[n_val:]\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # 方案 C：每样本 L2 归一化\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        # 早停状态（仅比较 ep1 vs ep2）\n",
        "        best_acc = -1.0\n",
        "        best_state = None\n",
        "        acc_ep1 = None\n",
        "\n",
        "        self.model.train()\n",
        "        for ep in range(self.epochs):\n",
        "            # — 训练一个 epoch —\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "            # — 只在 ep==0 和 ep==1 做一次小验证 —\n",
        "            if ep <= 1:\n",
        "                val_acc = self._val_acc_small(X_val, y_val)\n",
        "                if val_acc > best_acc + self.es_min_delta:\n",
        "                    best_acc = val_acc\n",
        "                    best_state = {k: v.detach().cpu().clone() for k, v in self.model.state_dict().items()}\n",
        "\n",
        "                if ep == 0:\n",
        "                    acc_ep1 = val_acc\n",
        "                else:\n",
        "                    # 若第二轮较第一轮提升不足阈值 ⇒ 不进入第三轮\n",
        "                    if val_acc < (acc_ep1 + self.es_min_delta):\n",
        "                        break  # 结束训练（最多跑到2个epoch）\n",
        "\n",
        "        # 恢复历史最优权重\n",
        "        if best_state is not None:\n",
        "            self.model.load_state_dict(best_state)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        X = _l2_per_sample(X)  # FE 与训练一致\n",
        "        self.model.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ===== Evaluate (same params; accuracy-first early stop) =====\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2,\n",
        "                 es_min_delta=0.0005, es_max_eval=4000)\n",
        "\n",
        "accs, times = [], []\n",
        "print(\"Evaluating TorchMLP Agent (BN, 2x100) + L2 FE + Conservative Early Stop (small-val)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TqD0weGAfQrH",
      "metadata": {
        "id": "TqD0weGAfQrH"
      },
      "source": [
        "#×TorchMLP par+early stopping\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jTbK4OFjfQSK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTbK4OFjfQSK",
        "outputId": "f3b4db15-a915-4295-a9c1-5b6de6ab9be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating TorchMLP Agent (BN + Early Stopping)\n",
            "==================================================\n",
            "Task 1: Accuracy = 97.22%, Time = 5.92s\n",
            "Task 2: Accuracy = 97.12%, Time = 5.89s\n",
            "Task 3: Accuracy = 97.16%, Time = 5.89s\n",
            "Task 4: Accuracy = 97.05%, Time = 5.93s\n",
            "Task 5: Accuracy = 96.89%, Time = 5.89s\n",
            "Task 6: Accuracy = 97.37%, Time = 6.09s\n",
            "Task 7: Accuracy = 97.20%, Time = 5.89s\n",
            "Task 8: Accuracy = 97.18%, Time = 5.89s\n",
            "Task 9: Accuracy = 97.08%, Time = 5.89s\n",
            "Task 10: Accuracy = 96.99%, Time = 5.89s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 97.13% ± 0.13%\n",
            "  Total time: 59.15s\n"
          ]
        }
      ],
      "source": [
        "# ==== TorchMLP-BN with Early Stopping + Evaluation (single cell) ====\n",
        "import time, numpy as np, torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 保守地限制CPU线程（与竞赛环境一致）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# --------- utils ----------\n",
        "def _to_float01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:  # 如果是0-255，缩放到0-1\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _fit_scalar_norm(x: torch.Tensor):\n",
        "    m = float(x.mean()); s = float(x.std())\n",
        "    return m, (s if s >= 1e-6 else 1.0)\n",
        "\n",
        "# --------- model ----------\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"784 -> 100 -> 100 -> 10 with BatchNorm + ReLU\"\"\"\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h, out_dim),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 3:  # (N,28,28)\n",
        "            x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "# --------- agent ----------\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    Torch MLP (2x100 with BN) + 早停 + 可选时间护栏\n",
        "    - Adam(lr=1e-3), batch=128, 最多epochs很大，靠early stopping/时间护栏停止\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim=10, seed=42,\n",
        "                 lr=1e-3, batch_size=128, val_ratio=0.2,\n",
        "                 patience=2, min_delta=0.001,\n",
        "                 time_budget=5.8, min_train_seconds=2.0):\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.output_dim = output_dim\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.val_ratio = val_ratio\n",
        "        self.patience = patience            # 连续多少次无明显提升则停\n",
        "        self.min_delta = min_delta          # 最小提升幅度(绝对提升)\n",
        "        self.time_budget = time_budget      # 每任务训练时间上限（秒）\n",
        "        self.min_train_seconds = min_train_seconds\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_BN(out_dim=self.output_dim).to(self.device)\n",
        "        self.mean_, self.std_ = 0.0, 1.0\n",
        "\n",
        "    def _loader(self, X, y, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        t0 = time.time()\n",
        "        X = _to_float01(X_train); y = _labels_1d(y_train)\n",
        "        n = X.shape[0]\n",
        "        n_val = max(1000, int(self.val_ratio * n))\n",
        "        idx = np.arange(n); np.random.shuffle(idx)\n",
        "        val_idx, tr_idx = idx[:n_val], idx[n_val:]\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # 标量标准化\n",
        "        self.mean_, self.std_ = _fit_scalar_norm(X_tr)\n",
        "        X_tr = (X_tr - self.mean_) / self.std_\n",
        "        X_val = (X_val - self.mean_) / self.std_\n",
        "\n",
        "        loader = self._loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        best_acc, no_improve = 0.0, 0\n",
        "        self.model.train()\n",
        "        # 不限定 epoch 上限，靠时间护栏和早停控制\n",
        "        while True:\n",
        "            # 一个 epoch\n",
        "            for xb, yb in loader:\n",
        "                if time.time() - t0 > self.time_budget:\n",
        "                    break\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                loss = crit(self.model(xb), yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            if time.time() - t0 > self.time_budget:\n",
        "                break\n",
        "\n",
        "            # 验证（大批量）\n",
        "            with torch.no_grad():\n",
        "                self.model.eval()\n",
        "                bs_eval = 8192\n",
        "                correct, total = 0, 0\n",
        "                for i in range(0, X_val.shape[0], bs_eval):\n",
        "                    logits = self.model(X_val[i:i+bs_eval])\n",
        "                    pred = logits.argmax(dim=1)\n",
        "                    correct += (pred == y_val[i:i+bs_eval]).sum().item()\n",
        "                    total   += min(bs_eval, X_val.shape[0]-i)\n",
        "                val_acc = correct / max(1, total)\n",
        "                self.model.train()\n",
        "\n",
        "            if val_acc > best_acc + self.min_delta:\n",
        "                best_acc = val_acc\n",
        "                no_improve = 0\n",
        "            else:\n",
        "                no_improve += 1\n",
        "\n",
        "            if (no_improve >= self.patience) and (time.time() - t0 >= self.min_train_seconds):\n",
        "                break\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = _to_float01(X_test).to(self.device)\n",
        "        X = (X - self.mean_) / self.std_\n",
        "        self.model.eval()\n",
        "        bs = 8192\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ================== Evaluate ==================\n",
        "import numpy as np, time as _time\n",
        "\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 lr=1e-3, batch_size=128, val_ratio=0.2,\n",
        "                 patience=2, min_delta=0.001,\n",
        "                 time_budget=5.8, min_train_seconds=2.0)\n",
        "\n",
        "accs, times = [], []\n",
        "print(\"Evaluating TorchMLP Agent (BN + Early Stopping)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "    agent.reset()\n",
        "    t0 = _time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = _time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ddqgua2YN1q9",
      "metadata": {
        "id": "Ddqgua2YN1q9"
      },
      "source": [
        "# NewAgent1 Not good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GkOLnllZNwhM",
      "metadata": {
        "id": "GkOLnllZNwhM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class _MLP(nn.Module):\n",
        "    def __init__(self, in_dim=784, h1=1024, h2=512, out_dim=10, p=0.1):\n",
        "        super().__init__()\n",
        "        # ReLU 版本，和 Kaiming(relu) 初始化配套，兼容所有常见 torch 版本\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h1), nn.ReLU(), nn.Dropout(p),\n",
        "            nn.Linear(h1, h2),     nn.ReLU(), nn.Dropout(p),\n",
        "            nn.Linear(h2, out_dim)\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2nDNMst8NaIj",
      "metadata": {
        "id": "2nDNMst8NaIj"
      },
      "outputs": [],
      "source": [
        "import time, math, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# 限制CPU线程，贴合评测（2核）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "class _MLP(nn.Module):\n",
        "    def __init__(self, in_dim=784, h1=1024, h2=512, out_dim=10, p=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h1), nn.GELU(), nn.Dropout(p),\n",
        "            nn.Linear(h1, h2),     nn.GELU(), nn.Dropout(p),\n",
        "            nn.Linear(h2, out_dim)\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"gelu\")\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class HighAccuracyAgent:\n",
        "    \"\"\"PermutedMNIST 高精度/限时 Agent：目标 ≥97%，<60s/任务（CPU）\"\"\"\n",
        "    def __init__(self, output_dim: int = 10, seed: int = 42):\n",
        "        self.output_dim = output_dim\n",
        "        self.seed = seed\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        self.time_budget = 58.0  # 留2秒给预测/收尾\n",
        "        self.cfg = dict(in_dim=784, h1=1024, h2=512, p=0.10,\n",
        "                        lr=3e-3, wd=1e-4, bs=2048,\n",
        "                        max_epochs=12, min_epochs=3, warmup_steps=10)\n",
        "        self.reset()\n",
        "\n",
        "    def _set_seed(self, s):\n",
        "        np.random.seed(s); torch.manual_seed(s)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _fit_norm(self, X_flat):\n",
        "        m, s = X_flat.mean().item(), X_flat.std().item()\n",
        "        self.mean_ = m; self.std_ = (s if s >= 1e-6 else 1.0)\n",
        "\n",
        "    def _make_loader(self, X, y, bs, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=bs, shuffle=shuffle,\n",
        "                                           drop_last=False, num_workers=0, pin_memory=False)\n",
        "\n",
        "    def reset(self):\n",
        "        self._set_seed(self.seed)\n",
        "        self.model = _MLP(self.cfg[\"in_dim\"], self.cfg[\"h1\"], self.cfg[\"h2\"],\n",
        "                          self.output_dim, self.cfg[\"p\"]).to(self.device)\n",
        "        self.mean_, self.std_ = None, None\n",
        "\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        t0 = time.time()\n",
        "        X = torch.from_numpy(X_train.reshape(-1, 28*28)).float().to(self.device)\n",
        "        y = torch.from_numpy(y_train.reshape(-1).astype(np.int64)).to(self.device)\n",
        "        self._fit_norm(X)\n",
        "        X = (X - self.mean_) / self.std_\n",
        "\n",
        "        opt = optim.AdamW(self.model.parameters(), lr=self.cfg[\"lr\"], weight_decay=self.cfg[\"wd\"])\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        # 估计每步耗时，用于自适应 epoch\n",
        "        bs = self.cfg[\"bs\"]\n",
        "        loader_est = self._make_loader(X[:8192], y[:8192], bs)\n",
        "        self.model.train()\n",
        "        dry_steps, t_dry0 = 0, time.time()\n",
        "        for xb, yb in loader_est:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss = crit(self.model(xb), yb)\n",
        "            loss.backward(); opt.step()\n",
        "            dry_steps += 1\n",
        "            if dry_steps >= 5: break\n",
        "        time_per_step = max(1e-6, (time.time()-t_dry0)/dry_steps)\n",
        "\n",
        "        steps_per_epoch = math.ceil(len(X)/bs)\n",
        "        train_budget = max(2.0, self.time_budget - (time.time()-t0) - 2.0)\n",
        "        est_epochs = int(train_budget / (time_per_step * steps_per_epoch))\n",
        "        epochs = int(np.clip(est_epochs, self.cfg[\"min_epochs\"], self.cfg[\"max_epochs\"]))\n",
        "\n",
        "        loader = self._make_loader(X, y, bs, shuffle=True)\n",
        "        total_steps = max(1, epochs * steps_per_epoch)\n",
        "        sched = optim.lr_scheduler.OneCycleLR(\n",
        "            opt, max_lr=self.cfg[\"lr\"], total_steps=total_steps,\n",
        "            pct_start=min(0.3, self.cfg[\"warmup_steps\"]/total_steps),\n",
        "            anneal_strategy=\"cos\", cycle_momentum=False\n",
        "        )\n",
        "\n",
        "        for ep in range(epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                loss = crit(self.model(xb), yb)\n",
        "                loss.backward(); opt.step(); sched.step()\n",
        "                if (time.time()-t0) > (self.time_budget - 2.0): break\n",
        "            if (time.time()-t0) > (self.time_budget - 2.0): break\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = torch.from_numpy(X_test.reshape(-1, 28*28)).float().to(self.device)\n",
        "        m, s = (self.mean_ if self.mean_ is not None else 0.0), (self.std_ if self.std_ is not None else 1.0)\n",
        "        X = (X - m) / s\n",
        "        self.model.eval()\n",
        "        bs = 4096; out = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            out.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(out, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CKE59EL5DXc_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "CKE59EL5DXc_",
        "outputId": "3a6cf41d-e1bc-4b7a-d6de-715a01f7b8c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating High-Accuracy Agent\n",
            "==================================================\n",
            "Task 1: Accuracy = 96.55%, Time = 43.56s\n",
            "Task 2: Accuracy = 97.61%, Time = 57.10s\n",
            "Task 3: Accuracy = 97.62%, Time = 55.72s\n",
            "Task 4: Accuracy = 97.56%, Time = 54.51s\n",
            "Task 5: Accuracy = 97.54%, Time = 54.95s\n",
            "Task 6: Accuracy = 97.85%, Time = 53.73s\n",
            "Task 7: Accuracy = 96.71%, Time = 36.55s\n",
            "Task 8: Accuracy = 97.53%, Time = 53.79s\n",
            "Task 9: Accuracy = 97.47%, Time = 54.51s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-624639799.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3772675301.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0msched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_budget\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_budget\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Reset environment\n",
        "env.reset()\n",
        "env.set_seed(42)\n",
        "\n",
        "# Create agent\n",
        "agent = HighAccuracyAgent(output_dim=10, seed=42)\n",
        "\n",
        "# Track performance\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating High-Accuracy Agent\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None: break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nHigh-Accuracy Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-PGw3BEYdK4r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PGw3BEYdK4r",
        "outputId": "60e5a9a6-0c4c-4045-e15c-aae5fef531fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== System Status ===\n",
            "CPU cores: 2\n",
            "Total RAM: 12.67 GB | Used: 1.80 GB | Free: 10.56 GB\n",
            "Current Python process memory: 1.15 GB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import psutil\n",
        "\n",
        "def show_system_status():\n",
        "    # CPU 核数\n",
        "    num_cores = os.cpu_count()\n",
        "\n",
        "    # 系统内存\n",
        "    mem = psutil.virtual_memory()\n",
        "    total_mem = mem.total / (1024 ** 3)   # GB\n",
        "    used_mem = mem.used / (1024 ** 3)     # GB\n",
        "    free_mem = mem.available / (1024 ** 3)# GB\n",
        "\n",
        "    # 当前 Python 进程占用内存\n",
        "    process = psutil.Process()\n",
        "    proc_mem = process.memory_info().rss / (1024 ** 3)  # GB\n",
        "\n",
        "    print(\"=== System Status ===\")\n",
        "    print(f\"CPU cores: {num_cores}\")\n",
        "    print(f\"Total RAM: {total_mem:.2f} GB | Used: {used_mem:.2f} GB | Free: {free_mem:.2f} GB\")\n",
        "    print(f\"Current Python process memory: {proc_mem:.2f} GB\")\n",
        "\n",
        "# 调用示例\n",
        "show_system_status()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7awt7uxpg",
      "metadata": {
        "id": "f7awt7uxpg"
      },
      "source": [
        "## 7. Performance Comparison\n",
        "\n",
        "Let's visualize and compare the performance of both agents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "juhqqv664hg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "juhqqv664hg",
        "outputId": "83cfd5b8-dd83-4d2d-d477-9441cdb3cdfa"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create comparison plots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy comparison\n",
        "tasks = np.arange(1, len(random_accuracies) + 1)\n",
        "ax1.plot(tasks, random_accuracies, 'o-', label='Random Agent', alpha=0.7, linewidth=2)\n",
        "ax1.plot(tasks, linear_accuracies, 's-', label='Linear Agent', alpha=0.7, linewidth=2)\n",
        "ax1.axhline(y=0.1, color='gray', linestyle='--', alpha=0.5, label='Random chance (10%)')\n",
        "ax1.set_xlabel('Task Number')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_title('Accuracy per Task')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_ylim([0, 1])\n",
        "\n",
        "# Time comparison\n",
        "ax2.bar(tasks - 0.2, random_times, 0.4, label='Random Agent', alpha=0.7)\n",
        "ax2.bar(tasks + 0.2, linear_times, 0.4, label='Linear Agent', alpha=0.7)\n",
        "ax2.axhline(y=60, color='red', linestyle='--', alpha=0.5, label='1 minute threshold')\n",
        "ax2.set_xlabel('Task Number')\n",
        "ax2.set_ylabel('Time (seconds)')\n",
        "ax2.set_title('Training + Prediction Time per Task')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kahhtoqy13",
      "metadata": {
        "id": "kahhtoqy13"
      },
      "source": [
        "## 8. Statistical Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tt1p6s1wyho",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt1p6s1wyho",
        "outputId": "59282e65-bcb5-4a91-ae78-f866deee414f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PERFORMANCE COMPARISON SUMMARY\n",
            "======================================================================\n",
            "Metric                    Random Agent         Linear Agent        \n",
            "----------------------------------------------------------------------\n",
            "Mean Accuracy             10.08%               90.82%              \n",
            "Std Accuracy              0.20%                0.11%               \n",
            "Min Accuracy              9.70%                90.63%              \n",
            "Max Accuracy              10.41%               91.02%              \n",
            "----------------------------------------------------------------------\n",
            "Mean Time per Task        0.0004              s 2.83                s\n",
            "Total Time                0.0038              s 28.34               s\n",
            "======================================================================\n",
            "\n",
            "📊 Linear agent shows 800.7% improvement over random baseline\n",
            "\n",
            "⏱️  Time Constraint Check (< 1 minute per task):\n",
            "   Random Agent: ✅ PASS (max: 0.00s)\n",
            "   Linear Agent: ✅ PASS (max: 3.43s)\n"
          ]
        }
      ],
      "source": [
        "# Create a summary comparison table\n",
        "print(\"PERFORMANCE COMPARISON SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Metric':<25} {'Random Agent':<20} {'Linear Agent':<20}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Accuracy metrics\n",
        "print(f\"{'Mean Accuracy':<25} {np.mean(random_accuracies):<20.2%} {np.mean(linear_accuracies):<20.2%}\")\n",
        "print(f\"{'Std Accuracy':<25} {np.std(random_accuracies):<20.2%} {np.std(linear_accuracies):<20.2%}\")\n",
        "print(f\"{'Min Accuracy':<25} {np.min(random_accuracies):<20.2%} {np.min(linear_accuracies):<20.2%}\")\n",
        "print(f\"{'Max Accuracy':<25} {np.max(random_accuracies):<20.2%} {np.max(linear_accuracies):<20.2%}\")\n",
        "\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Time metrics\n",
        "print(f\"{'Mean Time per Task':<25} {np.mean(random_times):<20.4f}s {np.mean(linear_times):<20.2f}s\")\n",
        "print(f\"{'Total Time':<25} {np.sum(random_times):<20.4f}s {np.sum(linear_times):<20.2f}s\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Performance analysis\n",
        "improvement = (np.mean(linear_accuracies) - np.mean(random_accuracies)) / np.mean(random_accuracies)\n",
        "print(f\"\\n📊 Linear agent shows {improvement:.1%} improvement over random baseline\")\n",
        "\n",
        "# Check if objective is met\n",
        "max_time_random = np.max(random_times)\n",
        "max_time_linear = np.max(linear_times)\n",
        "\n",
        "print(f\"\\n⏱️  Time Constraint Check (< 1 minute per task):\")\n",
        "print(f\"   Random Agent: {'✅ PASS' if max_time_random < 60 else '❌ FAIL'} (max: {max_time_random:.2f}s)\")\n",
        "print(f\"   Linear Agent: {'✅ PASS' if max_time_linear < 60 else '❌ FAIL'} (max: {max_time_linear:.2f}s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o7fuhntvo1b",
      "metadata": {
        "id": "o7fuhntvo1b"
      },
      "source": [
        "## 9. Key Insights and Conclusions\n",
        "\n",
        "### What we learned:\n",
        "\n",
        "1. **Random Baseline**: The random agent achieves ~10% accuracy, which is expected for random guessing on 10 classes.\n",
        "\n",
        "2. **Linear Agent Performance**: Despite the pixel and label permutations, the linear agent can learn patterns and achieve significantly better accuracy than random guessing.\n",
        "\n",
        "3. **Time Efficiency**: Both agents meet the < 1 minute requirement, with the random agent being faster (no learning) and the linear agent still being very efficient.\n",
        "\n",
        "4. **Meta-Learning Challenge**: Each task has different permutations, so the agent must learn from scratch each time. This tests the agent's ability to quickly adapt.\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Try implementing more sophisticated agents (e.g., neural networks with better architectures)\n",
        "- Experiment with different hyperparameters (learning rate, epochs, batch size)\n",
        "- Implement meta-learning algorithms that can leverage experience from previous tasks\n",
        "- Add early stopping or adaptive learning rates for better performance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3uiuylfq4zn",
      "metadata": {
        "id": "3uiuylfq4zn"
      },
      "source": [
        "## 10. Experiment: Tuning the Linear Agent\n",
        "\n",
        "Let's try different hyperparameters to see if we can improve performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o9f7p95dq8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "o9f7p95dq8e",
        "outputId": "925e3996-a0c2-4702-923d-cdce1fc9470c"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2518596083.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Run through tasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/permuted_mnist/permuted_mnist/env/permuted_mnist.py\u001b[0m in \u001b[0;36mget_next_task\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Apply pixel permutation and task-specific noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mtrain_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_permute_pixels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mtest_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_permute_pixels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/permuted_mnist/permuted_mnist/env/permuted_mnist.py\u001b[0m in \u001b[0;36m_permute_pixels\u001b[0;34m(self, images, task_id)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# Apply per-image random brightness/contrast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mpermuted_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermuted_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscales\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshifts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Test different learning rates\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "results = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    # Reset environment\n",
        "    env.reset()\n",
        "    env.set_seed(42)\n",
        "\n",
        "    # Create agent with different learning rate\n",
        "    agent = LinearAgent(input_dim=784, output_dim=10, learning_rate=lr)\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    # Run through tasks\n",
        "    while True:\n",
        "        task = env.get_next_task()\n",
        "        if task is None:\n",
        "            break\n",
        "\n",
        "        agent.reset()\n",
        "        agent.train(task['X_train'], task['y_train'], epochs=5, batch_size=32)\n",
        "        predictions = agent.predict(task['X_test'])\n",
        "        accuracy = env.evaluate(predictions, task['y_test'])\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "    results[lr] = np.mean(accuracies)\n",
        "    print(f\"Learning rate {lr}: Mean accuracy = {results[lr]:.2%}\")\n",
        "\n",
        "# Find best learning rate\n",
        "best_lr = max(results, key=results.get)\n",
        "print(f\"\\n🏆 Best learning rate: {best_lr} with {results[best_lr]:.2%} accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DBSzVpLcK842",
      "metadata": {
        "id": "DBSzVpLcK842"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}