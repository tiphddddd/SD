{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8903fa6d-3fc9-4d4c-8520-cbb67975b272",
      "metadata": {
        "id": "8903fa6d-3fc9-4d4c-8520-cbb67975b272"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "MY4PsPTGm0wb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY4PsPTGm0wb",
        "outputId": "968a615d-00e5-41fb-b0c9-7ec9970d4c96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'permuted_mnist'...\n",
            "remote: Enumerating objects: 194, done.\u001b[K\n",
            "remote: Counting objects: 100% (194/194), done.\u001b[K\n",
            "remote: Compressing objects: 100% (134/134), done.\u001b[K\n",
            "remote: Total 194 (delta 77), reused 163 (delta 50), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (194/194), 12.62 MiB | 4.81 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n",
            "Updating files: 100% (21/21), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ml-arena/permuted_mnist.git\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "YN2llLljnjq_",
      "metadata": {
        "id": "YN2llLljnjq_"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/permuted_mnist')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "p0edaud30tf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0edaud30tf",
        "outputId": "7f02e640-a848-43e7-fbda-0163689a2cf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Imports successful\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List\n",
        "\n",
        "# Import the environment and agents\n",
        "from permuted_mnist.env.permuted_mnist import PermutedMNISTEnv\n",
        "from permuted_mnist.agent.random.agent import Agent as RandomAgent\n",
        "from permuted_mnist.agent.linear.agent import Agent as LinearAgent\n",
        "from permuted_mnist.agent.torch_mlp.agent import Agent as TorchMLP\n",
        "\n",
        "print(\"✓ Imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vco5lhcip5b",
      "metadata": {
        "id": "vco5lhcip5b"
      },
      "source": [
        "## 2. Create the Environment\n",
        "\n",
        "Let's create an environment with 10 different permuted tasks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c7u41d9cvln",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7u41d9cvln",
        "outputId": "fe68c9d2-d76a-4962-90c2-c0c6aed64e20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment created with 10 permuted tasks\n",
            "Training set size: 60000 samples\n",
            "Test set size: 10000 samples\n"
          ]
        }
      ],
      "source": [
        "# Create environment with 10 episodes (tasks)\n",
        "env = PermutedMNISTEnv(number_episodes=10)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "env.set_seed(42)\n",
        "\n",
        "print(f\"Environment created with {env.number_episodes} permuted tasks\")\n",
        "print(f\"Training set size: {env.train_size} samples\")\n",
        "print(f\"Test set size: {env.test_size} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33a72b29-bd35-4254-b0fe-774b73dc4609",
      "metadata": {
        "id": "33a72b29-bd35-4254-b0fe-774b73dc4609",
        "scrolled": true
      },
      "source": [
        "## 3. Understanding the Task Structure\n",
        "\n",
        "Let's examine what a single task looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "z74jqitydh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z74jqitydh",
        "outputId": "01dab709-104d-4ca8-fa70-0deba322b5f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task structure:\n",
            "- X_train shape: (60000, 28, 28)\n",
            "- y_train shape: (60000, 1)\n",
            "- X_test shape: (10000, 28, 28)\n",
            "- y_test shape: (10000,)\n",
            "\n",
            "Label distribution in training set:\n",
            "  Label 0: 6131 samples\n",
            "  Label 1: 6742 samples\n",
            "  Label 2: 5421 samples\n",
            "  Label 3: 5851 samples\n",
            "  Label 4: 6265 samples\n",
            "  Label 5: 5958 samples\n",
            "  Label 6: 5949 samples\n",
            "  Label 7: 5842 samples\n",
            "  Label 8: 5923 samples\n",
            "  Label 9: 5918 samples\n"
          ]
        }
      ],
      "source": [
        "# Get the first task\n",
        "task = env.get_next_task()\n",
        "\n",
        "print(\"Task structure:\")\n",
        "print(f\"- X_train shape: {task['X_train'].shape}\")\n",
        "print(f\"- y_train shape: {task['y_train'].shape}\")\n",
        "print(f\"- X_test shape: {task['X_test'].shape}\")\n",
        "print(f\"- y_test shape: {task['y_test'].shape}\")\n",
        "print(f\"\\nLabel distribution in training set:\")\n",
        "unique, counts = np.unique(task['y_train'], return_counts=True)\n",
        "for label, count in zip(unique, counts):\n",
        "    print(f\"  Label {label}: {count} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ea4ardqha",
      "metadata": {
        "id": "5ea4ardqha"
      },
      "source": [
        "## 4. Visualize Permuted Images\n",
        "\n",
        "Let's see how the permutation affects the images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "joow2rsdve8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "joow2rsdve8",
        "outputId": "25af5f5e-4ee4-467c-e7d6-bb9f1337e06a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: The images look scrambled due to pixel permutation!\n",
            "The labels are also permuted (not the original MNIST labels).\n"
          ]
        }
      ],
      "source": [
        "# Display some examples from the permuted task\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "fig.suptitle('Permuted MNIST Examples (Task 1)', fontsize=16)\n",
        "\n",
        "for i in range(10):\n",
        "    ax = axes[i // 5, i % 5]\n",
        "    ax.imshow(task['X_train'][i], cmap='gray')\n",
        "    ax.set_title(f'Label: {task[\"y_train\"][i][0]}')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Note: The images look scrambled due to pixel permutation!\")\n",
        "print(\"The labels are also permuted (not the original MNIST labels).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b6luam88gn",
      "metadata": {
        "id": "6b6luam88gn"
      },
      "source": [
        "## 5. Baseline: Random Agent\n",
        "\n",
        "First, let's establish a baseline with an agent that makes random predictions:\n",
        "这个智能体（RandomAgent）什么也不会学，只是：\n",
        "\n",
        "收到训练数据后，什么也不做；\n",
        "\n",
        "在预测时，随机输出 0–9 之间的数字；\n",
        "\n",
        "用这个结果去计算准确率。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "lucs492dcp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lucs492dcp",
        "outputId": "581ec962-2426-4dc3-e5e3-8c0c6edacb43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Random Agent (Baseline)\n",
            "==================================================\n",
            "Task 1: Accuracy = 9.96%, Time = 0.0003s\n",
            "Task 2: Accuracy = 9.70%, Time = 0.0003s\n",
            "Task 3: Accuracy = 10.41%, Time = 0.0003s\n",
            "Task 4: Accuracy = 10.02%, Time = 0.0003s\n",
            "Task 5: Accuracy = 10.23%, Time = 0.0004s\n",
            "Task 6: Accuracy = 9.94%, Time = 0.0003s\n",
            "Task 7: Accuracy = 10.29%, Time = 0.0003s\n",
            "Task 8: Accuracy = 10.27%, Time = 0.0003s\n",
            "Task 9: Accuracy = 9.93%, Time = 0.0003s\n",
            "Task 10: Accuracy = 10.09%, Time = 0.0003s\n",
            "\n",
            "Random Agent Summary:\n",
            "  Mean accuracy: 10.08% ± 0.20%\n",
            "  Total time: 0.00s\n"
          ]
        }
      ],
      "source": [
        "# Reset environment for fresh start\n",
        "env.reset()\n",
        "env.set_seed(42)\n",
        "\n",
        "# Create random agent\n",
        "random_agent = RandomAgent(output_dim=10, seed=42)\n",
        "\n",
        "# Track performance\n",
        "random_accuracies = []\n",
        "random_times = []\n",
        "\n",
        "print(\"Evaluating Random Agent (Baseline)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Evaluate on all tasks\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # \"Train\" (random agent doesn't actually learn)\n",
        "    random_agent.train(task['X_train'], task['y_train'])\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = random_agent.predict(task['X_test'])\n",
        "\n",
        "    # Calculate time and accuracy\n",
        "    elapsed_time = time.time() - start_time\n",
        "    accuracy = env.evaluate(predictions, task['y_test'])\n",
        "\n",
        "    random_accuracies.append(accuracy)\n",
        "    random_times.append(elapsed_time)\n",
        "\n",
        "    print(f\"Task {task_num}: Accuracy = {accuracy:.2%}, Time = {elapsed_time:.4f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nRandom Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(random_accuracies):.2%} ± {np.std(random_accuracies):.2%}\")\n",
        "print(f\"  Total time: {np.sum(random_times):.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y1zoqupvez",
      "metadata": {
        "id": "y1zoqupvez"
      },
      "source": [
        "## 6. Linear Agent\n",
        "\n",
        "Now let's train a simple linear classifier that actually learns from the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "lf16ji120qa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf16ji120qa",
        "outputId": "a0635ff2-4216-4b42-af15-c05ff7f3f91e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Linear Agent\n",
            "==================================================\n",
            "Task 1: Accuracy = 90.87%, Time = 2.45s\n",
            "Task 2: Accuracy = 90.87%, Time = 2.42s\n",
            "Task 3: Accuracy = 90.72%, Time = 2.41s\n",
            "Task 4: Accuracy = 90.90%, Time = 2.38s\n",
            "Task 5: Accuracy = 90.70%, Time = 2.42s\n",
            "Task 6: Accuracy = 90.77%, Time = 2.43s\n",
            "Task 7: Accuracy = 90.79%, Time = 4.33s\n",
            "Task 8: Accuracy = 90.79%, Time = 3.45s\n",
            "Task 9: Accuracy = 90.83%, Time = 2.41s\n",
            "Task 10: Accuracy = 90.82%, Time = 2.38s\n",
            "\n",
            "Linear Agent Summary:\n",
            "  Mean accuracy: 90.81% ± 0.06%\n",
            "  Total time: 27.09s\n"
          ]
        }
      ],
      "source": [
        "# Reset environment\n",
        "env.reset()\n",
        "env.set_seed(42)\n",
        "\n",
        "# Create linear agent\n",
        "linear_agent = LinearAgent(input_dim=784, output_dim=10, learning_rate=0.01)\n",
        "\n",
        "# Track performance\n",
        "linear_accuracies = []\n",
        "linear_times = []\n",
        "\n",
        "print(\"Evaluating Linear Agent\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Evaluate on all tasks\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    # Reset agent for new task\n",
        "    linear_agent.reset()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the linear model\n",
        "    linear_agent.train(task['X_train'], task['y_train'], epochs=5, batch_size=32)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = linear_agent.predict(task['X_test'])\n",
        "\n",
        "    # Calculate time and accuracy\n",
        "    elapsed_time = time.time() - start_time\n",
        "    accuracy = env.evaluate(predictions, task['y_test'])\n",
        "\n",
        "    linear_accuracies.append(accuracy)\n",
        "    linear_times.append(elapsed_time)\n",
        "\n",
        "    print(f\"Task {task_num}: Accuracy = {accuracy:.2%}, Time = {elapsed_time:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nLinear Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(linear_accuracies):.2%} ± {np.std(linear_accuracies):.2%}\")\n",
        "print(f\"  Total time: {np.sum(linear_times):.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uPUx972-JR9b",
      "metadata": {
        "id": "uPUx972-JR9b"
      },
      "source": [
        "After the exploration of data,we can now evaluatino our model.\n",
        "This time ,we want to build a deep model that can truly learn nonlinear structures.\n",
        "Compared to the previous linear classifier, this model:\n",
        "\n",
        "Has at least one hidden layer (for example: 784 → 256 → 10);\n",
        "\n",
        "Uses activation functions (ReLU, Tanh, etc.) to introduce nonlinearity;\n",
        "\n",
        "Is retrained and makes predictions on each permuted task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "AmYncvF4Rn2O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmYncvF4Rn2O",
        "outputId": "38d7d0d5-454c-4c1c-95e4-02827d094aeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TorchMLP Agent\n",
            "==================================================\n",
            "epoch 0: 0.9623%\n",
            "epoch 1: 0.9715%\n",
            "epoch 2: 0.9735%\n",
            "epoch 3: 0.9763%\n",
            "epoch 4: 0.9776%\n",
            "epoch 5: 0.9781%\n",
            "epoch 6: 0.9812%\n",
            "epoch 7: 0.9777%\n",
            "epoch 8: 0.9816%\n",
            "epoch 9: 0.9832%\n",
            "Task 1: Accuracy = 98.02%, Time = 191.50s\n",
            "epoch 0: 0.9636%\n",
            "epoch 1: 0.9732%\n",
            "epoch 2: 0.9742%\n",
            "epoch 3: 0.9779%\n",
            "epoch 4: 0.9813%\n",
            "epoch 5: 0.9793%\n",
            "epoch 6: 0.9802%\n",
            "epoch 7: 0.9817%\n",
            "epoch 8: 0.9801%\n",
            "epoch 9: 0.9817%\n",
            "Task 2: Accuracy = 98.18%, Time = 161.67s\n",
            "epoch 0: 0.9665%\n",
            "epoch 1: 0.9719%\n",
            "epoch 2: 0.9732%\n",
            "epoch 3: 0.9742%\n",
            "epoch 4: 0.9782%\n",
            "epoch 5: 0.9769%\n",
            "epoch 6: 0.9764%\n",
            "epoch 7: 0.9810%\n",
            "epoch 8: 0.9779%\n",
            "epoch 9: 0.9811%\n",
            "Task 3: Accuracy = 98.05%, Time = 161.01s\n",
            "epoch 0: 0.9617%\n",
            "epoch 1: 0.9732%\n",
            "epoch 2: 0.9751%\n",
            "epoch 3: 0.9774%\n",
            "epoch 4: 0.9782%\n",
            "epoch 5: 0.9773%\n",
            "epoch 6: 0.9821%\n",
            "epoch 7: 0.9829%\n",
            "epoch 8: 0.9834%\n",
            "epoch 9: 0.9806%\n",
            "Task 4: Accuracy = 98.07%, Time = 158.19s\n",
            "epoch 0: 0.9576%\n",
            "epoch 1: 0.9756%\n",
            "epoch 2: 0.9762%\n",
            "epoch 3: 0.9770%\n",
            "epoch 4: 0.9807%\n",
            "epoch 5: 0.9776%\n",
            "epoch 6: 0.9808%\n",
            "epoch 7: 0.9821%\n",
            "epoch 8: 0.9795%\n",
            "epoch 9: 0.9826%\n",
            "Task 5: Accuracy = 98.19%, Time = 163.62s\n",
            "epoch 0: 0.9618%\n",
            "epoch 1: 0.9712%\n",
            "epoch 2: 0.9748%\n",
            "epoch 3: 0.9753%\n",
            "epoch 4: 0.9750%\n",
            "epoch 5: 0.9770%\n",
            "epoch 6: 0.9792%\n",
            "epoch 7: 0.9798%\n",
            "epoch 8: 0.9800%\n",
            "epoch 9: 0.9808%\n",
            "Task 6: Accuracy = 98.21%, Time = 160.54s\n",
            "epoch 0: 0.9632%\n",
            "epoch 1: 0.9720%\n",
            "epoch 2: 0.9762%\n",
            "epoch 3: 0.9785%\n",
            "epoch 4: 0.9742%\n",
            "epoch 5: 0.9800%\n",
            "epoch 6: 0.9798%\n",
            "epoch 7: 0.9805%\n",
            "epoch 8: 0.9798%\n",
            "epoch 9: 0.9801%\n",
            "Task 7: Accuracy = 98.25%, Time = 170.12s\n",
            "epoch 0: 0.9632%\n",
            "epoch 1: 0.9710%\n",
            "epoch 2: 0.9738%\n",
            "epoch 3: 0.9762%\n",
            "epoch 4: 0.9777%\n",
            "epoch 5: 0.9808%\n",
            "epoch 6: 0.9770%\n",
            "epoch 7: 0.9803%\n",
            "epoch 8: 0.9781%\n",
            "epoch 9: 0.9823%\n",
            "Task 8: Accuracy = 98.11%, Time = 190.55s\n",
            "epoch 0: 0.9688%\n",
            "epoch 1: 0.9737%\n",
            "epoch 2: 0.9727%\n",
            "epoch 3: 0.9777%\n",
            "epoch 4: 0.9792%\n",
            "epoch 5: 0.9805%\n",
            "epoch 6: 0.9763%\n",
            "epoch 7: 0.9812%\n",
            "epoch 8: 0.9803%\n",
            "epoch 9: 0.9829%\n",
            "Task 9: Accuracy = 98.19%, Time = 195.73s\n",
            "epoch 0: 0.9595%\n",
            "epoch 1: 0.9734%\n",
            "epoch 2: 0.9733%\n",
            "epoch 3: 0.9765%\n",
            "epoch 4: 0.9817%\n",
            "epoch 5: 0.9795%\n",
            "epoch 6: 0.9791%\n",
            "epoch 7: 0.9825%\n",
            "epoch 8: 0.9822%\n",
            "epoch 9: 0.9838%\n",
            "Task 10: Accuracy = 98.11%, Time = 187.48s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 98.14% ± 0.07%\n",
            "  Total time: 1740.41s\n"
          ]
        }
      ],
      "source": [
        "# Reset environment\n",
        "env.reset()\n",
        "env.set_seed(42)\n",
        "\n",
        "# Create linear agent\n",
        "torchmlp = TorchMLP(output_dim=10,seed=42)\n",
        "\n",
        "# Track performance\n",
        "torchmlp_accuracies = []\n",
        "torchmlp_times = []\n",
        "\n",
        "print(\"Evaluating TorchMLP Agent\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Evaluate on all tasks\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    # Reset agent for new task\n",
        "    torchmlp.reset()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the linear model\n",
        "    torchmlp.train(task['X_train'], task['y_train'])\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = torchmlp.predict(task['X_test'])\n",
        "\n",
        "    # Calculate time and accuracy\n",
        "    elapsed_time = time.time() - start_time\n",
        "    accuracy = env.evaluate(predictions, task['y_test'])\n",
        "\n",
        "    torchmlp_accuracies.append(accuracy)\n",
        "    torchmlp_times.append(elapsed_time)\n",
        "\n",
        "    print(f\"Task {task_num}: Accuracy = {accuracy:.2%}, Time = {elapsed_time:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(torchmlp_accuracies):.2%} ± {np.std(torchmlp_accuracies):.2%}\")\n",
        "print(f\"  Total time: {np.sum(torchmlp_times):.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We try to explore how the change of hyperameter influence the accuracy and time."
      ],
      "metadata": {
        "id": "pz6Xakxn2B8d"
      },
      "id": "pz6Xakxn2B8d"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "lzWLcFX5YPTD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzWLcFX5YPTD",
        "outputId": "cad046b9-b3ca-434a-bef1-50f7192ff6f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TorchMLP Agent\n",
            "==================================================\n",
            "Task 1: Accuracy = 94.36%, Time = 5.79s\n",
            "Task 2: Accuracy = 94.43%, Time = 6.01s\n",
            "Task 3: Accuracy = 94.16%, Time = 5.86s\n",
            "Task 4: Accuracy = 93.52%, Time = 5.78s\n",
            "Task 5: Accuracy = 94.90%, Time = 5.71s\n",
            "Task 6: Accuracy = 95.97%, Time = 5.79s\n",
            "Task 7: Accuracy = 94.97%, Time = 5.73s\n",
            "Task 8: Accuracy = 94.25%, Time = 5.96s\n",
            "Task 9: Accuracy = 92.52%, Time = 5.86s\n",
            "Task 10: Accuracy = 93.89%, Time = 6.45s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 94.30% ± 0.87%\n",
            "  Total time: 58.93s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP (speed profile with early stopping) =====\n",
        "import time, math, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _flatten_np(X):\n",
        "    return X.reshape(X.shape[0], -1).astype(np.float32, copy=False)\n",
        "\n",
        "def _labels_1d(y):\n",
        "    y = np.asarray(y)\n",
        "    return y.reshape(-1).astype(np.int64, copy=False)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _scalar_norm_fit(X):\n",
        "    m = float(X.mean()); s = float(X.std())\n",
        "    if s < 1e-6: s = 1.0\n",
        "    return m, s\n",
        "\n",
        "class _SpeedMLP(nn.Module):\n",
        "    def __init__(self, in_dim=784, h1=512, h2=256, out_dim=10, p=0.10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h1), nn.ReLU(), nn.Dropout(p),\n",
        "            nn.Linear(h1, h2),     nn.ReLU(), nn.Dropout(p),\n",
        "            nn.Linear(h2, out_dim)\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    def __init__(self, output_dim=10, seed=42,\n",
        "                 time_budget=5.5, subsample_n=20000,\n",
        "                 val_ratio=0.1, batch_size=2048,\n",
        "                 lr=3e-3, weight_decay=1e-4,\n",
        "                 patience=2, min_delta=0.001, min_train_seconds=2.5):\n",
        "        self.output_dim = output_dim\n",
        "        self.seed = seed\n",
        "        self.time_budget = float(time_budget)\n",
        "        self.subsample_n = int(subsample_n)\n",
        "        self.val_ratio = float(val_ratio)\n",
        "        self.batch_size = int(batch_size)\n",
        "        self.lr = float(lr)\n",
        "        self.weight_decay = float(weight_decay)\n",
        "        self.patience = int(patience)\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.min_train_seconds = float(min_train_seconds)\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        self.model = None\n",
        "        self.mean_, self.std_ = 0.0, 1.0\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        np.random.seed(self.seed); torch.manual_seed(self.seed)\n",
        "        self.model = _SpeedMLP(out_dim=self.output_dim).to(self.device)\n",
        "        self.mean_, self.std_ = 0.0, 1.0\n",
        "\n",
        "    def _make_loader(self, X, y, bs, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=bs, shuffle=shuffle,\n",
        "                                           drop_last=False, num_workers=0, pin_memory=False)\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        t0 = time.time()\n",
        "        Xf = _flatten_np(X_train)\n",
        "        y = _labels_1d(y_train)\n",
        "        N = Xf.shape[0]\n",
        "        ss = min(self.subsample_n, N)\n",
        "        idx = np.random.default_rng(self.seed).choice(N, ss, replace=False)\n",
        "        Xs = torch.from_numpy(Xf[idx]).float().to(self.device)\n",
        "        ys = torch.from_numpy(y[idx]).to(self.device)\n",
        "\n",
        "        n_val = max(1000, int(len(Xs) * 0.1))\n",
        "        n_tr  = len(Xs) - n_val\n",
        "        X_tr, X_val = Xs[:n_tr], Xs[n_tr:]\n",
        "        y_tr, y_val = ys[:n_tr], ys[n_tr:]\n",
        "\n",
        "        self.mean_, self.std_ = _scalar_norm_fit(X_tr)\n",
        "        X_tr = (X_tr - self.mean_) / self.std_\n",
        "        X_val = (X_val - self.mean_) / self.std_\n",
        "\n",
        "        opt = optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "        loader = self._make_loader(X_tr, y_tr, self.batch_size, shuffle=True)\n",
        "\n",
        "        best_acc, no_improve = 0.0, 0\n",
        "        self.model.train()\n",
        "        while True:\n",
        "            for xb, yb in loader:\n",
        "                if time.time() - t0 > self.time_budget: break\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                loss = crit(self.model(xb), yb)\n",
        "                loss.backward(); opt.step()\n",
        "            if time.time() - t0 > self.time_budget: break\n",
        "\n",
        "            # 验证\n",
        "            with torch.no_grad():\n",
        "                self.model.eval()\n",
        "                bs_eval = 8192\n",
        "                correct, total = 0, 0\n",
        "                for i in range(0, X_val.shape[0], bs_eval):\n",
        "                    logits = self.model(X_val[i:i+bs_eval])\n",
        "                    pred = logits.argmax(dim=1)\n",
        "                    correct += (pred == y_val[i:i+bs_eval]).sum().item()\n",
        "                    total   += min(bs_eval, X_val.shape[0]-i)\n",
        "                val_acc = correct / max(1, total)\n",
        "                self.model.train()\n",
        "\n",
        "            if val_acc > best_acc + self.min_delta:\n",
        "                best_acc = val_acc\n",
        "                no_improve = 0\n",
        "            else:\n",
        "                no_improve += 1\n",
        "\n",
        "            if no_improve >= self.patience and (time.time()-t0) >= self.min_train_seconds:\n",
        "                break\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test):\n",
        "        X = torch.from_numpy(_flatten_np(X_test)).float().to(self.device)\n",
        "        X = (X - self.mean_) / self.std_\n",
        "        self.model.eval()\n",
        "        bs = 8192\n",
        "        out = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            out.append(torch.argmax(self.model(X[i:i+bs]), dim=1).cpu().numpy())\n",
        "        return np.concatenate(out, axis=0)\n",
        "# Reset environment\n",
        "env.reset()\n",
        "env.set_seed(42)\n",
        "\n",
        "# Create agent\n",
        "torchmlp = TorchMLP(output_dim=10, seed=42)\n",
        "\n",
        "# Track performance\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating TorchMLP Agent\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    torchmlp.reset()\n",
        "    start = time.time()\n",
        "    torchmlp.train(task['X_train'], task['y_train'])\n",
        "    preds = torchmlp.predict(task['X_test'])\n",
        "    elapsed = time.time() - start\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J9tMuyVPfi-6",
      "metadata": {
        "id": "J9tMuyVPfi-6"
      },
      "source": [
        "There we use Optimizer Adam to improve our accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "QS_5kU4_Yeou",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS_5kU4_Yeou",
        "outputId": "21b839ac-532c-4cbc-b4ec-f601439cec6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TorchMLP Agent (BN, 2x100)\n",
            "==================================================\n",
            "Task 1: Accuracy = 97.26%, Time = 6.02s\n",
            "Task 2: Accuracy = 97.25%, Time = 5.90s\n",
            "Task 3: Accuracy = 97.25%, Time = 6.01s\n",
            "Task 4: Accuracy = 97.19%, Time = 6.58s\n",
            "Task 5: Accuracy = 96.93%, Time = 6.61s\n",
            "Task 6: Accuracy = 97.50%, Time = 8.14s\n",
            "Task 7: Accuracy = 97.10%, Time = 9.42s\n",
            "Task 8: Accuracy = 97.35%, Time = 8.34s\n",
            "Task 9: Accuracy = 97.31%, Time = 7.58s\n",
            "Task 10: Accuracy = 97.15%, Time = 9.53s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 97.23% ± 0.15%\n",
            "  Total time: 74.13s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP-BN (2x100, BN, ReLU; Adam lr=1e-3; bs=128; epochs=3; val=0.2) =====\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,28,28)/(N,784) np.uint8/float -> torch.float32 in [0,1]\"\"\"\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,) or (N,1) -> (N,) int64\"\"\"\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"784 -> 100 -> 100 -> 10 with BatchNorm + ReLU\"\"\"\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h, out_dim),\n",
        "        )\n",
        "        # 轻量初始化\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 3:           # (N,28,28)\n",
        "            x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "class TorchMLP:\n",
        "\n",
        "    def __init__(self, output_dim: int = 10, seed: int = 42,\n",
        "                 epochs: int = 3, batch_size: int = 128,\n",
        "                 lr: float = 1e-3, val_ratio: float = 0.2, weight_decay: float = 0.0):\n",
        "        self.output_dim   = output_dim\n",
        "        self.epochs       = int(epochs)\n",
        "        self.batch_size   = int(batch_size)\n",
        "        self.lr           = float(lr)\n",
        "        self.val_ratio    = float(val_ratio)\n",
        "        self.weight_decay = float(weight_decay)\n",
        "        self.device       = torch.device(\"cpu\")\n",
        "\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "\n",
        "        self.model = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_BN(in_dim=784, h=100, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X: torch.Tensor, y: torch.Tensor, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        X = _as_float_01(X_train)\n",
        "        y = _labels_1d(y_train)\n",
        "\n",
        "\n",
        "        n_total = X.shape[0]\n",
        "        n_val   = int(self.val_ratio * n_total)\n",
        "        idx     = np.arange(n_total)\n",
        "        np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]\n",
        "        tr_idx  = idx[n_val:]\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for ep in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        self.model.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "# ===== Evaluate TorchMLP-BN (fixed) =====\n",
        "import time, numpy as np\n",
        "\n",
        "env.reset()\n",
        "env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating TorchMLP Agent (BN, 2x100)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GsRXr2K_Qy7x",
      "metadata": {
        "id": "GsRXr2K_Qy7x"
      },
      "source": [
        "We add L2 Normalization in the previous model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "-HPVI_B8M9is",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HPVI_B8M9is",
        "outputId": "a640d236-7846-4c74-b119-6c313bf33ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TorchMLP Agent (BN, 2x100) with Per-Sample L2 Normalization\n",
            "==================================================\n",
            "Task 1: Accuracy = 97.41%, Time = 10.29s\n",
            "Task 2: Accuracy = 97.34%, Time = 8.98s\n",
            "Task 3: Accuracy = 97.35%, Time = 10.70s\n",
            "Task 4: Accuracy = 97.46%, Time = 6.49s\n",
            "Task 5: Accuracy = 97.15%, Time = 7.00s\n",
            "Task 6: Accuracy = 97.37%, Time = 7.06s\n",
            "Task 7: Accuracy = 97.15%, Time = 6.86s\n",
            "Task 8: Accuracy = 97.27%, Time = 6.16s\n",
            "Task 9: Accuracy = 97.33%, Time = 6.19s\n",
            "Task 10: Accuracy = 97.33%, Time = 6.59s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 97.32% ± 0.10%\n",
            "  Total time: 76.32s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP-BN (2x100, BN, ReLU; Adam lr=1e-3; bs=128; epochs=3; val=0.2) + FE: per-sample L2 normalization =====\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 限制CPU线程（贴合评测约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,28,28)/(N,784) np.uint8/float -> torch.float32 in [0,1]\"\"\"\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,) or (N,1) -> (N,) int64\"\"\"\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# ------------ FE: per-sample L2 normalization ------------\n",
        "def _l2_per_sample(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    orig_shape = x.shape\n",
        "    if x.dim() == 3:  # (N,28,28) -> (N,784)\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig_shape) == 3:\n",
        "        x = x.view(orig_shape)\n",
        "    return x\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"784 -> 100 -> 100 -> 10 with BatchNorm + ReLU\"\"\"\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h, out_dim),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 3:           # (N,28,28)\n",
        "            x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    Torch MLP with BatchNorm + FE：\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dim: int = 10, seed: int = 42,\n",
        "                 epochs: int = 3, batch_size: int = 128,\n",
        "                 lr: float = 1e-3, val_ratio: float = 0.2, weight_decay: float = 0.0):\n",
        "        self.output_dim   = output_dim\n",
        "        self.epochs       = int(epochs)\n",
        "        self.batch_size   = int(batch_size)\n",
        "        self.lr           = float(lr)\n",
        "        self.val_ratio    = float(val_ratio)\n",
        "        self.weight_decay = float(weight_decay)\n",
        "        self.device       = torch.device(\"cpu\")\n",
        "\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_BN(in_dim=784, h=100, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X: torch.Tensor, y: torch.Tensor, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        X = _as_float_01(X_train)\n",
        "        y = _labels_1d(y_train)\n",
        "\n",
        "        n_total = X.shape[0]\n",
        "        n_val   = int(self.val_ratio * n_total)\n",
        "        idx     = np.arange(n_total)\n",
        "        np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]\n",
        "        tr_idx  = idx[n_val:]\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for ep in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        X = _l2_per_sample(X)\n",
        "        self.model.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "import time, numpy as np\n",
        "\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating TorchMLP Agent (BN, 2x100) with Per-Sample L2 Normalization\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yDLV-riaWvW-",
      "metadata": {
        "id": "yDLV-riaWvW-"
      },
      "source": [
        "give a hiddensize a tapered width like pyramid!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "UDx_2M-6WaXz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDx_2M-6WaXz",
        "outputId": "2c45c0ea-76eb-4745-c537-952055a54df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TorchMLP Agent (BN, 256→128 pyramid) with Per-Sample L2 Normalization\n",
            "==================================================\n",
            "Task 1: Accuracy = 97.59%, Time = 10.12s\n",
            "Task 2: Accuracy = 97.38%, Time = 10.46s\n",
            "Task 3: Accuracy = 97.57%, Time = 11.98s\n",
            "Task 4: Accuracy = 97.58%, Time = 12.91s\n",
            "Task 5: Accuracy = 97.43%, Time = 11.09s\n",
            "Task 6: Accuracy = 97.66%, Time = 12.07s\n",
            "Task 7: Accuracy = 97.49%, Time = 12.51s\n",
            "Task 8: Accuracy = 97.59%, Time = 10.79s\n",
            "Task 9: Accuracy = 97.59%, Time = 12.67s\n",
            "Task 10: Accuracy = 97.69%, Time = 13.21s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 97.56% ± 0.09%\n",
            "  Total time: 117.81s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP-BN (256→128 pyramid, BN, ReLU; Adam lr=1e-3; bs=128; epochs=3; val=0.2) + FE: per-sample L2 normalization =====\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 限制CPU线程（贴合评测约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,28,28)/(N,784) np.uint8/float -> torch.float32 in [0,1]\"\"\"\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,) or (N,1) -> (N,) int64\"\"\"\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# ------------ FE: per-sample L2 normalization ------------\n",
        "def _l2_per_sample(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    对每一张图做 L2 归一化；返回与输入相同形状。\n",
        "    只做输入侧FE，其他训练/结构/超参一律不变。\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "    if x.dim() == 3:  # (N,28,28) -> (N,784)\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig_shape) == 3:\n",
        "        x = x.view(orig_shape)\n",
        "    return x\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"\n",
        "    方案B：金字塔宽度 784 -> 256 -> 128 -> 10，层内 BN + ReLU；\n",
        "    为了与外部调用保持完全兼容，保留参数签名(in_dim, h, out_dim)但忽略 h。\n",
        "    隐藏层用 bias=False（配合BN），输出层 bias=True；Xavier 初始化，bias=0。\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10):\n",
        "        super().__init__()\n",
        "        h1, h2 = 256, 128\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h1, bias=False),\n",
        "            nn.BatchNorm1d(h1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(h1, h2, bias=False),\n",
        "            nn.BatchNorm1d(h2),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(h2, out_dim, bias=True),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 3:           # (N,28,28)\n",
        "            x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    Torch MLP with BatchNorm（参数完全不变） + FE：\n",
        "      * 每张图做 L2 归一化；\n",
        "      * 训练和预测都使用同样的 FE；\n",
        "      * 其它超参/结构/循环不变。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dim: int = 10, seed: int = 42,\n",
        "                 epochs: int = 3, batch_size: int = 128,\n",
        "                 lr: float = 1e-3, val_ratio: float = 0.2, weight_decay: float = 0.0):\n",
        "        self.output_dim   = output_dim\n",
        "        self.epochs       = int(epochs)\n",
        "        self.batch_size   = int(batch_size)\n",
        "        self.lr           = float(lr)\n",
        "        self.val_ratio    = float(val_ratio)\n",
        "        self.weight_decay = float(weight_decay)\n",
        "        self.device       = torch.device(\"cpu\")\n",
        "\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # 保持外部调用格式不变：仍然传 h=100；内部模型忽略 h，固定为 256→128\n",
        "        self.model = _MLP_BN(in_dim=784, h=100, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X: torch.Tensor, y: torch.Tensor, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    # === 训练/预测逻辑保持不变，仅在输入端加入 per-sample L2 FE ===\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        X = _as_float_01(X_train)\n",
        "        y = _labels_1d(y_train)\n",
        "\n",
        "        n_total = X.shape[0]\n",
        "        n_val   = int(self.val_ratio * n_total)\n",
        "        idx     = np.arange(n_total)\n",
        "        np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]\n",
        "        tr_idx  = idx[n_val:]\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # ---- FE: 每样本 L2 归一化（仅此改动）----\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for ep in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        # ---- FE: 同样做每样本 L2 归一化（仅此改动）----\n",
        "        X = _l2_per_sample(X)\n",
        "        self.model.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ===== Evaluate TorchMLP-BN (same params, per-sample L2 FE) =====\n",
        "import time, numpy as np\n",
        "\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating TorchMLP Agent (BN, 256→128 pyramid) with Per-Sample L2 Normalization\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yXeK4nqWXc-Y",
      "metadata": {
        "id": "yXeK4nqWXc-Y"
      },
      "source": [
        "We try to replace ReLU in first layer with SiLU because it converge more quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "VnzpA4XWX4O4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnzpA4XWX4O4",
        "outputId": "2f541ef1-fb0a-4baa-803b-48732d3b02cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TorchMLP Agent (BN, 100→100; first SiLU then ReLU) with Per-Sample L2 Normalization\n",
            "==================================================\n",
            "Task 1: Accuracy = 97.42%, Time = 6.87s\n",
            "Task 2: Accuracy = 97.27%, Time = 8.36s\n",
            "Task 3: Accuracy = 97.25%, Time = 9.66s\n",
            "Task 4: Accuracy = 97.33%, Time = 9.60s\n",
            "Task 5: Accuracy = 97.24%, Time = 6.81s\n",
            "Task 6: Accuracy = 97.27%, Time = 8.53s\n",
            "Task 7: Accuracy = 97.19%, Time = 6.82s\n",
            "Task 8: Accuracy = 97.41%, Time = 6.36s\n",
            "Task 9: Accuracy = 97.34%, Time = 6.06s\n",
            "Task 10: Accuracy = 97.46%, Time = 5.83s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 97.32% ± 0.08%\n",
            "  Total time: 74.90s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP (首层 SiLU，其余 ReLU；Adam lr=1e-3; bs=128; epochs=3; val=0.2) + FE: per-sample L2 normalization =====\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 限制CPU线程（贴合评测约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,28,28)/(N,784) np.uint8/float -> torch.float32 in [0,1]\"\"\"\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,) or (N,1) -> (N,) int64\"\"\"\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# ------------ FE: per-sample L2 normalization ------------\n",
        "def _l2_per_sample(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    对每一张图做 L2 归一化；返回与输入相同形状。\n",
        "    只做输入侧FE，其他训练/结构/超参一律不变。\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "    if x.dim() == 3:  # (N,28,28) -> (N,784)\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig_shape) == 3:\n",
        "        x = x.view(orig_shape)\n",
        "    return x\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"\n",
        "    方案3：结构仍为 784 -> 100 -> 100 -> 10，层内 BN；\n",
        "          首层激活使用 SiLU，第二层保持 ReLU。\n",
        "    隐藏层 bias=False（配合BN），输出层 bias=True；Xavier 初始化 + bias=0。\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h, bias=False),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.SiLU(inplace=True),          # ← 首层激活换为 SiLU\n",
        "\n",
        "            nn.Linear(h, h, bias=False),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),          # ← 第二层保持 ReLU\n",
        "\n",
        "            nn.Linear(h, out_dim, bias=True),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 3:           # (N,28,28)\n",
        "            x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    Torch MLP with BatchNorm（参数完全不变） + FE：\n",
        "      * 每张图做 L2 归一化；\n",
        "      * 训练和预测都使用同样的 FE；\n",
        "      * 其它超参/结构/循环不变。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dim: int = 10, seed: int = 42,\n",
        "                 epochs: int = 3, batch_size: int = 128,\n",
        "                 lr: float = 1e-3, val_ratio: float = 0.2, weight_decay: float = 0.0):\n",
        "        self.output_dim   = output_dim\n",
        "        self.epochs       = int(epochs)\n",
        "        self.batch_size   = int(batch_size)\n",
        "        self.lr           = float(lr)\n",
        "        self.val_ratio    = float(val_ratio)\n",
        "        self.weight_decay = float(weight_decay)\n",
        "        self.device       = torch.device(\"cpu\")\n",
        "\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # 仍然以 h=100 调用，结构内部已按方案3定义\n",
        "        self.model = _MLP_BN(in_dim=784, h=100, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X: torch.Tensor, y: torch.Tensor, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    # === 训练/预测逻辑保持不变，仅在输入端加入 per-sample L2 FE ===\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        X = _as_float_01(X_train)\n",
        "        y = _labels_1d(y_train)\n",
        "\n",
        "        n_total = X.shape[0]\n",
        "        n_val   = int(self.val_ratio * n_total)\n",
        "        idx     = np.arange(n_total)\n",
        "        np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]\n",
        "        tr_idx  = idx[n_val:]\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # ---- FE: 每样本 L2 归一化（仅此改动）----\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for ep in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        # ---- FE: 同样做每样本 L2 归一化（仅此改动）----\n",
        "        X = _l2_per_sample(X)\n",
        "        self.model.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ===== Evaluate TorchMLP (SiLU first layer, ReLU second) with per-sample L2 FE =====\n",
        "import time, numpy as np\n",
        "\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating TorchMLP Agent (BN, 100→100; first SiLU then ReLU) with Per-Sample L2 Normalization\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We try to add one hidden layer to improve accuracy"
      ],
      "metadata": {
        "id": "RZZ-i4Vi9Tpj"
      },
      "id": "RZZ-i4Vi9Tpj"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "lE5ioYC4Ynm2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE5ioYC4Ynm2",
        "outputId": "3115af27-62f1-411c-e4da-e66f2e070503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TorchMLP Agent (Deep 128×3) with Per-Sample L2 Normalization\n",
            "==================================================\n",
            "Task 1: Accuracy = 97.62%, Time = 14.18s\n",
            "Task 2: Accuracy = 97.50%, Time = 11.29s\n",
            "Task 3: Accuracy = 97.34%, Time = 10.01s\n",
            "Task 4: Accuracy = 97.33%, Time = 10.45s\n",
            "Task 5: Accuracy = 97.33%, Time = 11.45s\n",
            "Task 6: Accuracy = 97.29%, Time = 8.76s\n",
            "Task 7: Accuracy = 97.37%, Time = 10.84s\n",
            "Task 8: Accuracy = 97.73%, Time = 9.95s\n",
            "Task 9: Accuracy = 97.41%, Time = 10.81s\n",
            "Task 10: Accuracy = 97.48%, Time = 10.54s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 97.44% ± 0.14%\n",
            "  Total time: 108.28s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP-Deep (128-128-128; BN+ReLU; Adam lr=1e-3; bs=128; epochs=3; val=0.2) + FE: per-sample L2 normalization =====\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 限制CPU线程（贴合评测约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,28,28)/(N,784) np.uint8/float -> torch.float32 in [0,1]\"\"\"\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,) or (N,1) -> (N,) int64\"\"\"\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# ------------ FE: per-sample L2 normalization ------------\n",
        "def _l2_per_sample(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    对每一张图做 L2 归一化；返回与输入相同形状。\n",
        "    只做输入侧FE，其他训练/结构/超参一律不变。\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "    if x.dim() == 3:  # (N,28,28) -> (N,784)\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig_shape) == 3:\n",
        "        x = x.view(orig_shape)\n",
        "    return x\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"\n",
        "    方案4：瘦而稍深 —— 784 -> 128 -> 128 -> 128 -> 10\n",
        "    三个隐藏层均为 BN + ReLU；隐藏层 bias=False（配合BN），输出层 bias=True。\n",
        "    保持与外部相同签名(in_dim, h, out_dim)，但内部固定为 128-128-128。\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10):\n",
        "        super().__init__()\n",
        "        d = 128\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, d, bias=False), nn.BatchNorm1d(d), nn.ReLU(inplace=True),\n",
        "            nn.Linear(d, d, bias=False),      nn.BatchNorm1d(d), nn.ReLU(inplace=True),\n",
        "            nn.Linear(d, d, bias=False),      nn.BatchNorm1d(d), nn.ReLU(inplace=True),\n",
        "            nn.Linear(d, out_dim, bias=True),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 3:           # (N,28,28)\n",
        "            x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    Torch MLP with BatchNorm（参数完全不变） + FE：\n",
        "      * 每张图做 L2 归一化；\n",
        "      * 训练和预测都使用同样的 FE；\n",
        "      * 其它超参/结构/循环不变。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dim: int = 10, seed: int = 42,\n",
        "                 epochs: int = 3, batch_size: int = 128,\n",
        "                 lr: float = 1e-3, val_ratio: float = 0.2, weight_decay: float = 0.0):\n",
        "        self.output_dim   = output_dim\n",
        "        self.epochs       = int(epochs)\n",
        "        self.batch_size   = int(batch_size)\n",
        "        self.lr           = float(lr)\n",
        "        self.val_ratio    = float(val_ratio)\n",
        "        self.weight_decay = float(weight_decay)\n",
        "        self.device       = torch.device(\"cpu\")\n",
        "\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # 保持外部调用格式不变：仍然传 h=100；内部固定为 128-128-128\n",
        "        self.model = _MLP_BN(in_dim=784, h=100, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X: torch.Tensor, y: torch.Tensor, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    # === 训练/预测逻辑保持不变，仅在输入端加入 per-sample L2 FE ===\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        X = _as_float_01(X_train)\n",
        "        y = _labels_1d(y_train)\n",
        "\n",
        "        n_total = X.shape[0]\n",
        "        n_val   = int(self.val_ratio * n_total)\n",
        "        idx     = np.arange(n_total)\n",
        "        np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]\n",
        "        tr_idx  = idx[n_val:]\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # ---- FE: 每样本 L2 归一化（仅此改动）----\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for ep in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        # ---- FE: 同样做每样本 L2 归一化（仅此改动）----\n",
        "        X = _l2_per_sample(X)\n",
        "        self.model.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ===== Evaluate TorchMLP-Deep (same params, per-sample L2 FE) =====\n",
        "import time, numpy as np\n",
        "\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating TorchMLP Agent (Deep 128×3) with Per-Sample L2 Normalization\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add bottleneck to the medium layer"
      ],
      "metadata": {
        "id": "ey8nGYvD9pNj"
      },
      "id": "ey8nGYvD9pNj"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8DcdbR7pY_Rw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DcdbR7pY_Rw",
        "outputId": "574e4a60-8224-4423-993e-d6250ed22746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TorchMLP Agent (Bottleneck Residual 256→64→256, then 128) with Per-Sample L2 Normalization\n",
            "==================================================\n",
            "Task 1: Accuracy = 97.63%, Time = 11.61s\n",
            "Task 2: Accuracy = 97.77%, Time = 11.58s\n",
            "Task 3: Accuracy = 97.85%, Time = 14.31s\n",
            "Task 4: Accuracy = 97.77%, Time = 16.63s\n",
            "Task 5: Accuracy = 97.57%, Time = 14.10s\n",
            "Task 6: Accuracy = 97.83%, Time = 11.81s\n",
            "Task 7: Accuracy = 97.80%, Time = 11.57s\n",
            "Task 8: Accuracy = 97.61%, Time = 11.49s\n",
            "Task 9: Accuracy = 97.75%, Time = 11.31s\n",
            "Task 10: Accuracy = 97.73%, Time = 17.71s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 97.73% ± 0.09%\n",
            "  Total time: 132.12s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP-BottleneckRes (784→256 → [Bottleneck 256→64→256] → 128 → 10; Adam lr=1e-3; bs=128; epochs=3; val=0.2)\n",
        "# + FE: per-sample L2 normalization =====\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 限制CPU线程（贴合评测约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,28,28)/(N,784) np.uint8/float -> torch.float32 in [0,1]\"\"\"\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,) or (N,1) -> (N,) int64\"\"\"\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# ------------ FE: per-sample L2 normalization ------------\n",
        "def _l2_per_sample(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    对每一张图做 L2 归一化；返回与输入相同形状。\n",
        "    只做输入侧FE，其他训练/结构/超参一律不变。\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "    if x.dim() == 3:  # (N,28,28) -> (N,784)\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig_shape) == 3:\n",
        "        x = x.view(orig_shape)\n",
        "    return x\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# ===== 方案5：瓶颈残差（更省算力的残差块）=====\n",
        "class _BottleneckBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    预激活瓶颈残差：\n",
        "      BN → SiLU → Linear(d→b, bias=False)\n",
        "      BN → SiLU → Linear(b→d, bias=False)\n",
        "    其中 b << d（本实现 b=64, d=256），计算量更低，3个epoch内更易稳定提升。\n",
        "    \"\"\"\n",
        "    def __init__(self, d: int, b: int):\n",
        "        super().__init__()\n",
        "        self.bn1  = nn.BatchNorm1d(d)\n",
        "        self.act1 = nn.SiLU(inplace=True)\n",
        "        self.fc1  = nn.Linear(d, b, bias=False)\n",
        "\n",
        "        self.bn2  = nn.BatchNorm1d(b)\n",
        "        self.act2 = nn.SiLU(inplace=True)\n",
        "        self.fc2  = nn.Linear(b, d, bias=False)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        h = self.fc1(self.act1(self.bn1(x)))\n",
        "        h = self.fc2(self.act2(self.bn2(h)))\n",
        "        return x + h\n",
        "\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"\n",
        "    为与外部调用保持兼容，保留签名(in_dim, h, out_dim)但内部结构固定：\n",
        "      784 → 256 → [Bottle(256→64→256) × 1] → 128 → 10\n",
        "    隐藏层 bias=False（配合BN），输出层 bias=True；Xavier 初始化 + bias=0。\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10, d1=256, bottleneck=64, d2=128):\n",
        "        super().__init__()\n",
        "        self.inp = nn.Linear(in_dim, d1, bias=False)\n",
        "        self.in_bn = nn.BatchNorm1d(d1)\n",
        "        self.in_act = nn.SiLU(inplace=True)\n",
        "\n",
        "        self.block = _BottleneckBlock(d=d1, b=bottleneck)\n",
        "\n",
        "        self.mid = nn.Sequential(\n",
        "            nn.BatchNorm1d(d1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(d1, d2, bias=False),\n",
        "            nn.BatchNorm1d(d2),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.head = nn.Linear(d2, out_dim, bias=True)\n",
        "\n",
        "        # 初始化\n",
        "        nn.init.xavier_uniform_(self.inp.weight)\n",
        "        nn.init.xavier_uniform_(self.head.weight)\n",
        "        if self.head.bias is not None:\n",
        "            nn.init.zeros_(self.head.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 3:           # (N,28,28)\n",
        "            x = x.view(x.size(0), -1)\n",
        "        h = self.in_act(self.in_bn(self.inp(x)))\n",
        "        h = self.block(h)\n",
        "        h = self.mid(h)\n",
        "        return self.head(h)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    Torch MLP with BatchNorm（参数完全不变） + FE：\n",
        "      * 每张图做 L2 归一化；\n",
        "      * 训练和预测都使用同样的 FE；\n",
        "      * 其它超参/结构/循环不变。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dim: int = 10, seed: int = 42,\n",
        "                 epochs: int = 3, batch_size: int = 128,\n",
        "                 lr: float = 1e-3, val_ratio: float = 0.2, weight_decay: float = 0.0):\n",
        "        self.output_dim   = output_dim\n",
        "        self.epochs       = int(epochs)\n",
        "        self.batch_size   = int(batch_size)\n",
        "        self.lr           = float(lr)\n",
        "        self.val_ratio    = float(val_ratio)\n",
        "        self.weight_decay = float(weight_decay)\n",
        "        self.device       = torch.device(\"cpu\")\n",
        "\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # 仍然以 h=100 调用；内部固定为瓶颈残差结构\n",
        "        self.model = _MLP_BN(in_dim=784, h=100, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X: torch.Tensor, y: torch.Tensor, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    # === 训练/预测逻辑保持不变，仅在输入端加入 per-sample L2 FE ===\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        X = _as_float_01(X_train)\n",
        "        y = _labels_1d(y_train)\n",
        "\n",
        "        n_total = X.shape[0]\n",
        "        n_val   = int(self.val_ratio * n_total)\n",
        "        idx     = np.arange(n_total)\n",
        "        np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]\n",
        "        tr_idx  = idx[n_val:]\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # ---- FE: 每样本 L2 归一化（仅此改动）----\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for ep in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        # ---- FE: 同样做每样本 L2 归一化（仅此改动）----\n",
        "        X = _l2_per_sample(X)\n",
        "        self.model.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ===== Evaluate TorchMLP-BottleneckRes (same params, per-sample L2 FE) =====\n",
        "import time, numpy as np\n",
        "\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating TorchMLP Agent (Bottleneck Residual 256→64→256, then 128) with Per-Sample L2 Normalization\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "try a new structre named DualPath"
      ],
      "metadata": {
        "id": "axwvJ56T92ql"
      },
      "id": "axwvJ56T92ql"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ooBbOXzwZPJ1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooBbOXzwZPJ1",
        "outputId": "d9288acc-a300-477a-8c1f-261fe6fe79ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TorchMLP Agent (DualPath: main 256→128 + skip 64, concat) with Per-Sample L2 Normalization\n",
            "==================================================\n",
            "Task 1: Accuracy = 97.56%, Time = 10.19s\n",
            "Task 2: Accuracy = 97.94%, Time = 10.05s\n",
            "Task 3: Accuracy = 97.68%, Time = 14.54s\n",
            "Task 4: Accuracy = 97.79%, Time = 12.05s\n",
            "Task 5: Accuracy = 97.79%, Time = 16.65s\n",
            "Task 6: Accuracy = 97.85%, Time = 15.88s\n",
            "Task 7: Accuracy = 97.55%, Time = 10.07s\n",
            "Task 8: Accuracy = 97.75%, Time = 14.94s\n",
            "Task 9: Accuracy = 97.63%, Time = 16.56s\n",
            "Task 10: Accuracy = 97.53%, Time = 9.98s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 97.71% ± 0.13%\n",
            "  Total time: 130.92s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP-DualPath (并联双路径：主干 256→128 + 线性快路 64；Concat→10; Adam lr=1e-3; bs=128; epochs=3; val=0.2)\n",
        "# + FE: per-sample L2 normalization =====\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 限制CPU线程（贴合评测约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,28,28)/(N,784) np.uint8/float -> torch.float32 in [0,1]\"\"\"\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,) or (N,1) -> (N,) int64\"\"\"\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# ------------ FE: per-sample L2 normalization ------------\n",
        "def _l2_per_sample(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    对每一张图做 L2 归一化；返回与输入相同形状。\n",
        "    只做输入侧FE，其他训练/结构/超参一律不变。\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "    if x.dim() == 3:  # (N,28,28) -> (N,784)\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig_shape) == 3:\n",
        "        x = x.view(orig_shape)\n",
        "    return x\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"\n",
        "    方案6：并联“双路径”：一个主干非线性通道 + 一个浅层线性快路，最后拼接后分类。\n",
        "      - 主干：784 → 256 → 128  （BN+ReLU）\n",
        "      - 快路：784 → 64         （BN+ReLU）   —— 相当于给模型一条“近端”路径捕获线性/弱非线性\n",
        "      - 融合：Concat(128, 64)=192 → 10\n",
        "    隐藏层 bias=False（配合BN），输出层 bias=True；Xavier 初始化 + bias=0。\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10, d1=256, d2=128, d_skip=64):\n",
        "        super().__init__()\n",
        "        # 主干路径\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(in_dim, d1, bias=False), nn.BatchNorm1d(d1), nn.ReLU(inplace=True),\n",
        "            nn.Linear(d1, d2,   bias=False),   nn.BatchNorm1d(d2), nn.ReLU(inplace=True),\n",
        "        )\n",
        "        # 线性快路（浅路径）\n",
        "        self.skip = nn.Sequential(\n",
        "            nn.Linear(in_dim, d_skip, bias=False), nn.BatchNorm1d(d_skip), nn.ReLU(inplace=True),\n",
        "        )\n",
        "        # 融合后的分类头\n",
        "        self.head = nn.Linear(d2 + d_skip, out_dim, bias=True)\n",
        "\n",
        "        # 初始化\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 3:           # (N,28,28)\n",
        "            x = x.view(x.size(0), -1)\n",
        "        h_main = self.main(x)\n",
        "        h_skip = self.skip(x)\n",
        "        h = torch.cat([h_main, h_skip], dim=1)\n",
        "        return self.head(h)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    Torch MLP with BatchNorm（参数完全不变） + FE：\n",
        "      * 每张图做 L2 归一化；\n",
        "      * 训练和预测都使用同样的 FE；\n",
        "      * 其它超参/结构/循环不变。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dim: int = 10, seed: int = 42,\n",
        "                 epochs: int = 3, batch_size: int = 128,\n",
        "                 lr: float = 1e-3, val_ratio: float = 0.2, weight_decay: float = 0.0):\n",
        "        self.output_dim   = output_dim\n",
        "        self.epochs       = int(epochs)\n",
        "        self.batch_size   = int(batch_size)\n",
        "        self.lr           = float(lr)\n",
        "        self.val_ratio    = float(val_ratio)\n",
        "        self.weight_decay = float(weight_decay)\n",
        "        self.device       = torch.device(\"cpu\")\n",
        "\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # 保持外部调用签名一致（h=100）；内部采用双路径结构\n",
        "        self.model = _MLP_BN(in_dim=784, h=100, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X: torch.Tensor, y: torch.Tensor, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    # === 训练/预测逻辑保持不变，仅在输入端加入 per-sample L2 FE ===\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        X = _as_float_01(X_train)\n",
        "        y = _labels_1d(y_train)\n",
        "\n",
        "        n_total = X.shape[0]\n",
        "        n_val   = int(self.val_ratio * n_total)\n",
        "        idx     = np.arange(n_total)\n",
        "        np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]\n",
        "        tr_idx  = idx[n_val:]\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # ---- FE: 每样本 L2 归一化（仅此改动）----\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for ep in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        # ---- FE: 同样做每样本 L2 归一化（仅此改动）----\n",
        "        X = _l2_per_sample(X)\n",
        "        self.model.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ===== Evaluate TorchMLP-DualPath (same params, per-sample L2 FE) =====\n",
        "import time, numpy as np\n",
        "\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating TorchMLP Agent (DualPath: main 256→128 + skip 64, concat) with Per-Sample L2 Normalization\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mj_8RktPat_n",
      "metadata": {
        "id": "mj_8RktPat_n"
      },
      "source": [
        "we try to add optimizer in our code. Actually,we have tried Adam,RMSprop,SGD,but finally we show the test of RMSprop here because we find it both obtain best accuracy and take least time."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GCdLel_U-NpK"
      },
      "id": "GCdLel_U-NpK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "gR8FOeg5flLD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gR8FOeg5flLD",
        "outputId": "142839a7-53ff-4a5d-9f74-923b874d5e1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TorchMLP (RMSprop + Cosine, 256→128) with Per-Sample L2 Normalization\n",
            "==================================================\n",
            "Task 1: Accuracy = 98.05%, Time = 8.10s\n",
            "Task 2: Accuracy = 98.11%, Time = 8.15s\n",
            "Task 3: Accuracy = 97.94%, Time = 8.04s\n",
            "Task 4: Accuracy = 98.03%, Time = 9.20s\n",
            "Task 5: Accuracy = 98.15%, Time = 8.69s\n",
            "Task 6: Accuracy = 98.08%, Time = 10.93s\n",
            "Task 7: Accuracy = 98.05%, Time = 8.47s\n",
            "Task 8: Accuracy = 97.92%, Time = 10.41s\n",
            "Task 9: Accuracy = 98.06%, Time = 8.20s\n",
            "Task 10: Accuracy = 98.09%, Time = 8.18s\n",
            "\n",
            "TorchMLP Agent Summary:\n",
            "  Mean accuracy: 98.05% ± 0.07%\n",
            "  Total time: 88.36s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP-RMSprop + Cosine (与两优化器脚本中 RMSprop 分支等价) =====\n",
        "# 结构/超参/FE 均与原脚本一致：lr=1e-3, bs=128, epochs=3, val=0.2, per-sample L2\n",
        "import time, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# 限制CPU线程（贴合评测约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np):\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0: x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np):\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# ------------ FE: per-sample L2 normalization ------------\n",
        "def _l2_per_sample(x, eps=1e-6):\n",
        "    orig = x.shape\n",
        "    if x.dim()==3: x = x.view(x.size(0), -1)  # (N,28,28)->(N,784)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig)==3: x = x.view(orig)\n",
        "    return x\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# === 模型：金字塔 256→128（BN+ReLU），与对比脚本相同 ===\n",
        "class _MLP_BN(nn.Module):\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10):\n",
        "        super().__init__()\n",
        "        h1, h2 = 256, 128\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h1, bias=False), nn.BatchNorm1d(h1), nn.ReLU(inplace=True),\n",
        "            nn.Linear(h1, h2, bias=False),     nn.BatchNorm1d(h2), nn.ReLU(inplace=True),\n",
        "            nn.Linear(h2, out_dim, bias=True),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim()==3: x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    固定 RMSprop（alpha=0.99, momentum=0.0, centered=False）+ 余弦退火（与对比脚本一致）\n",
        "    其它保持：epochs=3, bs=128, lr=1e-3, val_ratio=0.2, FE=per-sample L2\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim=10, seed=42, epochs=3, batch_size=128, lr=1e-3,\n",
        "                 val_ratio=0.2, weight_decay=0.0, use_cosine=True):\n",
        "        self.output_dim=output_dim; self.epochs=epochs; self.batch_size=batch_size\n",
        "        self.lr=lr; self.val_ratio=val_ratio; self.weight_decay=weight_decay\n",
        "        self.use_cosine=use_cosine\n",
        "        self.device=torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model=None; self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_BN(in_dim=784, h=100, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X, y, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X = _as_float_01(X_train); y = _labels_1d(y_train)\n",
        "        n_total = X.shape[0]; n_val = int(self.val_ratio * n_total)\n",
        "        idx = np.arange(n_total); np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]; tr_idx = idx[n_val:]\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # FE：每样本 L2（训练/验证一致）\n",
        "        X_tr = _l2_per_sample(X_tr); X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "\n",
        "        # === RMSprop 与对比脚本一致 ===\n",
        "        opt = optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99,\n",
        "                            momentum=0.0, centered=False, weight_decay=self.weight_decay)\n",
        "        sch = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=self.epochs, eta_min=0.0) if self.use_cosine else None\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for _ in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            if sch is not None: sch.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test):\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        X = _l2_per_sample(X)\n",
        "        self.model.eval()\n",
        "        bs=4096; outs=[]\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ===== Evaluate: 单独 RMSprop（与对比脚本 RMSprop 分支完全一致）=====\n",
        "import time, numpy as np\n",
        "\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2,\n",
        "                 use_cosine=True)  # 与两优化器脚本里的 RMSprop=True 对齐\n",
        "\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating TorchMLP (RMSprop + Cosine, 256→128) with Per-Sample L2 Normalization\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None: break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VshOu8SynmDS",
      "metadata": {
        "id": "VshOu8SynmDS"
      },
      "source": [
        "Hyperparameter Tuning for n_epochs = [5, 7, 10] and hidden_size   = [(256,128), (384,192)].Moreover to retraint time, we use compressed model like Dynamic INT8to disease time using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cZae4IQ1nhg_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZae4IQ1nhg_",
        "outputId": "9a4af910-b534-4a2f-8e28-b4a3505542f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Grid Search (RMSprop + FE + Dynamic INT8 inference) ===\n",
            "\n",
            "---- Config: epochs=5, hidden=(256,128), batch_size=128 ----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-941692077.py:109: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  self.model_int8 = tq.quantize_dynamic(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1: acc=98.20%, time=16.26s\n",
            "Task 2: acc=98.24%, time=13.51s\n",
            "Task 3: acc=98.19%, time=13.13s\n",
            "Task 4: acc=98.20%, time=13.68s\n",
            "Task 5: acc=98.23%, time=13.63s\n",
            "Task 6: acc=98.18%, time=12.99s\n",
            "Task 7: acc=98.31%, time=13.09s\n",
            "Task 8: acc=98.27%, time=12.80s\n",
            "Task 9: acc=98.13%, time=12.74s\n",
            "Task 10: acc=98.27%, time=13.04s\n",
            "** Summary -> Mean acc: 98.22% | Total time: 134.87s\n",
            "\n",
            "---- Config: epochs=5, hidden=(384,192), batch_size=128 ----\n",
            "Task 1: acc=98.54%, time=16.63s\n",
            "Task 2: acc=98.36%, time=17.20s\n",
            "Task 3: acc=98.27%, time=18.35s\n",
            "Task 4: acc=98.29%, time=18.13s\n",
            "Task 5: acc=98.35%, time=17.21s\n",
            "Task 6: acc=98.20%, time=17.45s\n",
            "Task 7: acc=98.24%, time=17.65s\n",
            "Task 8: acc=98.35%, time=17.01s\n",
            "Task 9: acc=98.24%, time=17.34s\n",
            "Task 10: acc=98.25%, time=17.54s\n",
            "** Summary -> Mean acc: 98.31% | Total time: 174.51s\n",
            "\n",
            "---- Config: epochs=7, hidden=(256,128), batch_size=128 ----\n",
            "Task 1: acc=98.32%, time=16.98s\n",
            "Task 2: acc=98.27%, time=17.05s\n",
            "Task 3: acc=98.09%, time=17.83s\n",
            "Task 4: acc=98.15%, time=17.63s\n",
            "Task 5: acc=98.27%, time=17.54s\n",
            "Task 6: acc=98.10%, time=18.24s\n",
            "Task 7: acc=98.19%, time=17.67s\n",
            "Task 8: acc=98.23%, time=17.45s\n",
            "Task 9: acc=98.28%, time=17.81s\n",
            "Task 10: acc=98.13%, time=18.60s\n",
            "** Summary -> Mean acc: 98.20% | Total time: 176.79s\n",
            "\n",
            "---- Config: epochs=7, hidden=(384,192), batch_size=128 ----\n",
            "Task 1: acc=98.47%, time=24.48s\n",
            "Task 2: acc=98.38%, time=24.40s\n",
            "Task 3: acc=98.37%, time=24.14s\n",
            "Task 4: acc=98.42%, time=24.27s\n",
            "Task 5: acc=98.50%, time=24.38s\n",
            "Task 6: acc=98.19%, time=24.21s\n",
            "Task 7: acc=98.39%, time=23.75s\n",
            "Task 8: acc=98.32%, time=24.35s\n",
            "Task 9: acc=98.29%, time=25.81s\n",
            "Task 10: acc=98.20%, time=24.33s\n",
            "** Summary -> Mean acc: 98.35% | Total time: 244.11s\n",
            "\n",
            "---- Config: epochs=10, hidden=(256,128), batch_size=128 ----\n",
            "Task 1: acc=98.47%, time=24.55s\n",
            "Task 2: acc=98.30%, time=24.11s\n",
            "Task 3: acc=98.28%, time=24.10s\n",
            "Task 4: acc=98.27%, time=24.02s\n",
            "Task 5: acc=98.32%, time=23.95s\n",
            "Task 6: acc=98.27%, time=24.43s\n",
            "Task 7: acc=98.33%, time=24.47s\n",
            "Task 8: acc=98.36%, time=24.91s\n",
            "Task 9: acc=98.40%, time=24.32s\n",
            "Task 10: acc=98.24%, time=24.18s\n",
            "** Summary -> Mean acc: 98.32% | Total time: 243.05s\n",
            "\n",
            "---- Config: epochs=10, hidden=(384,192), batch_size=128 ----\n",
            "Task 1: acc=98.54%, time=32.43s\n",
            "Task 2: acc=98.40%, time=34.12s\n",
            "Task 3: acc=98.39%, time=34.16s\n",
            "Task 4: acc=98.49%, time=33.31s\n",
            "Task 5: acc=98.40%, time=33.63s\n"
          ]
        }
      ],
      "source": [
        "# ===== TorchMLP-RMSprop(+Cosine) + Dynamic INT8（仅推理） + 6组网格评测 =====\n",
        "# 保持：FE=per-sample L2、RMSprop(alpha=0.99,momentum=0,centered=False)、Cosine LR、CE 损失、CPU 推理\n",
        "import time, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# 评测约束：限制CPU线程\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---------------- I/O 与 FE ----------------\n",
        "def _as_float_01(x_np):\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0: x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np):\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "def _l2_per_sample(x, eps=1e-6):\n",
        "    orig = x.shape\n",
        "    if x.dim()==3: x = x.view(x.size(0), -1)  # (N,28,28)->(N,784)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig)==3: x = x.view(orig)\n",
        "    return x\n",
        "\n",
        "# ---------------- 模型（隐藏层可参数化） ----------------\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"\n",
        "    结构：784 -> h1 -> h2 -> 10，每层 BN+ReLU（输出层无 BN/激活）\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=784, h1=256, h2=128, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h1, bias=False), nn.BatchNorm1d(h1), nn.ReLU(inplace=True),\n",
        "            nn.Linear(h1, h2, bias=False),     nn.BatchNorm1d(h2), nn.ReLU(inplace=True),\n",
        "            nn.Linear(h2, out_dim, bias=True),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim()==3: x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "# ---------------- Agent（新增 dynamic INT8 压缩器） ----------------\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    固定：RMSprop(+Cosine)、CE、FE=L2、CPU\n",
        "    仅新增：compress_dynamic_int8() —— 推理前对 Linear 做 dynamic quant(INT8)\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim=10, seed=42, epochs=3, batch_size=128, lr=1e-3,\n",
        "                 val_ratio=0.2, weight_decay=0.0, use_cosine=True, h1=256, h2=128):\n",
        "        self.output_dim=output_dim; self.epochs=epochs; self.batch_size=batch_size\n",
        "        self.lr=lr; self.val_ratio=val_ratio; self.weight_decay=weight_decay\n",
        "        self.use_cosine=use_cosine; self.h1=h1; self.h2=h2\n",
        "        self.device=torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model=None; self.model_int8=None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_BN(in_dim=784, h1=self.h1, h2=self.h2, out_dim=self.output_dim).to(self.device)\n",
        "        self.model_int8 = None\n",
        "\n",
        "    def _make_loader(self, X, y, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X = _as_float_01(X_train); y = _labels_1d(y_train)\n",
        "        n_total = X.shape[0]; n_val = int(self.val_ratio * n_total)\n",
        "        idx = np.arange(n_total); np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]; tr_idx = idx[n_val:]\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # FE\n",
        "        X_tr = _l2_per_sample(X_tr); X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "\n",
        "        opt = optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99,\n",
        "                            momentum=0.0, centered=False, weight_decay=self.weight_decay)\n",
        "        sch = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=self.epochs, eta_min=0.0) if self.use_cosine else None\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for _ in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            if sch is not None: sch.step()\n",
        "\n",
        "    # ---- 动态 INT8 压缩（只量化 Linear，推理专用）----\n",
        "    def compress_dynamic_int8(self):\n",
        "        import torch.quantization as tq  # 与经典用法一致\n",
        "        mdl = self.model.to(\"cpu\").eval()\n",
        "        self.model_int8 = tq.quantize_dynamic(\n",
        "            mdl, {nn.Linear}, dtype=torch.qint8, inplace=False\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test):\n",
        "        X = _as_float_01(X_test)          # CPU\n",
        "        X = _l2_per_sample(X)\n",
        "        mdl = self.model_int8 if self.model_int8 is not None else self.model\n",
        "        mdl.eval()\n",
        "        bs=4096; outs=[]\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = mdl(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ---------------- 6 组网格评测：epochs×hidden ----------------\n",
        "from permuted_mnist.env.permuted_mnist import PermutedMNISTEnv\n",
        "\n",
        "# 若已有 env，可沿用其 episode 数；否则默认 10\n",
        "try:\n",
        "    EPISODES = env.number_episodes\n",
        "except Exception:\n",
        "    EPISODES = 10\n",
        "\n",
        "search_epochs = [5, 7, 10]\n",
        "hidden_cfgs   = [(256,128), (384,192)]   # 两套隐藏层：常规模型 vs 更大容量\n",
        "\n",
        "all_results = []\n",
        "print(\"\\n=== Grid Search (RMSprop + FE + Dynamic INT8 inference) ===\")\n",
        "\n",
        "for ep in search_epochs:\n",
        "    for (h1, h2) in hidden_cfgs:\n",
        "        # 为可比性，重建环境并固定随机种子\n",
        "        e = PermutedMNISTEnv(number_episodes=EPISODES)\n",
        "        e.set_seed(42)\n",
        "\n",
        "        agent = TorchMLP(output_dim=10, seed=42,\n",
        "                         epochs=ep, batch_size=128, lr=1e-3, val_ratio=0.2,\n",
        "                         use_cosine=True, h1=h1, h2=h2)\n",
        "\n",
        "        accs, times = [], []\n",
        "        print(f\"\\n---- Config: epochs={ep}, hidden=({h1},{h2}), batch_size=128 ----\")\n",
        "        tid = 1\n",
        "        while True:\n",
        "            task = e.get_next_task()\n",
        "            if task is None: break\n",
        "            agent.reset()\n",
        "\n",
        "            t0 = time.time()\n",
        "            agent.train(task['X_train'], task['y_train'])\n",
        "\n",
        "            # 仅在推理前做动态 INT8 压缩；其他完全不变\n",
        "            agent.compress_dynamic_int8()\n",
        "\n",
        "            preds   = agent.predict(task['X_test'])\n",
        "            elapsed = time.time() - t0\n",
        "\n",
        "            acc = e.evaluate(preds, task['y_test'])\n",
        "            accs.append(acc); times.append(elapsed)\n",
        "            print(f\"Task {tid}: acc={acc:.2%}, time={elapsed:.2f}s\")\n",
        "            tid += 1\n",
        "\n",
        "        mean_acc = float(np.mean(accs))\n",
        "        total_t  = float(np.sum(times))\n",
        "        all_results.append((ep, h1, h2, mean_acc, total_t))\n",
        "        print(f\"** Summary -> Mean acc: {mean_acc:.2%} | Total time: {total_t:.2f}s\")\n",
        "\n",
        "# 终汇总\n",
        "print(\"\\n===== Overall Summary (Dynamic INT8 inference) =====\")\n",
        "print(\"epochs |  h1  |  h2  | mean_acc | total_time(s)\")\n",
        "for ep, h1, h2, ma, tt in all_results:\n",
        "    print(f\"{ep:>6} | {h1:>4} | {h2:>4} | {ma:>8.2%} | {tt:>12.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sAWODlPtqmEy",
      "metadata": {
        "id": "sAWODlPtqmEy"
      },
      "source": [
        "Compression under fixed budget: Dynamic Quantization & Global Pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CsYqMZuNwLWo",
      "metadata": {
        "id": "CsYqMZuNwLWo"
      },
      "outputs": [],
      "source": [
        "# ===== Compression under fixed budget: Dynamic Quantization & Global Pruning =====\n",
        "# 固定：epochs=3, bs=128, lr=1e-3, val=0.2, FE=L2 per-sample, Optimizer=RMSprop\n",
        "import time, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "# ---- 贴合评测约束：限制CPU线程 ----\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---------------- 工具/FE ----------------\n",
        "def _as_float_01(x_np):\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0: x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np):\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "def _l2_per_sample(x, eps=1e-6):\n",
        "    orig = x.shape\n",
        "    if x.dim()==3: x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig)==3: x = x.view(orig)\n",
        "    return x\n",
        "\n",
        "# --------------- 模型（与你当前基线一致：784→256→128→10, BN+ReLU） ---------------\n",
        "class _MLP_BN(nn.Module):\n",
        "    def __init__(self, in_dim=784, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 256, bias=False), nn.BatchNorm1d(256), nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 128, bias=False),    nn.BatchNorm1d(128), nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, out_dim, bias=True),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "    def forward(self, x):\n",
        "        if x.dim()==3: x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    def __init__(self, output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2, weight_decay=0.0, use_cosine=True):\n",
        "        self.output_dim=output_dim; self.epochs=epochs; self.batch_size=batch_size\n",
        "        self.lr=lr; self.val_ratio=val_ratio; self.weight_decay=weight_decay; self.use_cosine=use_cosine\n",
        "        self.device=torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model=None; self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_BN(in_dim=784, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X, y, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size, shuffle=shuffle,\n",
        "                                           drop_last=False, num_workers=0, pin_memory=False)\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X = _as_float_01(X_train); y = _labels_1d(y_train)\n",
        "        n_total = X.shape[0]; n_val = int(self.val_ratio*n_total)\n",
        "        idx = np.arange(n_total); np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]; tr_idx = idx[n_val:]\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        X_tr = _l2_per_sample(X_tr); X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99,\n",
        "                            momentum=0.0, centered=False, weight_decay=self.weight_decay)\n",
        "        sch = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=self.epochs, eta_min=0.0) if self.use_cosine else None\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for _ in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            if sch is not None: sch.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test, model=None, bs=4096):\n",
        "        if model is None: model = self.model\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        X = _l2_per_sample(X)\n",
        "        model.eval()\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, 0)\n",
        "\n",
        "# --------------- 压缩函数 ---------------\n",
        "def dynamic_quantize(model: nn.Module) -> nn.Module:\n",
        "    \"\"\"\n",
        "    仅对 Linear 层做 INT8 动态量化；BN/激活保持 FP32。\n",
        "    \"\"\"\n",
        "    qmodel = torch.quantization.quantize_dynamic(\n",
        "        model, {nn.Linear}, dtype=torch.qint8, inplace=False\n",
        "    )\n",
        "    return qmodel\n",
        "\n",
        "def global_prune_and_make_permanent(model: nn.Module, amount: float = 0.4) -> nn.Module:\n",
        "    \"\"\"\n",
        "    对所有 Linear.weight 做全局无结构剪枝，按幅值排名置零，随后移除reparam使其永久化。\n",
        "    amount=0.4 表示总体 40% 权重为零（经验上对该任务较稳）。\n",
        "    \"\"\"\n",
        "    params_to_prune = []\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            params_to_prune.append((m, 'weight'))\n",
        "\n",
        "    # 全局剪枝（按绝对值）\n",
        "    prune.global_unstructured(params_to_prune, pruning_method=prune.L1Unstructured, amount=amount)\n",
        "\n",
        "    # 移除 reparam（把掩码乘积并入 weight.data）\n",
        "    for (m, _) in params_to_prune:\n",
        "        prune.remove(m, 'weight')\n",
        "    return model\n",
        "\n",
        "# --------------- 评测：Baseline vs Quant vs Prune+Quant ---------------\n",
        "import time\n",
        "\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "acc_base, acc_quant, acc_pruneq = [], [], []\n",
        "time_base, time_quant, time_pruneq = [], [], []\n",
        "\n",
        "print(\"Compression study under fixed training budget (RMSprop + FE):\")\n",
        "print(\"Variants: Baseline (FP32) | Dynamic-Quant (INT8 Linear) | Prune(40%)+Dynamic-Quant\")\n",
        "print(\"=\"*85)\n",
        "\n",
        "task_id = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None: break\n",
        "\n",
        "    # 训练（固定预算）\n",
        "    agent = TorchMLP(output_dim=10, seed=42, epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    # ---- Baseline 推理 ----\n",
        "    preds_base = agent.predict(task['X_test'])\n",
        "    t_base = time.time() - t0\n",
        "    a_base = env.evaluate(preds_base, task['y_test'])\n",
        "\n",
        "    # ---- 动态量化（不改变训练；仅推理形态）----\n",
        "    qmodel = dynamic_quantize(agent.model)\n",
        "    t1 = time.time()\n",
        "    preds_q = agent.predict(task['X_test'], model=qmodel)\n",
        "    t_quant = time.time() - t1\n",
        "    a_quant = env.evaluate(preds_q, task['y_test'])\n",
        "\n",
        "    # ---- 剪枝 + 动态量化（剪枝后不做额外训练，以满足“预算不变”）----\n",
        "    pruned = global_prune_and_make_permanent(_MLP_BN(), amount=0.4)\n",
        "    pruned.load_state_dict(agent.model.state_dict(), strict=True)  # 从训练权重复制\n",
        "    pruned = global_prune_and_make_permanent(pruned, amount=0.4)  # 再执行一次确保持久剪枝\n",
        "    pqmodel = dynamic_quantize(pruned)\n",
        "\n",
        "    t2 = time.time()\n",
        "    preds_pq = agent.predict(task['X_test'], model=pqmodel)\n",
        "    t_pruneq = time.time() - t2\n",
        "    a_pruneq = env.evaluate(preds_pq, task['y_test'])\n",
        "\n",
        "    acc_base.append(a_base); acc_quant.append(a_quant); acc_pruneq.append(a_pruneq)\n",
        "    time_base.append(t_base); time_quant.append(t_quant); time_pruneq.append(t_pruneq)\n",
        "\n",
        "    print(f\"Task {task_id}: \"\n",
        "          f\"FP32 acc={a_base:.2%}, time={t_base:.2f}s  |  \"\n",
        "          f\"INT8 acc={a_quant:.2%}, time={t_quant:.2f}s  |  \"\n",
        "          f\"Prune40%+INT8 acc={a_pruneq:.2%}, time={t_pruneq:.2f}s\")\n",
        "    task_id += 1\n",
        "\n",
        "print(\"\\nSummary (mean ± std):\")\n",
        "print(f\"  Baseline     : acc={np.mean(acc_base):.2%} ± {np.std(acc_base):.2%} | time(total)={np.sum(time_base):.2f}s\")\n",
        "print(f\"  Dynamic INT8 : acc={np.mean(acc_quant):.2%} ± {np.std(acc_quant):.2%} | time(total)={np.sum(time_quant):.2f}s\")\n",
        "print(f\"  Prune40%+INT8: acc={np.mean(acc_pruneq):.2%} ± {np.std(acc_pruneq):.2%} | time(total)={np.sum(time_pruneq):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3uvq-VYenXGh",
      "metadata": {
        "id": "3uvq-VYenXGh"
      },
      "source": [
        "Add all models we used before in the base Torchmlp.And to avoid overfitting,we add weightNorm head+labelSmoothing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f-nIXNHDpoVw",
      "metadata": {
        "id": "f-nIXNHDpoVw"
      },
      "outputs": [],
      "source": [
        "# ===== TorchMLP (RMSprop + FE) with SiLU-first + Bottleneck-ResMLP + WeightNorm head + LabelSmoothing + WarmupCosine =====\n",
        "import time, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# CPU 线程限制\n",
        "try: torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception: pass\n",
        "\n",
        "def _as_float_01(x_np):\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0: x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np):\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# ------------ FE: per-sample L2 normalization ------------\n",
        "def _l2_per_sample(x, eps=1e-6):\n",
        "    orig = x.shape\n",
        "    if x.dim()==3: x = x.view(x.size(0), -1)  # (N,28,28)->(N,784)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig)==3: x = x.view(orig)\n",
        "    return x\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# ---- Label Smoothing CE（稳定短训）----\n",
        "class SmoothCE(nn.Module):\n",
        "    def __init__(self, eps=0.05, num_classes=10):\n",
        "        super().__init__(); self.eps=eps; self.C=num_classes\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, logits, target):\n",
        "        logp = self.logsoftmax(logits)\n",
        "        with torch.no_grad():\n",
        "            t = torch.zeros_like(logp).scatter_(1, target.view(-1,1), 1.0)\n",
        "            t = t*(1-self.eps) + self.eps/self.C\n",
        "        return (-t*logp).sum(dim=1).mean()\n",
        "\n",
        "# ---- 1-epoch warmup + cosine（总 epoch 不变）----\n",
        "class WarmupCosine:\n",
        "    def __init__(self, optimizer, total_epochs, warmup_epochs=1):\n",
        "        self.opt=optimizer; self.total=total_epochs; self.warm=warmup_epochs\n",
        "        self.base=[g['lr'] for g in optimizer.param_groups]; self.ep=0\n",
        "    def step(self):\n",
        "        self.ep += 1\n",
        "        for i,g in enumerate(self.opt.param_groups):\n",
        "            base=self.base[i]\n",
        "            if self.ep<=self.warm:\n",
        "                lr = base*self.ep/max(1,self.warm)\n",
        "            else:\n",
        "                prog=(self.ep-self.warm)/max(1,(self.total-self.warm))\n",
        "                lr = 0.5*base*(1+np.cos(np.pi*prog))\n",
        "            g['lr']=lr\n",
        "\n",
        "# ---- 轻量残差瓶颈（pre-act，256→64→256）----\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, d=256, b=64):\n",
        "        super().__init__()\n",
        "        self.bn1=nn.BatchNorm1d(d); self.act1=nn.SiLU(inplace=True)\n",
        "        self.fc1=nn.Linear(d,b, bias=False)\n",
        "        self.bn2=nn.BatchNorm1d(b); self.act2=nn.SiLU(inplace=True)\n",
        "        self.fc2=nn.Linear(b,d, bias=False)\n",
        "        nn.init.kaiming_uniform_(self.fc1.weight, a=0, nonlinearity='relu')\n",
        "        nn.init.kaiming_uniform_(self.fc2.weight, a=0, nonlinearity='relu')\n",
        "    def forward(self, x):\n",
        "        h=self.fc1(self.act1(self.bn1(x)))\n",
        "        h=self.fc2(self.act2(self.bn2(h)))\n",
        "        return x+h\n",
        "\n",
        "# ---- 模型：784→256→(bottleneck)→128→10；首层 SiLU；分类头 WeightNorm ----\n",
        "class _MLP_ResBN(nn.Module):\n",
        "    def __init__(self, in_dim=784, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, 256, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(256); self.act1 = nn.SiLU(inplace=True)\n",
        "\n",
        "        self.block = Bottleneck(d=256, b=64)  # 仅 1 个，计算开销很小\n",
        "\n",
        "        self.fc2 = nn.Linear(256, 128, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(128); self.act2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        head = nn.Linear(128, out_dim, bias=True)\n",
        "        self.head = nn.utils.weight_norm(head)   # 稳定最后分类层\n",
        "\n",
        "        # 初始化（ReLU/SiLU 更适合 Kaiming）\n",
        "        for m in [self.fc1, self.fc2, head]:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, a=0, nonlinearity='relu')\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim()==3: x=x.view(x.size(0), -1)\n",
        "        x = self.act1(self.bn1(self.fc1(x)))\n",
        "        x = self.block(x)\n",
        "        x = self.act2(self.bn2(self.fc2(x)))\n",
        "        return self.head(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    不改基础预算：epochs=3, batch=128, lr=1e-3, RMSprop, FE=L2 per-sample\n",
        "    仅加：SiLU首层 + 1个瓶颈残差 + WeightNorm头 + LabelSmoothing + WarmupCosine\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim=10, seed=42, epochs=3, batch_size=128, lr=1e-3,\n",
        "                 val_ratio=0.2, weight_decay=0.0):\n",
        "        self.output_dim=output_dim; self.epochs=epochs; self.batch_size=batch_size\n",
        "        self.lr=lr; self.val_ratio=val_ratio; self.weight_decay=weight_decay\n",
        "        self.device=torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model=None; self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_ResBN(in_dim=784, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X, y, shuffle=True):\n",
        "        ds=torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X=_as_float_01(X_train); y=_labels_1d(y_train)\n",
        "        n_total=X.shape[0]; n_val=int(self.val_ratio*n_total)\n",
        "        idx=np.arange(n_total); np.random.shuffle(idx)\n",
        "        val_idx=idx[:n_val]; tr_idx=idx[n_val:]\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # FE：每样本 L2\n",
        "        X_tr=_l2_per_sample(X_tr); X_val=_l2_per_sample(X_val)\n",
        "\n",
        "        loader=self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "\n",
        "        opt=optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99,\n",
        "                          momentum=0.0, centered=False, weight_decay=self.weight_decay)\n",
        "        sch=WarmupCosine(opt, total_epochs=self.epochs, warmup_epochs=1)\n",
        "        crit=SmoothCE(0.05, self.output_dim)\n",
        "\n",
        "        self.model.train()\n",
        "        for _ in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits=self.model(xb)\n",
        "                loss=crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            sch.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test):\n",
        "        X=_as_float_01(X_test).to(self.device)\n",
        "        X=_l2_per_sample(X)\n",
        "        self.model.eval()\n",
        "        bs=4096; outs=[]\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits=self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ===== Evaluate =====\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "print(\"Evaluating TorchMLP (RMSprop + FE + SiLU + Bottleneck + WeightNorm + LS + WarmupCosine)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "task_num=1\n",
        "while True:\n",
        "    task=env.get_next_task()\n",
        "    if task is None: break\n",
        "    agent.reset()\n",
        "    t0=time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds=agent.predict(task['X_test'])\n",
        "    elapsed=time.time()-t0\n",
        "    acc=env.evaluate(preds, task['y_test'])\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: acc={acc:.2%}, time={elapsed:.2f}s\")\n",
        "    task_num+=1\n",
        "\n",
        "print(\"\\nSummary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nStk5bOleFE8",
      "metadata": {
        "id": "nStk5bOleFE8"
      },
      "source": [
        "we are satisfied with is output.So we add compressed model for hyparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9yjNbcxxeJET",
      "metadata": {
        "id": "9yjNbcxxeJET"
      },
      "outputs": [],
      "source": [
        "# ===== TorchMLP (RMSprop + FE) with SiLU-first + Bottleneck-ResMLP + WeightNorm head\n",
        "#       + LabelSmoothing + WarmupCosine + Dynamic INT8（仅推理） + 网格搜索 =====\n",
        "import time, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# CPU 线程限制\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ------------------------- I/O 与预处理 -------------------------\n",
        "def _as_float_01(x_np):\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np):\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# FE：每样本 L2 归一化\n",
        "def _l2_per_sample(x, eps=1e-6):\n",
        "    orig = x.shape\n",
        "    if x.dim() == 3:\n",
        "        x = x.view(x.size(0), -1)  # (N,28,28)->(N,784)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig) == 3:\n",
        "        x = x.view(orig)\n",
        "    return x\n",
        "\n",
        "# ------------------------- 损失与调度 -------------------------\n",
        "class SmoothCE(nn.Module):\n",
        "    def __init__(self, eps=0.05, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.C = num_classes\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        logp = self.logsoftmax(logits)\n",
        "        with torch.no_grad():\n",
        "            t = torch.zeros_like(logp).scatter_(1, target.view(-1, 1), 1.0)\n",
        "            t = t * (1 - self.eps) + self.eps / self.C\n",
        "        return (-t * logp).sum(dim=1).mean()\n",
        "\n",
        "class WarmupCosine:\n",
        "    def __init__(self, optimizer, total_epochs, warmup_epochs=1):\n",
        "        self.opt = optimizer\n",
        "        self.total = total_epochs\n",
        "        self.warm = warmup_epochs\n",
        "        self.base = [g['lr'] for g in optimizer.param_groups]\n",
        "        self.ep = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.ep += 1\n",
        "        for i, g in enumerate(self.opt.param_groups):\n",
        "            base = self.base[i]\n",
        "            if self.ep <= self.warm:\n",
        "                lr = base * self.ep / max(1, self.warm)\n",
        "            else:\n",
        "                prog = (self.ep - self.warm) / max(1, (self.total - self.warm))\n",
        "                lr = 0.5 * base * (1 + np.cos(np.pi * prog))\n",
        "            g['lr'] = lr\n",
        "\n",
        "# ------------------------- 模型 -------------------------\n",
        "class Bottleneck(nn.Module):\n",
        "    # 轻量残差瓶颈（pre-act，256→64→256）\n",
        "    def __init__(self, d=256, b=64):\n",
        "        super().__init__()\n",
        "        self.bn1 = nn.BatchNorm1d(d)\n",
        "        self.act1 = nn.SiLU(inplace=True)\n",
        "        self.fc1 = nn.Linear(d, b, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(b)\n",
        "        self.act2 = nn.SiLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(b, d, bias=False)\n",
        "        nn.init.kaiming_uniform_(self.fc1.weight, a=0, nonlinearity='relu')\n",
        "        nn.init.kaiming_uniform_(self.fc2.weight, a=0, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.fc1(self.act1(self.bn1(x)))\n",
        "        h = self.fc2(self.act2(self.bn2(h)))\n",
        "        return x + h\n",
        "\n",
        "class _MLP_ResBN(nn.Module):\n",
        "    # 784→256→(bottleneck)→128→10；首层 SiLU；分类头 WeightNorm\n",
        "    def __init__(self, in_dim=784, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, 256, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.act1 = nn.SiLU(inplace=True)\n",
        "\n",
        "        self.block = Bottleneck(d=256, b=64)\n",
        "\n",
        "        self.fc2 = nn.Linear(256, 128, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.act2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        head = nn.Linear(128, out_dim, bias=True)\n",
        "        self.head = nn.utils.weight_norm(head)  # 最后一层加 WeightNorm\n",
        "\n",
        "        for m in [self.fc1, self.fc2, head]:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, a=0, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 3:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        x = self.act1(self.bn1(self.fc1(x)))\n",
        "        x = self.block(x)\n",
        "        x = self.act2(self.bn2(self.fc2(x)))\n",
        "        return self.head(x)\n",
        "\n",
        "# ------------------------- Agent（仅新增 dynamic INT8 压缩器） -------------------------\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    预算与流程不变：\n",
        "      RMSprop、FE(L2)、SiLU首层、瓶颈、WeightNorm、LabelSmoothing、WarmupCosine\n",
        "    仅新增：\n",
        "      - compress_dynamic_int8(): 推理前做 dynamic quant（INT8, Linear）\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim=10, seed=42, epochs=3, batch_size=128, lr=1e-3,\n",
        "                 val_ratio=0.2, weight_decay=0.0):\n",
        "        self.output_dim = output_dim\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.val_ratio = val_ratio\n",
        "        self.weight_decay = weight_decay\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.model_int8 = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_ResBN(in_dim=784, out_dim=self.output_dim).to(self.device)\n",
        "        self.model_int8 = None  # 每个任务重置\n",
        "\n",
        "    def _make_loader(self, X, y, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(\n",
        "            ds, batch_size=self.batch_size, shuffle=shuffle,\n",
        "            drop_last=False, num_workers=0, pin_memory=False\n",
        "        )\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X = _as_float_01(X_train); y = _labels_1d(y_train)\n",
        "        n_total = X.shape[0]; n_val = int(self.val_ratio * n_total)\n",
        "        idx = np.arange(n_total); np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]; tr_idx = idx[n_val:]\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # FE：每样本 L2\n",
        "        X_tr = _l2_per_sample(X_tr); X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "\n",
        "        opt = optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99,\n",
        "                            momentum=0.0, centered=False, weight_decay=self.weight_decay)\n",
        "        sch = WarmupCosine(opt, total_epochs=self.epochs, warmup_epochs=1)\n",
        "        crit = SmoothCE(0.05, self.output_dim)\n",
        "\n",
        "        self.model.train()\n",
        "        for _ in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            sch.step()\n",
        "\n",
        "    # ---- 动态 INT8（与截图同款：torch.quantization.quantize_dynamic） ----\n",
        "    def compress_dynamic_int8(self):\n",
        "        \"\"\"\n",
        "        - 只量化 nn.Linear 到 INT8\n",
        "        - 推理专用，训练完全不改\n",
        "        - 量化前移除 head 的 weight_norm，避免 deepcopy 报错\n",
        "        \"\"\"\n",
        "        import torch.quantization as tq\n",
        "        mdl = self.model.to(\"cpu\").eval()\n",
        "\n",
        "        # 移除 WeightNorm（否则 quantize_dynamic 内部 deepcopy 可能报错）\n",
        "        try:\n",
        "            nn.utils.remove_weight_norm(mdl.head)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        self.model_int8 = tq.quantize_dynamic(\n",
        "            mdl, {nn.Linear}, dtype=torch.qint8, inplace=False\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test):\n",
        "        X = _as_float_01(X_test)  # 保持 CPU\n",
        "        X = _l2_per_sample(X)\n",
        "        mdl = self.model_int8 if self.model_int8 is not None else self.model\n",
        "        mdl.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = mdl(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ------------------------- 网格搜索（6 组） -------------------------\n",
        "from permuted_mnist.env.permuted_mnist import PermutedMNISTEnv\n",
        "\n",
        "# 若你 notebook 里已有 env，则沿用它的 episode 数；否则默认 10\n",
        "try:\n",
        "    EPISODES = env.number_episodes\n",
        "except Exception:\n",
        "    EPISODES = 10\n",
        "\n",
        "search_epochs = [5, 7, 10]\n",
        "search_batch  = [100, 128]\n",
        "\n",
        "all_results = []\n",
        "print(\"\\n=== Grid Search with Dynamic INT8 (only inference) ===\")\n",
        "\n",
        "for ep in search_epochs:\n",
        "    for bs in search_batch:\n",
        "        env_gs = PermutedMNISTEnv(number_episodes=EPISODES)\n",
        "        env_gs.set_seed(42)\n",
        "\n",
        "        agent = TorchMLP(output_dim=10, seed=42,\n",
        "                         epochs=ep, batch_size=bs, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "        accs, times = [], []\n",
        "        print(f\"\\n---- Config: epochs={ep}, batch_size={bs} ----\")\n",
        "        tid = 1\n",
        "        while True:\n",
        "            task = env_gs.get_next_task()\n",
        "            if task is None:\n",
        "                break\n",
        "            agent.reset()\n",
        "\n",
        "            t0 = time.time()\n",
        "            agent.train(task['X_train'], task['y_train'])\n",
        "\n",
        "            # 只在推理前做动态 INT8；其他保持不变\n",
        "            agent.compress_dynamic_int8()\n",
        "\n",
        "            preds   = agent.predict(task['X_test'])\n",
        "            elapsed = time.time() - t0\n",
        "\n",
        "            acc = env_gs.evaluate(preds, task['y_test'])\n",
        "            accs.append(acc); times.append(elapsed)\n",
        "            print(f\"Task {tid}: acc={acc:.2%}, time={elapsed:.2f}s\")\n",
        "            tid += 1\n",
        "\n",
        "        mean_acc = float(np.mean(accs))\n",
        "        total_t  = float(np.sum(times))\n",
        "        all_results.append((ep, bs, mean_acc, total_t))\n",
        "        print(f\"** Summary (epochs={ep}, batch={bs}) -> \"\n",
        "              f\"Mean acc: {mean_acc:.2%} | Total time: {total_t:.2f}s\")\n",
        "\n",
        "print(\"\\n===== Overall Summary (Dynamic INT8) =====\")\n",
        "print(\"epochs | batch | mean_acc | total_time(s)\")\n",
        "for ep, bs, ma, tt in all_results:\n",
        "    print(f\"{ep:>6} | {bs:>5} | {ma:>8.2%} | {tt:>12.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rAxaZMIMyixB",
      "metadata": {
        "id": "rAxaZMIMyixB"
      },
      "source": [
        "So at last ,we used this version with n_epochs=10 and batch_size=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KAPNW_ZQtn7Z",
      "metadata": {
        "id": "KAPNW_ZQtn7Z"
      },
      "outputs": [],
      "source": [
        "# ===== TorchMLP (RMSprop + FE + SiLU + Bottleneck + WeightNorm + LabelSmoothing + WarmupCosine + Dynamic INT8) =====\n",
        "import time, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# 限制 CPU 线程（2 核约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ------------------------- I/O 与预处理 -------------------------\n",
        "def _as_float_01(x_np):\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np):\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "def _l2_per_sample(x, eps=1e-6):\n",
        "    orig = x.shape\n",
        "    if x.dim() == 3:\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig) == 3:\n",
        "        x = x.view(orig)\n",
        "    return x\n",
        "\n",
        "# ------------------------- 损失与调度 -------------------------\n",
        "class SmoothCE(nn.Module):\n",
        "    def __init__(self, eps=0.05, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.C = num_classes\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        logp = self.logsoftmax(logits)\n",
        "        with torch.no_grad():\n",
        "            t = torch.zeros_like(logp).scatter_(1, target.view(-1, 1), 1.0)\n",
        "            t = t * (1 - self.eps) + self.eps / self.C\n",
        "        return (-t * logp).sum(dim=1).mean()\n",
        "\n",
        "class WarmupCosine:\n",
        "    def __init__(self, optimizer, total_epochs, warmup_epochs=1):\n",
        "        self.opt = optimizer\n",
        "        self.total = total_epochs\n",
        "        self.warm = warmup_epochs\n",
        "        self.base = [g['lr'] for g in optimizer.param_groups]\n",
        "        self.ep = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.ep += 1\n",
        "        for i, g in enumerate(self.opt.param_groups):\n",
        "            base = self.base[i]\n",
        "            if self.ep <= self.warm:\n",
        "                lr = base * self.ep / max(1, self.warm)\n",
        "            else:\n",
        "                prog = (self.ep - self.warm) / max(1, (self.total - self.warm))\n",
        "                lr = 0.5 * base * (1 + np.cos(np.pi * prog))\n",
        "            g['lr'] = lr\n",
        "\n",
        "# ------------------------- 模型 -------------------------\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, d=256, b=64):\n",
        "        super().__init__()\n",
        "        self.bn1 = nn.BatchNorm1d(d)\n",
        "        self.act1 = nn.SiLU(inplace=True)\n",
        "        self.fc1 = nn.Linear(d, b, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(b)\n",
        "        self.act2 = nn.SiLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(b, d, bias=False)\n",
        "        nn.init.kaiming_uniform_(self.fc1.weight, a=0, nonlinearity='relu')\n",
        "        nn.init.kaiming_uniform_(self.fc2.weight, a=0, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.fc1(self.act1(self.bn1(x)))\n",
        "        h = self.fc2(self.act2(self.bn2(h)))\n",
        "        return x + h\n",
        "\n",
        "class _MLP_ResBN(nn.Module):\n",
        "    def __init__(self, in_dim=784, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, 256, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.act1 = nn.SiLU(inplace=True)\n",
        "        self.block = Bottleneck(d=256, b=64)\n",
        "        self.fc2 = nn.Linear(256, 128, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.act2 = nn.ReLU(inplace=True)\n",
        "        head = nn.Linear(128, out_dim, bias=True)\n",
        "        self.head = nn.utils.weight_norm(head)\n",
        "        for m in [self.fc1, self.fc2, head]:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, a=0, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 3:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        x = self.act1(self.bn1(self.fc1(x)))\n",
        "        x = self.block(x)\n",
        "        x = self.act2(self.bn2(self.fc2(x)))\n",
        "        return self.head(x)\n",
        "\n",
        "# ------------------------- Agent（含 dynamic INT8 压缩器） -------------------------\n",
        "class TorchMLP:\n",
        "    def __init__(self, output_dim=10, seed=42, epochs=10, batch_size=128, lr=1e-3,\n",
        "                 val_ratio=0.2, weight_decay=0.0):\n",
        "        self.output_dim = output_dim\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.val_ratio = val_ratio\n",
        "        self.weight_decay = weight_decay\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.model_int8 = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_ResBN(in_dim=784, out_dim=self.output_dim).to(self.device)\n",
        "        self.model_int8 = None\n",
        "\n",
        "    def _make_loader(self, X, y, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(\n",
        "            ds, batch_size=self.batch_size, shuffle=shuffle,\n",
        "            drop_last=False, num_workers=0, pin_memory=False\n",
        "        )\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X = _as_float_01(X_train); y = _labels_1d(y_train)\n",
        "        n_total = X.shape[0]; n_val = int(self.val_ratio * n_total)\n",
        "        idx = np.arange(n_total); np.random.shuffle(idx)\n",
        "        val_idx = idx[:n_val]; tr_idx = idx[n_val:]\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "\n",
        "        opt = optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99,\n",
        "                            momentum=0.0, centered=False, weight_decay=self.weight_decay)\n",
        "        sch = WarmupCosine(opt, total_epochs=self.epochs, warmup_epochs=1)\n",
        "        crit = SmoothCE(0.05, self.output_dim)\n",
        "\n",
        "        self.model.train()\n",
        "        for _ in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            sch.step()\n",
        "\n",
        "    def compress_dynamic_int8(self):\n",
        "        import torch.quantization as tq\n",
        "        mdl = self.model.to(\"cpu\").eval()\n",
        "        try:\n",
        "            nn.utils.remove_weight_norm(mdl.head)\n",
        "        except Exception:\n",
        "            pass\n",
        "        self.model_int8 = tq.quantize_dynamic(\n",
        "            mdl, {nn.Linear}, dtype=torch.qint8, inplace=False\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test):\n",
        "        X = _as_float_01(X_test)\n",
        "        X = _l2_per_sample(X)\n",
        "        mdl = self.model_int8 if self.model_int8 is not None else self.model\n",
        "        mdl.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = mdl(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ------------------------- 单组评测：epoch=10, batch=128 -------------------------\n",
        "from permuted_mnist.env.permuted_mnist import PermutedMNISTEnv\n",
        "env_gs = PermutedMNISTEnv(number_episodes=10)\n",
        "env_gs.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42, epochs=10, batch_size=128, lr=1e-3, val_ratio=0.2)\n",
        "\n",
        "accs, times = [], []\n",
        "print(\"\\n=== TorchMLP (Dynamic INT8, epochs=10, batch=128) ===\")\n",
        "tid = 1\n",
        "while True:\n",
        "    task = env_gs.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "    agent.reset()\n",
        "\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    agent.compress_dynamic_int8()\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "\n",
        "    acc = env_gs.evaluate(preds, task['y_test'])\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {tid}: acc={acc:.2%}, time={elapsed:.2f}s\")\n",
        "    tid += 1\n",
        "\n",
        "mean_acc = float(np.mean(accs))\n",
        "total_t = float(np.sum(times))\n",
        "print(f\"\\n** Summary (epochs=10, batch=128) -> Mean acc: {mean_acc:.2%} | Total time: {total_t:.2f}s **\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## failed experiment"
      ],
      "metadata": {
        "id": "yymK01aUBmE7"
      },
      "id": "yymK01aUBmE7"
    },
    {
      "cell_type": "markdown",
      "id": "Zw1wIGDKlTVh",
      "metadata": {
        "id": "Zw1wIGDKlTVh"
      },
      "source": [
        "Grid Search over (epochs, batch_size) for RMSprop + FE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6KfjXyceihqp",
      "metadata": {
        "id": "6KfjXyceihqp"
      },
      "outputs": [],
      "source": [
        "# ===== Grid Search over (epochs, batch_size) for RMSprop + FE =====\n",
        "import time, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np):\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0: x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np):\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "def _l2_per_sample(x, eps=1e-6):\n",
        "    orig = x.shape\n",
        "    if x.dim()==3: x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig)==3: x = x.view(orig)\n",
        "    return x\n",
        "\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"784 -> 256 -> 128 -> 10, BatchNorm + ReLU\"\"\"\n",
        "    def __init__(self, in_dim=784, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 256, bias=False), nn.BatchNorm1d(256), nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 128, bias=False),    nn.BatchNorm1d(128), nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, out_dim, bias=True)\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "    def forward(self, x):\n",
        "        if x.dim()==3: x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "class TorchMLP_RMSprop:\n",
        "    def __init__(self, output_dim=10, seed=42, epochs=3, batch_size=128,\n",
        "                 lr=1e-3, val_ratio=0.2, weight_decay=0.0, use_cosine=True):\n",
        "        self.output_dim=output_dim; self.epochs=epochs; self.batch_size=batch_size\n",
        "        self.lr=lr; self.val_ratio=val_ratio; self.weight_decay=weight_decay\n",
        "        self.use_cosine=use_cosine\n",
        "        self.device=torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model=None; self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model=_MLP_BN(784, self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X, y, shuffle=True):\n",
        "        ds=torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X=_as_float_01(X_train); y=_labels_1d(y_train)\n",
        "        n_total=X.shape[0]; n_val=int(self.val_ratio*n_total)\n",
        "        idx=np.arange(n_total); np.random.shuffle(idx)\n",
        "        val_idx=idx[:n_val]; tr_idx=idx[n_val:]\n",
        "        X_tr, y_tr=X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val=X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "        X_tr=_l2_per_sample(X_tr); X_val=_l2_per_sample(X_val)\n",
        "\n",
        "        loader=self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt=optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99,\n",
        "                          momentum=0.0, centered=False, weight_decay=self.weight_decay)\n",
        "        sch=optim.lr_scheduler.CosineAnnealingLR(opt, T_max=self.epochs, eta_min=0.0) if self.use_cosine else None\n",
        "        crit=nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.train()\n",
        "        for _ in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits=self.model(xb)\n",
        "                loss=crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            if sch is not None: sch.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test):\n",
        "        X=_as_float_01(X_test).to(self.device)\n",
        "        X=_l2_per_sample(X)\n",
        "        self.model.eval()\n",
        "        bs=4096; outs=[]\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits=self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ====== 组合测试 ======\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "param_grid = [\n",
        "    (3, 128),\n",
        "    (3, 64),\n",
        "    (4, 128),\n",
        "    (4, 64),\n",
        "]\n",
        "\n",
        "results = []\n",
        "for ep, bs in param_grid:\n",
        "    agent = TorchMLP_RMSprop(output_dim=10, seed=42,\n",
        "                             epochs=ep, batch_size=bs, lr=1e-3,\n",
        "                             val_ratio=0.2, weight_decay=0.0,\n",
        "                             use_cosine=True)\n",
        "    accs, times = [], []\n",
        "    print(f\"\\nEvaluating RMSprop + Cosine | epochs={ep}, batch_size={bs}\")\n",
        "    print(\"=\"*60)\n",
        "    task_num = 1\n",
        "    env.reset()\n",
        "    while True:\n",
        "        task = env.get_next_task()\n",
        "        if task is None: break\n",
        "        agent.reset()\n",
        "        t0 = time.time()\n",
        "        agent.train(task['X_train'], task['y_train'])\n",
        "        preds = agent.predict(task['X_test'])\n",
        "        elapsed = time.time() - t0\n",
        "        acc = env.evaluate(preds, task['y_test'])\n",
        "        accs.append(acc); times.append(elapsed)\n",
        "        print(f\"Task {task_num}: acc={acc:.2%}, time={elapsed:.2f}s\")\n",
        "        task_num += 1\n",
        "    mean_acc, std_acc = np.mean(accs), np.std(accs)\n",
        "    total_time = np.sum(times)\n",
        "    print(f\"→ Summary: mean={mean_acc:.2%} ± {std_acc:.2%} | total={total_time:.2f}s\")\n",
        "    results.append((ep, bs, mean_acc, std_acc, total_time))\n",
        "\n",
        "print(\"\\n=== Grid Search Summary (RMSprop + FE + Cosine) ===\")\n",
        "for ep, bs, m, s, t in results:\n",
        "    print(f\"epochs={ep:>2}, batch={bs:>3} | acc={m:.2%} ± {s:.2%} | time={t:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SzuIh8CmQ-gD",
      "metadata": {
        "id": "SzuIh8CmQ-gD"
      },
      "source": [
        "#　×TorchMLP　调参＋L２正则化＋保守早停"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NQkqtbgFQ7ho",
      "metadata": {
        "id": "NQkqtbgFQ7ho"
      },
      "outputs": [],
      "source": [
        "# ===== TorchMLP-BN (2x100, BN, ReLU; Adam lr=1e-3; bs=128; epochs=3; val=0.2)\n",
        "#      + FE: per-sample L2 normalization\n",
        "#      + Conservative Early Stop (small-val check, best-weight restore) =====\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 限制CPU线程（贴合评测约束）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _as_float_01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,28,28)/(N,784) np.uint8/float -> torch.float32 in [0,1]\"\"\"\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"(N,) or (N,1) -> (N,) int64\"\"\"\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "# ------------ FE: per-sample L2 normalization ------------\n",
        "def _l2_per_sample(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"对每一张图做 L2 归一化；返回与输入相同形状\"\"\"\n",
        "    orig_shape = x.shape\n",
        "    if x.dim() == 3:  # (N,28,28) -> (N,784)\n",
        "        x = x.view(x.size(0), -1)\n",
        "    n = x.norm(dim=1, keepdim=True).clamp_min(eps)\n",
        "    x = x / n\n",
        "    if len(orig_shape) == 3:\n",
        "        x = x.view(orig_shape)\n",
        "    return x\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"784 -> 100 -> 100 -> 10 with BatchNorm + ReLU\"\"\"\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h, out_dim),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 3:           # (N,28,28)\n",
        "            x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    Torch MLP with BatchNorm（参数保持不变） + 方案C FE + 保守早停：\n",
        "      - 结构/超参不变：2×100+BN, Adam(1e-3), bs=128, epochs=3, val=0.2\n",
        "      - FE：每样本 L2 归一化（train/test 一致）\n",
        "      - EarlyStopping：仅在 epoch1/epoch2 用小子集验证；若提升 < min_delta 则不跑 epoch3\n",
        "        并恢复 best 权重；其余流程不变。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dim: int = 10, seed: int = 42,\n",
        "                 epochs: int = 3, batch_size: int = 128,\n",
        "                 lr: float = 1e-3, val_ratio: float = 0.2, weight_decay: float = 0.0,\n",
        "                 es_min_delta: float = 0.0005, es_max_eval: int = 4000):\n",
        "        self.output_dim   = output_dim\n",
        "        self.epochs       = int(epochs)          # 3（不变）\n",
        "        self.batch_size   = int(batch_size)      # 128（不变）\n",
        "        self.lr           = float(lr)            # 1e-3（不变）\n",
        "        self.val_ratio    = float(val_ratio)     # 0.2（不变）\n",
        "        self.weight_decay = float(weight_decay)  # 0.0（不变）\n",
        "\n",
        "        # 早停相关（仅控制是否进入第3轮；不改变上限）\n",
        "        self.es_min_delta = float(es_min_delta)  # 0.05% 提升阈值\n",
        "        self.es_max_eval  = int(es_max_eval)     # 验证子集上限（降低验证时间）\n",
        "\n",
        "        self.device       = torch.device(\"cpu\")\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.model = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_BN(in_dim=784, h=100, out_dim=self.output_dim).to(self.device)\n",
        "\n",
        "    def _make_loader(self, X: torch.Tensor, y: torch.Tensor, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _val_acc_small(self, X_val: torch.Tensor, y_val: torch.Tensor) -> float:\n",
        "        \"\"\"仅用小子集快速验证，用于决定是否进入 epoch3\"\"\"\n",
        "        n = X_val.shape[0]\n",
        "        m = min(self.es_max_eval, n)\n",
        "        Xs = X_val[:m]; ys = y_val[:m]\n",
        "        self.model.eval()\n",
        "        bs_eval = 16384\n",
        "        correct = 0\n",
        "        for i in range(0, m, bs_eval):\n",
        "            logits = self.model(Xs[i:i+bs_eval])\n",
        "            pred = logits.argmax(1)\n",
        "            correct += (pred == ys[i:i+bs_eval]).sum().item()\n",
        "        self.model.train()\n",
        "        return correct / float(m)\n",
        "\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        # === 数据准备（不变） ===\n",
        "        X = _as_float_01(X_train)\n",
        "        y = _labels_1d(y_train)\n",
        "\n",
        "        n_total = X.shape[0]\n",
        "        n_val   = int(self.val_ratio * n_total)\n",
        "        idx     = np.random.permutation(n_total)\n",
        "        val_idx = idx[:n_val]; tr_idx = idx[n_val:]\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # 方案 C：每样本 L2 归一化\n",
        "        X_tr = _l2_per_sample(X_tr)\n",
        "        X_val = _l2_per_sample(X_val)\n",
        "\n",
        "        loader = self._make_loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        # 早停状态（仅比较 ep1 vs ep2）\n",
        "        best_acc = -1.0\n",
        "        best_state = None\n",
        "        acc_ep1 = None\n",
        "\n",
        "        self.model.train()\n",
        "        for ep in range(self.epochs):\n",
        "            # — 训练一个 epoch —\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = self.model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "            # — 只在 ep==0 和 ep==1 做一次小验证 —\n",
        "            if ep <= 1:\n",
        "                val_acc = self._val_acc_small(X_val, y_val)\n",
        "                if val_acc > best_acc + self.es_min_delta:\n",
        "                    best_acc = val_acc\n",
        "                    best_state = {k: v.detach().cpu().clone() for k, v in self.model.state_dict().items()}\n",
        "\n",
        "                if ep == 0:\n",
        "                    acc_ep1 = val_acc\n",
        "                else:\n",
        "                    # 若第二轮较第一轮提升不足阈值 ⇒ 不进入第三轮\n",
        "                    if val_acc < (acc_ep1 + self.es_min_delta):\n",
        "                        break  # 结束训练（最多跑到2个epoch）\n",
        "\n",
        "        # 恢复历史最优权重\n",
        "        if best_state is not None:\n",
        "            self.model.load_state_dict(best_state)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = _as_float_01(X_test).to(self.device)\n",
        "        X = _l2_per_sample(X)  # FE 与训练一致\n",
        "        self.model.eval()\n",
        "        bs = 4096\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ===== Evaluate (same params; accuracy-first early stop) =====\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 epochs=3, batch_size=128, lr=1e-3, val_ratio=0.2,\n",
        "                 es_min_delta=0.0005, es_max_eval=4000)\n",
        "\n",
        "accs, times = [], []\n",
        "print(\"Evaluating TorchMLP Agent (BN, 2x100) + L2 FE + Conservative Early Stop (small-val)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TqD0weGAfQrH",
      "metadata": {
        "id": "TqD0weGAfQrH"
      },
      "source": [
        "#×TorchMLP调参+早停\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jTbK4OFjfQSK",
      "metadata": {
        "id": "jTbK4OFjfQSK"
      },
      "outputs": [],
      "source": [
        "# ==== TorchMLP-BN with Early Stopping + Evaluation (single cell) ====\n",
        "import time, numpy as np, torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 保守地限制CPU线程（与竞赛环境一致）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# --------- utils ----------\n",
        "def _to_float01(x_np: np.ndarray) -> torch.Tensor:\n",
        "    x = torch.from_numpy(x_np).float()\n",
        "    if x.max() > 1.0:  # 如果是0-255，缩放到0-1\n",
        "        x = x / 255.0\n",
        "    return x\n",
        "\n",
        "def _labels_1d(y_np: np.ndarray) -> torch.Tensor:\n",
        "    y = np.asarray(y_np).reshape(-1).astype(np.int64, copy=False)\n",
        "    return torch.from_numpy(y)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _fit_scalar_norm(x: torch.Tensor):\n",
        "    m = float(x.mean()); s = float(x.std())\n",
        "    return m, (s if s >= 1e-6 else 1.0)\n",
        "\n",
        "# --------- model ----------\n",
        "class _MLP_BN(nn.Module):\n",
        "    \"\"\"784 -> 100 -> 100 -> 10 with BatchNorm + ReLU\"\"\"\n",
        "    def __init__(self, in_dim=784, h=100, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h, h),\n",
        "            nn.BatchNorm1d(h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h, out_dim),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 3:  # (N,28,28)\n",
        "            x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "# --------- agent ----------\n",
        "class TorchMLP:\n",
        "    \"\"\"\n",
        "    Torch MLP (2x100 with BN) + 早停 + 可选时间护栏\n",
        "    - Adam(lr=1e-3), batch=128, 最多epochs很大，靠early stopping/时间护栏停止\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim=10, seed=42,\n",
        "                 lr=1e-3, batch_size=128, val_ratio=0.2,\n",
        "                 patience=2, min_delta=0.001,\n",
        "                 time_budget=5.8, min_train_seconds=2.0):\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "        self.output_dim = output_dim\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.val_ratio = val_ratio\n",
        "        self.patience = patience            # 连续多少次无明显提升则停\n",
        "        self.min_delta = min_delta          # 最小提升幅度(绝对提升)\n",
        "        self.time_budget = time_budget      # 每任务训练时间上限（秒）\n",
        "        self.min_train_seconds = min_train_seconds\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.model = _MLP_BN(out_dim=self.output_dim).to(self.device)\n",
        "        self.mean_, self.std_ = 0.0, 1.0\n",
        "\n",
        "    def _loader(self, X, y, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=self.batch_size,\n",
        "                                           shuffle=shuffle, drop_last=False,\n",
        "                                           num_workers=0, pin_memory=False)\n",
        "\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        t0 = time.time()\n",
        "        X = _to_float01(X_train); y = _labels_1d(y_train)\n",
        "        n = X.shape[0]\n",
        "        n_val = max(1000, int(self.val_ratio * n))\n",
        "        idx = np.arange(n); np.random.shuffle(idx)\n",
        "        val_idx, tr_idx = idx[:n_val], idx[n_val:]\n",
        "        X_tr, y_tr = X[tr_idx].to(self.device), y[tr_idx].to(self.device)\n",
        "        X_val, y_val = X[val_idx].to(self.device), y[val_idx].to(self.device)\n",
        "\n",
        "        # 标量标准化\n",
        "        self.mean_, self.std_ = _fit_scalar_norm(X_tr)\n",
        "        X_tr = (X_tr - self.mean_) / self.std_\n",
        "        X_val = (X_val - self.mean_) / self.std_\n",
        "\n",
        "        loader = self._loader(X_tr, y_tr, shuffle=True)\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        best_acc, no_improve = 0.0, 0\n",
        "        self.model.train()\n",
        "        # 不限定 epoch 上限，靠时间护栏和早停控制\n",
        "        while True:\n",
        "            # 一个 epoch\n",
        "            for xb, yb in loader:\n",
        "                if time.time() - t0 > self.time_budget:\n",
        "                    break\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                loss = crit(self.model(xb), yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            if time.time() - t0 > self.time_budget:\n",
        "                break\n",
        "\n",
        "            # 验证（大批量）\n",
        "            with torch.no_grad():\n",
        "                self.model.eval()\n",
        "                bs_eval = 8192\n",
        "                correct, total = 0, 0\n",
        "                for i in range(0, X_val.shape[0], bs_eval):\n",
        "                    logits = self.model(X_val[i:i+bs_eval])\n",
        "                    pred = logits.argmax(dim=1)\n",
        "                    correct += (pred == y_val[i:i+bs_eval]).sum().item()\n",
        "                    total   += min(bs_eval, X_val.shape[0]-i)\n",
        "                val_acc = correct / max(1, total)\n",
        "                self.model.train()\n",
        "\n",
        "            if val_acc > best_acc + self.min_delta:\n",
        "                best_acc = val_acc\n",
        "                no_improve = 0\n",
        "            else:\n",
        "                no_improve += 1\n",
        "\n",
        "            if (no_improve >= self.patience) and (time.time() - t0 >= self.min_train_seconds):\n",
        "                break\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = _to_float01(X_test).to(self.device)\n",
        "        X = (X - self.mean_) / self.std_\n",
        "        self.model.eval()\n",
        "        bs = 8192\n",
        "        outs = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            outs.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "# ================== Evaluate ==================\n",
        "import numpy as np, time as _time\n",
        "\n",
        "env.reset(); env.set_seed(42)\n",
        "\n",
        "agent = TorchMLP(output_dim=10, seed=42,\n",
        "                 lr=1e-3, batch_size=128, val_ratio=0.2,\n",
        "                 patience=2, min_delta=0.001,\n",
        "                 time_budget=5.8, min_train_seconds=2.0)\n",
        "\n",
        "accs, times = [], []\n",
        "print(\"Evaluating TorchMLP Agent (BN + Early Stopping)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None:\n",
        "        break\n",
        "    agent.reset()\n",
        "    t0 = _time.time()\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "    elapsed = _time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(\"\\nTorchMLP Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ddqgua2YN1q9",
      "metadata": {
        "id": "Ddqgua2YN1q9"
      },
      "source": [
        "# NewAgent1 精度高但是时间不对劲"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GkOLnllZNwhM",
      "metadata": {
        "id": "GkOLnllZNwhM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class _MLP(nn.Module):\n",
        "    def __init__(self, in_dim=784, h1=1024, h2=512, out_dim=10, p=0.1):\n",
        "        super().__init__()\n",
        "        # ReLU 版本，和 Kaiming(relu) 初始化配套，兼容所有常见 torch 版本\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h1), nn.ReLU(), nn.Dropout(p),\n",
        "            nn.Linear(h1, h2),     nn.ReLU(), nn.Dropout(p),\n",
        "            nn.Linear(h2, out_dim)\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2nDNMst8NaIj",
      "metadata": {
        "id": "2nDNMst8NaIj"
      },
      "outputs": [],
      "source": [
        "import time, math, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# 限制CPU线程，贴合评测（2核）\n",
        "try:\n",
        "    torch.set_num_threads(min(2, torch.get_num_threads()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "class _MLP(nn.Module):\n",
        "    def __init__(self, in_dim=784, h1=1024, h2=512, out_dim=10, p=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h1), nn.GELU(), nn.Dropout(p),\n",
        "            nn.Linear(h1, h2),     nn.GELU(), nn.Dropout(p),\n",
        "            nn.Linear(h2, out_dim)\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"gelu\")\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class HighAccuracyAgent:\n",
        "    \"\"\"PermutedMNIST 高精度/限时 Agent：目标 ≥97%，<60s/任务（CPU）\"\"\"\n",
        "    def __init__(self, output_dim: int = 10, seed: int = 42):\n",
        "        self.output_dim = output_dim\n",
        "        self.seed = seed\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        self.time_budget = 58.0  # 留2秒给预测/收尾\n",
        "        self.cfg = dict(in_dim=784, h1=1024, h2=512, p=0.10,\n",
        "                        lr=3e-3, wd=1e-4, bs=2048,\n",
        "                        max_epochs=12, min_epochs=3, warmup_steps=10)\n",
        "        self.reset()\n",
        "\n",
        "    def _set_seed(self, s):\n",
        "        np.random.seed(s); torch.manual_seed(s)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _fit_norm(self, X_flat):\n",
        "        m, s = X_flat.mean().item(), X_flat.std().item()\n",
        "        self.mean_ = m; self.std_ = (s if s >= 1e-6 else 1.0)\n",
        "\n",
        "    def _make_loader(self, X, y, bs, shuffle=True):\n",
        "        ds = torch.utils.data.TensorDataset(X, y)\n",
        "        return torch.utils.data.DataLoader(ds, batch_size=bs, shuffle=shuffle,\n",
        "                                           drop_last=False, num_workers=0, pin_memory=False)\n",
        "\n",
        "    def reset(self):\n",
        "        self._set_seed(self.seed)\n",
        "        self.model = _MLP(self.cfg[\"in_dim\"], self.cfg[\"h1\"], self.cfg[\"h2\"],\n",
        "                          self.output_dim, self.cfg[\"p\"]).to(self.device)\n",
        "        self.mean_, self.std_ = None, None\n",
        "\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        t0 = time.time()\n",
        "        X = torch.from_numpy(X_train.reshape(-1, 28*28)).float().to(self.device)\n",
        "        y = torch.from_numpy(y_train.reshape(-1).astype(np.int64)).to(self.device)\n",
        "        self._fit_norm(X)\n",
        "        X = (X - self.mean_) / self.std_\n",
        "\n",
        "        opt = optim.AdamW(self.model.parameters(), lr=self.cfg[\"lr\"], weight_decay=self.cfg[\"wd\"])\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        # 估计每步耗时，用于自适应 epoch\n",
        "        bs = self.cfg[\"bs\"]\n",
        "        loader_est = self._make_loader(X[:8192], y[:8192], bs)\n",
        "        self.model.train()\n",
        "        dry_steps, t_dry0 = 0, time.time()\n",
        "        for xb, yb in loader_est:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss = crit(self.model(xb), yb)\n",
        "            loss.backward(); opt.step()\n",
        "            dry_steps += 1\n",
        "            if dry_steps >= 5: break\n",
        "        time_per_step = max(1e-6, (time.time()-t_dry0)/dry_steps)\n",
        "\n",
        "        steps_per_epoch = math.ceil(len(X)/bs)\n",
        "        train_budget = max(2.0, self.time_budget - (time.time()-t0) - 2.0)\n",
        "        est_epochs = int(train_budget / (time_per_step * steps_per_epoch))\n",
        "        epochs = int(np.clip(est_epochs, self.cfg[\"min_epochs\"], self.cfg[\"max_epochs\"]))\n",
        "\n",
        "        loader = self._make_loader(X, y, bs, shuffle=True)\n",
        "        total_steps = max(1, epochs * steps_per_epoch)\n",
        "        sched = optim.lr_scheduler.OneCycleLR(\n",
        "            opt, max_lr=self.cfg[\"lr\"], total_steps=total_steps,\n",
        "            pct_start=min(0.3, self.cfg[\"warmup_steps\"]/total_steps),\n",
        "            anneal_strategy=\"cos\", cycle_momentum=False\n",
        "        )\n",
        "\n",
        "        for ep in range(epochs):\n",
        "            for xb, yb in loader:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                loss = crit(self.model(xb), yb)\n",
        "                loss.backward(); opt.step(); sched.step()\n",
        "                if (time.time()-t0) > (self.time_budget - 2.0): break\n",
        "            if (time.time()-t0) > (self.time_budget - 2.0): break\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        X = torch.from_numpy(X_test.reshape(-1, 28*28)).float().to(self.device)\n",
        "        m, s = (self.mean_ if self.mean_ is not None else 0.0), (self.std_ if self.std_ is not None else 1.0)\n",
        "        X = (X - m) / s\n",
        "        self.model.eval()\n",
        "        bs = 4096; out = []\n",
        "        for i in range(0, X.shape[0], bs):\n",
        "            logits = self.model(X[i:i+bs])\n",
        "            out.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        return np.concatenate(out, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CKE59EL5DXc_",
      "metadata": {
        "id": "CKE59EL5DXc_"
      },
      "outputs": [],
      "source": [
        "# Reset environment\n",
        "env.reset()\n",
        "env.set_seed(42)\n",
        "\n",
        "# Create agent\n",
        "agent = HighAccuracyAgent(output_dim=10, seed=42)\n",
        "\n",
        "# Track performance\n",
        "accs, times = [], []\n",
        "\n",
        "print(\"Evaluating High-Accuracy Agent\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "task_num = 1\n",
        "while True:\n",
        "    task = env.get_next_task()\n",
        "    if task is None: break\n",
        "\n",
        "    agent.reset()\n",
        "    t0 = time.time()\n",
        "\n",
        "    agent.train(task['X_train'], task['y_train'])\n",
        "    preds = agent.predict(task['X_test'])\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    acc = env.evaluate(preds, task['y_test'])\n",
        "\n",
        "    accs.append(acc); times.append(elapsed)\n",
        "    print(f\"Task {task_num}: Accuracy = {acc:.2%}, Time = {elapsed:.2f}s\")\n",
        "    task_num += 1\n",
        "\n",
        "print(f\"\\nHigh-Accuracy Agent Summary:\")\n",
        "print(f\"  Mean accuracy: {np.mean(accs):.2%} ± {np.std(accs):.2%}\")\n",
        "print(f\"  Total time: {np.sum(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-PGw3BEYdK4r",
      "metadata": {
        "id": "-PGw3BEYdK4r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import psutil\n",
        "\n",
        "def show_system_status():\n",
        "    # CPU 核数\n",
        "    num_cores = os.cpu_count()\n",
        "\n",
        "    # 系统内存\n",
        "    mem = psutil.virtual_memory()\n",
        "    total_mem = mem.total / (1024 ** 3)   # GB\n",
        "    used_mem = mem.used / (1024 ** 3)     # GB\n",
        "    free_mem = mem.available / (1024 ** 3)# GB\n",
        "\n",
        "    # 当前 Python 进程占用内存\n",
        "    process = psutil.Process()\n",
        "    proc_mem = process.memory_info().rss / (1024 ** 3)  # GB\n",
        "\n",
        "    print(\"=== System Status ===\")\n",
        "    print(f\"CPU cores: {num_cores}\")\n",
        "    print(f\"Total RAM: {total_mem:.2f} GB | Used: {used_mem:.2f} GB | Free: {free_mem:.2f} GB\")\n",
        "    print(f\"Current Python process memory: {proc_mem:.2f} GB\")\n",
        "\n",
        "# 调用示例\n",
        "show_system_status()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7awt7uxpg",
      "metadata": {
        "id": "f7awt7uxpg"
      },
      "source": [
        "## 7. Performance Comparison\n",
        "\n",
        "Let's visualize and compare the performance of both agents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "juhqqv664hg",
      "metadata": {
        "id": "juhqqv664hg"
      },
      "outputs": [],
      "source": [
        "# Create comparison plots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy comparison\n",
        "tasks = np.arange(1, len(random_accuracies) + 1)\n",
        "ax1.plot(tasks, random_accuracies, 'o-', label='Random Agent', alpha=0.7, linewidth=2)\n",
        "ax1.plot(tasks, linear_accuracies, 's-', label='Linear Agent', alpha=0.7, linewidth=2)\n",
        "ax1.axhline(y=0.1, color='gray', linestyle='--', alpha=0.5, label='Random chance (10%)')\n",
        "ax1.set_xlabel('Task Number')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_title('Accuracy per Task')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_ylim([0, 1])\n",
        "\n",
        "# Time comparison\n",
        "ax2.bar(tasks - 0.2, random_times, 0.4, label='Random Agent', alpha=0.7)\n",
        "ax2.bar(tasks + 0.2, linear_times, 0.4, label='Linear Agent', alpha=0.7)\n",
        "ax2.axhline(y=60, color='red', linestyle='--', alpha=0.5, label='1 minute threshold')\n",
        "ax2.set_xlabel('Task Number')\n",
        "ax2.set_ylabel('Time (seconds)')\n",
        "ax2.set_title('Training + Prediction Time per Task')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kahhtoqy13",
      "metadata": {
        "id": "kahhtoqy13"
      },
      "source": [
        "## 8. Statistical Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tt1p6s1wyho",
      "metadata": {
        "id": "tt1p6s1wyho"
      },
      "outputs": [],
      "source": [
        "# Create a summary comparison table\n",
        "print(\"PERFORMANCE COMPARISON SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Metric':<25} {'Random Agent':<20} {'Linear Agent':<20}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Accuracy metrics\n",
        "print(f\"{'Mean Accuracy':<25} {np.mean(random_accuracies):<20.2%} {np.mean(linear_accuracies):<20.2%}\")\n",
        "print(f\"{'Std Accuracy':<25} {np.std(random_accuracies):<20.2%} {np.std(linear_accuracies):<20.2%}\")\n",
        "print(f\"{'Min Accuracy':<25} {np.min(random_accuracies):<20.2%} {np.min(linear_accuracies):<20.2%}\")\n",
        "print(f\"{'Max Accuracy':<25} {np.max(random_accuracies):<20.2%} {np.max(linear_accuracies):<20.2%}\")\n",
        "\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Time metrics\n",
        "print(f\"{'Mean Time per Task':<25} {np.mean(random_times):<20.4f}s {np.mean(linear_times):<20.2f}s\")\n",
        "print(f\"{'Total Time':<25} {np.sum(random_times):<20.4f}s {np.sum(linear_times):<20.2f}s\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Performance analysis\n",
        "improvement = (np.mean(linear_accuracies) - np.mean(random_accuracies)) / np.mean(random_accuracies)\n",
        "print(f\"\\n📊 Linear agent shows {improvement:.1%} improvement over random baseline\")\n",
        "\n",
        "# Check if objective is met\n",
        "max_time_random = np.max(random_times)\n",
        "max_time_linear = np.max(linear_times)\n",
        "\n",
        "print(f\"\\n⏱️  Time Constraint Check (< 1 minute per task):\")\n",
        "print(f\"   Random Agent: {'✅ PASS' if max_time_random < 60 else '❌ FAIL'} (max: {max_time_random:.2f}s)\")\n",
        "print(f\"   Linear Agent: {'✅ PASS' if max_time_linear < 60 else '❌ FAIL'} (max: {max_time_linear:.2f}s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o7fuhntvo1b",
      "metadata": {
        "id": "o7fuhntvo1b"
      },
      "source": [
        "## 9. Key Insights and Conclusions\n",
        "\n",
        "### What we learned:\n",
        "\n",
        "1. **Random Baseline**: The random agent achieves ~10% accuracy, which is expected for random guessing on 10 classes.\n",
        "\n",
        "2. **Linear Agent Performance**: Despite the pixel and label permutations, the linear agent can learn patterns and achieve significantly better accuracy than random guessing.\n",
        "\n",
        "3. **Time Efficiency**: Both agents meet the < 1 minute requirement, with the random agent being faster (no learning) and the linear agent still being very efficient.\n",
        "\n",
        "4. **Meta-Learning Challenge**: Each task has different permutations, so the agent must learn from scratch each time. This tests the agent's ability to quickly adapt.\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Try implementing more sophisticated agents (e.g., neural networks with better architectures)\n",
        "- Experiment with different hyperparameters (learning rate, epochs, batch size)\n",
        "- Implement meta-learning algorithms that can leverage experience from previous tasks\n",
        "- Add early stopping or adaptive learning rates for better performance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3uiuylfq4zn",
      "metadata": {
        "id": "3uiuylfq4zn"
      },
      "source": [
        "## 10. Experiment: Tuning the Linear Agent\n",
        "\n",
        "Let's try different hyperparameters to see if we can improve performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o9f7p95dq8e",
      "metadata": {
        "id": "o9f7p95dq8e"
      },
      "outputs": [],
      "source": [
        "# Test different learning rates\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "results = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    # Reset environment\n",
        "    env.reset()\n",
        "    env.set_seed(42)\n",
        "\n",
        "    # Create agent with different learning rate\n",
        "    agent = LinearAgent(input_dim=784, output_dim=10, learning_rate=lr)\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    # Run through tasks\n",
        "    while True:\n",
        "        task = env.get_next_task()\n",
        "        if task is None:\n",
        "            break\n",
        "\n",
        "        agent.reset()\n",
        "        agent.train(task['X_train'], task['y_train'], epochs=5, batch_size=32)\n",
        "        predictions = agent.predict(task['X_test'])\n",
        "        accuracy = env.evaluate(predictions, task['y_test'])\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "    results[lr] = np.mean(accuracies)\n",
        "    print(f\"Learning rate {lr}: Mean accuracy = {results[lr]:.2%}\")\n",
        "\n",
        "# Find best learning rate\n",
        "best_lr = max(results, key=results.get)\n",
        "print(f\"\\n🏆 Best learning rate: {best_lr} with {results[best_lr]:.2%} accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DBSzVpLcK842",
      "metadata": {
        "id": "DBSzVpLcK842"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}